position	channelId	channelTitle	videoId	publishedAt	publishedAtSQL	videoTitle	videoDescription	videoCategoryId	videoCategoryLabel	duration	durationSec	dimension	definition	caption	thumbnail_maxres	licensedContent	viewCount	likeCount	dislikeCount	favoriteCount	commentCount
1	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	gHHy2w2agEo	2020-07-21T02:35:44Z	2020-07-21 02:35:44	Language Model Evaluation and Perplexity	Course Link: https://www.coursera.org/lecture/probabilistic-models-in-nlp/language-model-evaluation-SEO4T Transcript: In this video I'll show you how to evaluate a language model. The metric for this is called perplexity and I will explain what this is. First, you'll divide the text corpus into train validation and test data, then you will dive into the concepts of perplexity an important metric used to evaluate language models. So, how can you tell how well your language model is performing? Recall from the previous videos that a language model assigns a probability to each sentence. The model was trained on the corpus. So for the training sentences, it may assign very high probabilities. You should therefore first split the corpus to have some testing and validation data that are not used for the training. As you may have done in the other machine learning projects, you'll create the following splits of training validation and test sets. The training set is used to train your model. The validation set is used for things like tuning hyper-parameters, and the test set is held out for the end. Where you test it once and get an accuracy score that reflects how well your model performs on unseen data.	28	Science & Technology	PT6M46S	406	2d	hd	false	https://i.ytimg.com/vi/gHHy2w2agEo/maxresdefault.jpg	1	375	13	0	0	0
2	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	_z-a6WoNC2s	2020-06-02T12:48:05Z	2020-06-02 12:48:05	Common Patterns in Time Series: Seasonality, Trend and Autocorrelation	Course link: https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction Time-series come in all shapes and sizes, but there are a number of very common patterns. So it's useful to recognize them when you see them. For the next few minutes we'll take a look at some examples. The first is trend, where time series have a specific direction that they're moving in. As you can see from the Moore's Law example we showed earlier, this is an upwards facing trend. Another concept is seasonality, which is seen when patterns repeat at predictable intervals. For example, take a look at this chart showing active users at a website for software developers. It follows a very distinct pattern of regular dips. Can you guess what they are? Well, what if I told you if it was up for five units and then down for two? Then you could tell that it very clearly dips on the weekends when less people are working and thus it shows seasonality. Other seasonal series could be shopping sites that peak on weekends or sport sites that peak at various times throughout the year, like the draft or opening day, the All-Star day playoffs and maybe the championship game. Of course, some time series can have a combination of both trend and seasonality as this chart shows.	28	Science & Technology	PT5M6S	306	2d	hd	false	https://i.ytimg.com/vi/_z-a6WoNC2s/maxresdefault.jpg	1	529	24	1	0	0
3	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	H6oOhElB3yE	2020-03-20T22:30:41Z	2020-03-20 22:30:41	Limitations of Graph Neural Networks (Stanford University)		28	Science & Technology	PT1H26M35S	1595	2d	hd	false	https://i.ytimg.com/vi/H6oOhElB3yE/maxresdefault.jpg	1	4285	131	0	0	2
4	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	0lpT-yveuIA	2020-02-12T20:30:47Z	2020-02-12 20:30:47	Understanding Metropolis-Hastings algorithm	Course link: https://www.coursera.org/learn/mcmc-bayesian-statistics Metropolis-Hastings is an algorithm that allows us to sample from a generic probability distribution, which we'll call our target distribution, even if we don't know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution that we're looking for. It consists of picking an arbitrary starting value and then iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample. Let's say we want to produce samples from a target distribution. We're going to call it p of theta. But we only know it up to a normalizing constant or up to proportionality. What we have is g of theta. So we don't know the normalizing constant because perhaps this is difficult to integrate. So we only have g of theta to work with. The Metropolis Hastings Algorithm will proceed as follows. The first step is to select an initial value for theta. We're going to call it theta-naught. The next step is for a large number of iterations, so for i from 1 up to some large number m, we're going to repeat the following. The first thing we're going to do is draw a candidate. We'll call that theta-star as our candidate. And we're going to draw this from a proposal distribution. We're going to call the proposal distribution q of theta-star, given the previous iteration's value of theta. We'll take more about this q distribution soon. The next step is to compute the following ratio. We're going to call this alpha. It is this g function evaluated at the candidate divided by the distribution, or the density here of q, evaluated at the candidate given the previous iteration. And all of this will be divided by g evaluated at the old iteration. That divided by q, evaluated at the old iteration. Given the candidate value. If we rearrange this, it'll be g of the candidate times q of the previous value given the candidate divided by g at the previous value. And q evaluated at the candidate, given the previous value.....	28	Science & Technology	PT9M49S	589	2d	hd	false	https://i.ytimg.com/vi/0lpT-yveuIA/maxresdefault.jpg	1	7769	163	7	0	12
5	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	ByeRnmHJ-uk	2020-02-07T11:55:39Z	2020-02-07 11:55:39	Learning to learn: An Introduction to Meta Learning	Slides PDF: https://drive.google.com/file/d/1DuHyotdwEAEhmuHQWwRosdiVBVGm8uYx/view Abstract: In recent years, high-capacity models, such as deep neural networks, have enabled very powerful machine learning techniques in domains where data is plentiful. However, domains where data is scarce have proven challenging for such methods because high-capacity function approximators critically rely on large datasets for generalization. This can pose a major challenge for domains ranging from supervised medical image processing to reinforcement learning where real-world data collection (e.g., for robots) poses a major logistical challenge. Meta-learning or few-shot learning offers a potential solution to this problem: by learning to learn across data from many previous tasks, few-shot meta-learning algorithms can discover the structure among tasks to enable fast learning of new tasks. The objective of this tutorial is to provide a unified perspective of meta-learning: teaching the audience about modern approaches, describing the conceptual and theoretical principles surrounding these techniques, presenting where these methods have been applied previously, and discussing the fundamental open problems and challenges within the area. We hope that this tutorial is useful for both machine learning researchers whose expertise lies in other areas, while also providing a new perspective to meta-learning researchers. All in all, we aim to provide audience members with the ability to apply meta-learning to their own applications, and develop new meta-learning algorithms and theoretical analyses driven by the current challenges and limitations of existing work. We will provide a unified perspective of how a variety of meta-learning algorithms enable learning from small datasets, an overview of applications where meta-learning can and cannot be easily applied, and a discussion of the outstanding challenges and frontiers of this sub-field.	28	Science & Technology	PT1H27M17S	1637	2d	sd	false		1	3238	67	1	0	4
6	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	-zq9-6RbKZc	2019-12-20T13:11:20Z	2019-12-20 13:11:20	Page Ranking: Web as a Graph (Stanford University 2019)		28	Science & Technology	PT1H26M56S	1616	2d	hd	false	https://i.ytimg.com/vi/-zq9-6RbKZc/maxresdefault.jpg	1	1578	55	0	0	2
7	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	yFLiiK8c9CU	2019-11-14T17:06:02Z	2019-11-14 17:06:02	Deep Graph Generative Models (Stanford University - 2019)	In this video you will learn about the generative models which are applied directly on graph structures. This is a lecture of Stanford University.	28	Science & Technology	PT1H22M31S	1351	2d	hd	false	https://i.ytimg.com/vi/yFLiiK8c9CU/maxresdefault.jpg	1	6728	152	0	0	8
8	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	7JELX6DiUxQ	2019-11-01T11:41:07Z	2019-11-01 11:41:07	Graph Node Embedding Algorithms (Stanford - Fall 2019)	In this video a group of the most recent node embedding algorithms like Word2vec, Deepwalk, NBNE, Random Walk and GraphSAGE are explained by Jure Leskovec. Amazing class!	28	Science & Technology	PT1H25M51S	1551	2d	hd	false	https://i.ytimg.com/vi/7JELX6DiUxQ/maxresdefault.jpg	1	17964	438	2	0	33
9	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	YrhBZUtgG4E	2019-10-03T17:50:14Z	2019-10-03 17:50:14	Graph Representation Learning (Stanford university)	Slide link: http://snap.stanford.edu/class/cs224w-2018/handouts/09-node2vec.pdf	28	Science & Technology	PT1H16M53S	1013	2d	hd	false		1	32090	738	5	0	22
10	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	vyExfvVMk7A	2019-09-16T02:55:14Z	2019-09-16 02:55:14	Understanding Word Embeddings	Full course link: https://www.coursera.org/learn/intro-to-deep-learning So, the core idea here is that basically you want the words that have similar neighbores , similar contexts, to be similar in this new virtual representation. Now, let's see how we achieve that. But before we do that, let's actually cover some kind of math of how do we represent the words efficiently. So have a word named word. And technically, to fit it into TensorFlow, you'd probably have to represent it as some kind of number. For example, the ID of this word in your dictionary. And basically, the way you usually use this word in your pipeline is you take one-hot vectors, this large size of a dictionary vector that only has one nonzero value. And then push it through some kind of linear models or neural networks, or similar stuff. The only problem is, you're actually doing this thing very inefficiently. So you have this one-hot vector, and then you multiply it by a weight vector, or a weight matrix. It actually, it's actually process, because you have a lot of weights that gets multiplied by zeros. Now, you could actually compute this kind of weighted sum much more efficiently. If you look slightly closer, you could actually write the answer, you could actually write the answer itself without any sums or multiplications....	28	Science & Technology	PT13M22S	802	2d	hd	false	https://i.ytimg.com/vi/vyExfvVMk7A/maxresdefault.jpg	1	3804	38	15	0	4
11	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	V22sLWRZwF0	2019-07-29T18:37:38Z	2019-07-29 18:37:38	Variational Autoencoders - Part 2 ( Modeling a Distribution of Images )	Let's start with discussing a problem of fitting a distribution P of X into a data-set of points. Why? Well, we have already discussed this problem in week one, when we discussed how to fit a Gaussian to a data-set of points, we discussed it in week two, when we discussed clustering problem, and how we can solve it by fitting the Gaussian mixture model into our data. And also, we discussed probabilistic PC which is kind of an infinite mixture of Gaussians. But now, we will want to return to this question because it turns out that the methods we covered, like Gaussian or Gaussian mixture model on the probabilistic PC, are not enough to capture the complicated objects like images, like natural images. So, you may want to fit your data-set of natural images into a probabilistic distribution, for example, to generate new data. And, if you try to do that with Gaussian mixture model, it will work, but it will not work as well as some more sophisticated models we will discuss this week. And so, in this example for example, we generated some fake celebrity faces by using a generative model, and you can do these kinds of things if you have a probability distribution of your training data, so you can sample new images from this distribution. And also you can, if you have such a model, like P of X, you can also do a kind of Photoshop of the future applications, like here. So you can, with a few brush strokes, you can change a few pixels in your image, and the program will try to recolor everything else, so the picture will stay for the realistic. So, it will change the color of the hair and etc...	28	Science & Technology	PT10M33S	633	2d	hd	false	https://i.ytimg.com/vi/V22sLWRZwF0/maxresdefault.jpg	1	725	19	1	0	0
12	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	LvKt71lE04w	2019-07-24T17:30:01Z	2019-07-24 17:30:01	Variational Autoencoders - Part 1 (Scaling Variational Inference & Unbiased estimates)	Course Link: https://www.coursera.org/learn/bayesian-methods-in-machine-learning Welcome to week five of our course. This week we're going to talk about how to scale Bayesian methods to large data sets. So, even like 10 years ago, people used to think that Bayesian methods are mostly suited for small data sets because first of all, they're expensive, computation expensive. So if you want to do full Bayesian inference on like one million training examples, you are going to face lots of troubles. And second of all, there may not be beneficial anyway in the case of large data because people used to think that the main idea, the main benefit of Bayesian methods is to utilize your model, and to be able to extract as much information as possible from small data set. And if you have free large data set, then you don't need that, you can use any method you want and it will work just fine.But things changed then,Bayesian methods met deep learning, and people started to make some mixture models that has neural networks instead of a probabilistic model.And this is what this week will be about,how to combine neural networks with the Bayesian methods.So we'll discuss that.We'll discuss how to combine these two ideas.We'll see a particular example of variational old encoder, which allows you to generate nice samples,nice images by using neural network which has some probabilistic interpretation.And then, in the second module of Professor Dmitry Vetrov,will tell you about scalable methods for Bayesian neural networks,and about his cutting edge research inthis area that allowed him to compress neural networks by a lot,and then to fight severe over fitting on some complicated data sets.So, to start with,let's discuss a little bit of the concept of estimation being unbiased.We have already touched on that in the previous week, on week four,on Markov Chain Monte Carlo,but let's make our self a little bit more clear here.We'll need that to build unbiased estimates for gradients of some neural networks...	28	Science & Technology	PT6M26S	386	2d	sd	false		1	1536	23	2	0	1
13	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	6jl9KkmgDIw	2019-06-01T17:23:57Z	2019-06-01 17:23:57	DBSCAN: Part 2	Hello and welcome. In this video, we'll be covering DB scan. A density-based clustering algorithm which is appropriate to use when examining spatial data. So let's get started. Most of the traditional clustering techniques such as K-Means, hierarchical, and Fuzzy clustering can be used to group data in an unsupervised way. However, when applied to tasks with arbitrary shaped clusters or clusters within clusters, traditional techniques might not be able to achieve good results that is, elements in the same cluster might not share enough similarity or the performance may be poor. Additionally, while partitioning based algorithms such asK-Means may be easy to understand and implement in practice, the algorithm has no notion of outliers that is, all points are assigned to a cluster even if they do not belong in any. In the domain of anomaly detection, this causes problems as anomalous points will be assigned to the same cluster as normal data points. The anomalous points pull the cluster centroid towards them making it harder to classify them as anomalous points. In contrast, density-based clustering locates regions ofhigh density that are separated from one another by regions of low density. Density in this context is defined as the number of points within a specified radius.A specific and very popular type of density-based clustering is DBSCAN.DBSCAN is particularly effective for taskslike class identification on a spatial context.The wonderful attributes of the DBSCAN algorithm is that it canfind out any arbitrary shaped cluster without getting effected by noise.	28	Science & Technology	PT6M58S	418	2d	hd	false	https://i.ytimg.com/vi/6jl9KkmgDIw/maxresdefault.jpg	1	9142	199	10	0	29
14	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	sKRUfsc8zp4	2019-05-13T23:58:28Z	2019-05-13 23:58:28	DBSCAN: Part 1	In this session, we are going to introduce a density-based clustering algorithm called DBSCAN. DBSCAN is a density-based spatial clustering algorithm introduced by Martin Ester, Hanz-Peter Kriegel's group in KDD 1996. This paper received the highest impact paper award in the conference of KDD of 2014. This paper developed an interesting algorithms that can discover clusters of arbitrary shape. Actually, DBSCAN itself is acronym of density-based spatial clustering of applications with noise.	28	Science & Technology	PT8M21S	501	2d	hd	false	https://i.ytimg.com/vi/sKRUfsc8zp4/maxresdefault.jpg	1	13497	159	6	0	8
15	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	DODphRRL79c	2019-05-08T18:36:22Z	2019-05-08 18:36:22	Gaussian Mixture Models for Clustering	Now that we provided some background on Gaussian distributions, we can turn to a very important special case of a mixture model, and one that we're going to emphasize quite a lot in this course and in the assignment, and that's called a mixture of Gaussians. And remember that for any one of our image categories, and for any dimension of our observed vector like the blue intensity in that image, we're going to assume a Gaussian distribution to model that random variable. So for example, for forest images, if we just look at the blue intensity, then we might have a Gaussian distribution shown with the green curve here, which is centered about this value 0.42. And I want to mention here that we're actually assuming a Gaussian for the entire three-dimensional vector RGB. And that Gaussian can have correlation structure and it will have correlation structure between these different intensities, because the amount of RGB in an image tends not to be independent, especially within a given image class. But for the sake of illustrations and keeping all the drawings simple, we're just going to look at one dimension like this blue intensity here. But really, in your head, imagine these Gaussians in this higher dimensional space.........	28	Science & Technology	PT12M13S	733	2d	hd	false	https://i.ytimg.com/vi/DODphRRL79c/maxresdefault.jpg	1	16281	274	12	0	12
16	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	uoV1g3i9Qmw	2019-04-09T18:35:53Z	2019-04-09 18:35:53	Understanding Irreducible Error and Bias (By Emily Fox)	Okay, so, we've talked about three different measures of error. And now in this part, we're gonna talk about three different sources of error. And this is gonna lead us into a conversation of the bias variance trade-off. Okay, so when we were forming our prediction, there are three different sources of error. Noise, bias, and variance. And in this part, we're gonna walk through these three different components, at a very high level. At a more intuitive level. And then following this, there are gonna be two optional sections that go into much more formalism and detail about this. But those are optional because we're not requiring that you know this to get through the course. But for those that are interested, we will be providing the formalism behind these notions that I'm presenting now. Let's look at this first term, this noise term. And as we've mentioned many times in this specialization, data are inherently noisy.	28	Science & Technology	PT6M27S	387	2d	hd	false	https://i.ytimg.com/vi/uoV1g3i9Qmw/maxresdefault.jpg	1	2149	54	1	0	1
17	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	TyKzBoEaeEM	2019-03-21T22:19:40Z	2019-03-21 22:19:40	Python Libraries for Machine Learning You Must Know!	Course Link: https://www.coursera.org/learn/python-machine-learning Okay, now that we've covered a little bit of background on what machine learning is and some of the major types of machine learning problems, there's nothing like getting started with our own machine learning application in Python. And we're going to get started on that right now. So to do that, we're going to make use of several important Python libraries that will support our work. These include scikit-learn, SciPy, NumPy, pandas, and matplotlib. We recommend installing all of these using the Anaconda Python distribution since it comes with all the libraries we'll need in this part of the course. If you have some other existing Python installation, you can install the libraries we'll be using from the command line using pip, like this. The most important library we'll be using for machine learning is called scikit-learn. Scikit-learn is the most widely used Python library for machine learning and it will be the basis for this course .............	28	Science & Technology	PT4M40S	280	2d	hd	false	https://i.ytimg.com/vi/TyKzBoEaeEM/maxresdefault.jpg	1	982	29	0	0	0
18	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	FdBrwaS8_Ts	2019-03-04T16:57:08Z	2019-03-04 16:57:08	Conditional Probability	Course Link: https://www.coursera.org/learn/probability-intro In this video, we will define marginal, joint, and conditional probabilities. Introduce Bayes’ theorem for calculating conditional probabilities. And generalize the product rule for calc, calculating joint probabilities, regardless of whether the events are dependent or independent. Remember that previously we've talked about the probability of A and B equals probability of A times probability of B rule. And we said that there was a caveat to this rule, that the events had to be independent. So, we'll wrap up the discussion in this video with what do we do when the events are dependent, or if we don't know and cannot check whether the events are independent or not.	28	Science & Technology	PT12M41S	761	2d	hd	false	https://i.ytimg.com/vi/FdBrwaS8_Ts/maxresdefault.jpg	1	495	16	0	0	0
19	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	AcKA-0d8S1g	2019-01-30T14:04:51Z	2019-01-30 14:04:51	Training Tips for Deep Learning (By Intel)	Full Course Link: https://www.coursera.org/learn/intro-practical-deep-learning (An Introduction to Practical Deep Learning) In this lecture we will provide some training tips and tricks that are maybe useful when training deep learning models. We will start with a quick review then discuss overfitting, data augmentation, and end with a discussion on training validation development and testing. In classical machine learning, traditionally you have an image that is end by end, and you engineer a smaller set of K features. These features can be for example the ratio of the length to the height of the object, or the number of circle objects in the image. Then you apply your valid algorithm to learn to associate these patterns of features with an identity, In this case, vehicle. With supervised learning, we should learn algorithms used pre-labled training data to infer a function. Here imagine that you're trying to classify image into five categories. Vehicles, animals, faces, fruits, and chairs using two features, the two axises here. Supervised learning using the mission learning algorithms listed here allows for the determination of decision boundaries, shown here, using pre-labeled data. Here we can see that, while the decision boundaries are not perfect, they do a reasonable good job classifying the data. We'll now talk about overfitting, a common problem in machine learning.	28	Science & Technology	PT6M9S	369	2d	hd	false	https://i.ytimg.com/vi/AcKA-0d8S1g/maxresdefault.jpg	1	718	9	4	0	0
20	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	f9wIElV4s0A	2019-01-18T02:28:39Z	2019-01-18 02:28:39	Finding the Gradient with Newton-Raphson Method	In this module, we'll start to use the calculus we've done and put it together with vectors in order to start solving equations. In this first video, we'll look at a nice simple case where we just need to find the gradient, or the derivative in order to solve an equation using what's called the Newton-Raphson method. Now, say we've got that distribution of heights again with a mean, an average, mu and width sigma, and we want to fit an equation to that distribution. So, we don't have to after we've fitted it bother about carrying around all the data points, we just have a model with two parameters; a mean and a width. And we can do everything using just the model. And that would be loads faster simpler, I would let us make predictions and so on. So, it would be much, much nicer, but how do we find the right parameters for the model? How do we find the best mu and sigma we can? What we're going to do is, we're going to find some expression for how well the model fits the data, and then look at how that goodness of fit varies, is the fitting parameters mu and sigma vary.	28	Science & Technology	PT8M15S	495	2d	hd	false	https://i.ytimg.com/vi/f9wIElV4s0A/maxresdefault.jpg	1	742	17	0	0	0
21	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	4nqD5TBlOWU	2018-12-16T19:20:01Z	2018-12-16 19:20:01	Why Does Regularization Reduce Overfitting in Deep Neural Networks?	Why does regularization help with overfitting?Why does it help with reducing variance problems?Let's go through a couple examples to gain some intuition about how it works.So, recall that high bias, high variance.And I just write pictures from our earlier video that looks something like this.Now, let's see a fitting large and deep neural network.I know I haven't drawn this one too large or too deep,unless you think some neural network and this currently overfitting.So you have some cost function like J of W,B equals sum of the losses.So what we did for regularization was add this extra term that penalizes the weight matrices from being too large.So that was the Frobenius norm.So why is it that shrinking the L two norm orthe Frobenius norm or the parameters might cause less overfitting?One piece of intuition is that if youcrank regularisation lambda to be really, really big,they'll be really incentivized to setthe weight matrices W to be reasonably close to zero.So one piece of intuition is maybe it set the weight to be so close to zero fora lot of hidden units that's basically zeroing out a lot of the impact of these hidden units.And if that's the case,then this much simplified neural network becomes a much smaller neural network........	28	Science & Technology	PT7M10S	430	2d	hd	false	https://i.ytimg.com/vi/4nqD5TBlOWU/maxresdefault.jpg	1	1134	15	1	0	0
22	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	EFBDsJt9EM8	2018-12-11T00:44:19Z	2018-12-11 00:44:19	A Gentle Introduction to the Central Limit Theorem (CLT)	This unit is a formal introduction to statistical inference where you will see building blocks from the previous units come together in commonly used statistical inference methods just as confidence, intervals and hypothesis tests. In this until will also introduce the central limit theorem which provides the basis for these methods. Let's start with an example for a survey conducted by the Pew Research Center. The study is titled Young, Underemployed, and Optimistic, Coming of Age, Slowly, in a Tough Economy. Young adults hit hard by the recession. A plurality of the public. 41% believes young adults rather than middle aged or older adults are having the toughest time in today's economy. Tough economic times, altering young adults daily lives and long term plans. While, negative trends in the labor market have been felt most acutely by the youngest workers. Many adults in their late 20s and early 30s have also felt the impact of the weak economy. Among all 18 to 34 year olds, fully half, 49%, say they have taken a job they didn't want just to pay the bills with 24% saying they have taken an unpaid job to gain work experience ....	28	Science & Technology	PT31M53S	1913	2d	hd	false	https://i.ytimg.com/vi/EFBDsJt9EM8/maxresdefault.jpg	1	1204	21	0	0	2
23	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	dEhGM708xUs	2018-12-08T12:07:20Z	2018-12-08 12:07:20	Deep Neural Network Regularization - Part 1	If you suspect your neural network is over fitting your data. That is you have a high variance problem, one of the first things you should try per probably regularization. The other way to address high variance, is to get more training data that's also quite reliable. But you can't always get more training data, or it could be expensive to get more data. But adding regularization will often help to prevent overfitting, or to reduce the errors in your network. So let's see how regularization works. Let's develop these ideas using logistic regression. Recall that for logistic regression, you try to minimize the cost function J, which is defined as this cost function. Some of your training examples of the losses of the individual predictions in the different examples, where you recall that w and b in the logistic regression, are the parameters. So w is an x-dimensional parameter vector, and b is a real number. And so to add regularization to the logistic regression, what you do is add to it this thing, lambda, which is called the regularization parameter. I'll say more about that in a second. But lambda/2m times the norm of w squared. So here, the norm of w squared is just equal to sum from j equals 1 to nx of wj squared, or this can also be written w transpose w, it's just a square Euclidean norm of the prime to vector w. And this is called L2 regularization.......	28	Science & Technology	PT9M43S	583	2d	hd	false		1	996	14	1	0	2
24	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	PKuL6eEbKX8	2018-11-25T01:13:03Z	2018-11-25 01:13:03	The No Bullshit Guide to P-value: Introduction + Frequentist Statistics	If you read articles in the scientific literature, you'll often see people report P-values when they report statistical tests. P-values are widely used, and it's important to understand what they mean. They're also widely criticized, because people often misinterpret p-values. So in this lecture, the goal is to understand what they mean and how to correctly interpret them. When we talk about p-values, the first question we should ask ourselves is why are they so popular in scientific articles? Well, there's a reason for this, and Benjamini expresses it quite nicely. He says in some sense it offers a first line of defense against being fooled by randomness, separating the signal from the noise. So, this is what the p-values allow you to do. When you interpret your data, you might be very likely to interpret data in favor of the hypothesis that you have, even when the effect might be only slightly in the right direction. The risk is that you're fooling yourself. You might be too likely to declare that something is going on, when you're actually looking at random variation in data. So, p-values are one way to prevent you from fooling yourself. P-values tell you how surprising the data is, assuming that there is no effect. And we'll look at all these aspects in more detail. What surprising means, why they're statements about the data, and why they're built on the idea that there is no effect. Now, some people say that p-values are more accurately explained as what you use if you don't know Bayesian statistics yet. In Bayesian statistics, people don't use p-values.	28	Science & Technology	PT20M31S	1231	2d	hd	false	https://i.ytimg.com/vi/PKuL6eEbKX8/maxresdefault.jpg	1	1152	52	0	0	2
25	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	hfqDy508iC8	2018-11-11T18:24:54Z	2018-11-11 18:24:54	Deep Q-Learning in Tensorflow for CartPole - Part 2	Reinforcement Learning is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Just like how we learn to ride a bicycle, this kind of AI learns by trial and error. As seen in the picture, the brain represents the AI agent, which acts on the environment. After each action, the agent receives the feedback. The feedback consists of the reward and next state of the environment. The reward is usually defined by a human. If we use the analogy of the bicycle, we can define reward as the distance from the original starting point. Usually, training an agent to play an Atari game takes a while (from few hours to a day). So we will make an agent to play a simpler game called CartPole, but using the same idea used in the paper. CartPole is one of the simplest environments in OpenAI gym (a game simulator). As you can see in the animation from the top, the goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as the angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right.	28	Science & Technology	PT5M10S	310	2d	hd	false		1	719	11	0	0	0
26	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	r596XZ5UJQ8	2018-11-10T14:10:38Z	2018-11-10 14:10:38	Deep Q-Learning using Python - Part 1	Today we’ll learn about Q-Learning. Q-Learning is a value-based Reinforcement Learning algorithm. This video is the first part of a series of video tutorial about Deep Reinforcement Learning. In this video you’ll learn: What Q-Learning is? Some techniques in Deep Q-Learning Stay tuned for the next parts :) Until then, enjoy it!	28	Science & Technology	PT9M14S	554	2d	hd	false		1	1073	16	2	0	0
27	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	yGrzu9LU15E	2018-10-25T15:26:38Z	2018-10-25 15:26:38	Deep Learning for Handwritten Digit Recognition - Part 3	As we promised, this is the third part of Deep Learning for Handwritten Digit Recognition series. I hope you enjoy it. Please like and share this video so that you can help other users.	28	Science & Technology	PT6M3S	363	2d	hd	false		1	15524	156	23	0	20
28	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	oaYsCVtHveQ	2018-10-22T21:56:46Z	2018-10-22 21:56:46	Perplexity: is Our Model Surprised with a Real Text?	Hey, and welcome back. This is what you have already seen in the end of our previous video. So just to remind, we have some sequences and we are going to predict the probabilities of these sequences. So we learnt that with bigger language model, you can factorize your probability into some terms. So these are the probabilities of the next word, given the previous words. Now, take a moment to see whether everything is okay with the indices on this slide. Well, you can notice that i can be equal to 0 or to k plus 1, and it goes out of range of our sequence. But that's okay because if you remember our previous video, we discussed that we should have some fake tokens in the beginning of the sequence and in the end of the sequence. So this is qual to 0 and to k plus 1 will be exactly these fake tokens. So everything good here. Let us move forward. This is just a generalization. This is n-gram language model. So the only difference here is that the history gets longer. So we condition not only on the previous words but on the whole sequence of n minus 1 previous words. So just take a note to these denotions here. This is just a brief way to show that we have a sequence of n minus one words. Great. We have some intuition how to estimate these probabilities. So you remember that we can just count some n-grams and normalize these counts. But, now, I want to give you some intuition, not only just intuition but mathematical justification...	28	Science & Technology	PT8M6S	486	2d	hd	false	https://i.ytimg.com/vi/oaYsCVtHveQ/maxresdefault.jpg	1	2538	50	4	0	2
29	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	pYRIOGTPRPU	2018-08-26T12:30:01Z	2018-08-26 12:30:01	Understanding Gated Recurrent Unit (GRU) Deep Neural Network	You've seen how a basic RNN works.In this video, you learn about the Gated Recurrent Unit which is a modification to the RNN hidden layer that makes it much better capturing long range connections and helps a lot with the vanishing gradient problems.Let's take a look.You've already seen the formula for computing the activations at time t of RNN.It's the activation function applied tothe parameter Wa times the activations in the previous time set,the current input and then plus ba. So I'm going to draw this as a picture.So the RNN unit, I'm going to draw as a picture,drawn as a box which inputs a of t-1, the activation for the last time-step.And also inputs xt and these two go together.And after some weights and after this type of linear calculation,if g is a tanh activation function,then after the tanh, it computes the output activation a.And the output activation a(t) might also be passed to say a softener unit or something that could then be used to output yt. So this is maybe a visualization of the RNN unit of the hidden layer of the RNN in terms of a picture.And I want to show you this picture because we're going to use a similar picture to explain the GRU or the Gated Recurrent Unit.Lots of the idea of GRU were due to these two papers respectively by Yu Young Chang, Kagawa, Gaza Hera, Chang Hung Chu and Jose Banjo.And I'm sometimes going to refer to this sentence which we'd seen in the last video to motivate that............	28	Science & Technology	PT17M7S	1027	2d	hd	false		1	13241	140	12	0	3
30	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	GiyMGBuu45w	2018-08-12T13:43:53Z	2018-08-12 13:43:53	NLP: Understanding the N-gram language models	Hi, everyone. You are very welcome to week two of our NLP course. And this week is about very core NLP tasks. So we are going to speak about language models first, and then about some models that work with sequences of words, for example, part-of-speech tagging or named-entity recognition. All those tasks are building blocks for NLP applications. And they're very, very useful. So first thing's first. Let's start with language models. Imagine you see some beginning of a sentence, like This is the. How would you continue it? Probably, as a human,you know that This is how sounds nice, or This is did sounds not nice. You have some intuition. So how do you know this? Well, you have written books. You have seen some texts. So that's obvious for you. Can I build similar intuition for computers? Well, we can try. So we can try to estimate probabilities of the next words, given the previous words. But to do this, first of all,we need some data. So let us get some toy corpus. This is a nice toy corpus about the house that Jack built. And let us try to use it to estimate the probability of house, given This is the. So there are four interesting fragments here. And only one of them is exactly what we need. This is the house. So it means that the probability will be one 1 of 4. By c here, I denote the count. So this the count of This is the house,or any other pieces of text. And these pieces of text are n-grams. n-gram is a sequence of n words. So we can speak about 4-grams here. We can also speak about unigrams, bigrams, trigrams, etc. And we can try to choose the best n,and we will speak about it later. But for now, what about bigrams? Can you imagine what happens for bigrams, for example, how to estimate probability of Jack,given built? Okay, so we can count all different bigrams here, like that Jack, that lay, etc., and say that only four of them are that Jack. It means that the probability should be 4 divided by 10. So what's next? We can count some probabilities. We can estimate them from data. Well, why do we need this? How can we use this? Actually, we need this everywhere. So to begin with,let's discuss this Smart Reply technology. This is a technology by Google. You can get some email, and it tries to suggest some automatic reply. So for example, it can suggest that you should say thank you. How does this happen? Well, this is some text generation, right? This is some language model. And we will speak about this later,in many, many details, during week four. So also, there are some other applications, like machine translation or speech recognition. In all of these applications, you try to generate some text from some other data. It means that you want to evaluate probabilities of text, probabilities of long sequences. Like here, can we evaluate the probability of This is the house, or the probability of a long,long sequence of 100 words? Well, it can be complicated because maybe the whole sequence never occurs in the data. So we can count something, but we need somehow to deal with small pieces of this sequence, right? So let's do some math to understand how to deal with small pieces of this sequence. So here, this is our sequence of keywords. And we would like to estimate this probability. And we can apply chain rule,which means that we take the probability of the first word, and then condition the next word on this word, and so on. So that's already better. But what about this last term here? It's still kind of complicated because the prefix, the condition, there is too long. So can we get rid of it? Yes, we can. So actually, Markov assumption says you shouldn't care about all the history. You should just forget it. You should just take the last n terms and condition on them, or to be correct, last n-1 terms. So this is where they introduce assumption, because not everything in the text is connected. And this is definitely very helpful for us because now we have some chance to estimate these probabilities. So here, what happens for n = 2, for bigram model? You can recognize that we already know how to estimate all those small probabilities in the right-hand side,which means we can solve our task. So for a toy corpus again,we can estimate the probabilities. And that's what we get. Is it clear for now? I hope it is. But I want you to think about if everything is nice here. Are we done?	28	Science & Technology	PT10M33S	633	2d	hd	false		1	40951	580	35	0	37
31	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	wNBaNhvL4pg	2018-07-27T12:34:34Z	2018-07-27 12:34:34	Simple Deep Neural Networks for Text Classification	Hi. In this video, we will apply neural networks for text. And let's first remember, what is text? You can think of it as a sequence of characters, words or anything else. And in this video, we will continue to think of text as a sequence of words or tokens. And let's remember how bag of words works. You have every word and forever distinct word that you have in your dataset, you have a feature column. And you actually effectively vectorizing each word with one-hot-encoded vector that is a huge vector of zeros that has only one non-zero value which is in the column corresponding to that particular word. So in this example, we have very, good, and movie, and all of them are vectorized independently. And in this setting, you actually for real world problems, you have like hundreds of thousands of columns. And how do we get to bag of words representation? You can actually see that we can sum up all those values, all those vectors, and we come up with a bag of words vectorization that now corresponds to very, good, movie. And so, it could be good to think about bag of words representation as a sum of sparse one-hot-encoded vectors corresponding to each particular word. Okay, let's move to neural network way. And opposite to the sparse way that we've seen in bag of words, in neural networks, we usually like dense representation. And that means that we can replace each word by a dense vector that is much shorter. It can have 300 values, and now it has any real valued items in those vectors. And an example of such vectors is word2vec embeddings, that are pretrained embeddings that are done in an unsupervised manner. And we will actually dive into details on word2vec in the next two weeks. But, all we have to know right now is that, word2vec vectors have a nice property. Words that have similar context in terms of neighboring words, they tend to have vectors that are collinear, that actually point to roughly the same direction. And that is a very nice property that we will further use. Okay, so, now we can replace each word with a dense vector of 300 real values. What do we do next? How can we come up with a feature descriptor for the whole text? Actually, we can use the same manner as we used for bag of words. We can just dig the sum of those vectors and we have a representation based on word2vec embeddings for the whole text, like very good movie. And, that's some of word2vec vectors actually works in practice. It can give you a great baseline descriptor, a baseline features for your classifier and that can actually work pretty well. Another approach is doing a neural network over these embeddings.	28	Science & Technology	PT14M47S	887	2d	hd	false	https://i.ytimg.com/vi/wNBaNhvL4pg/maxresdefault.jpg	1	66710	1142	39	0	57
32	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	h-Tpb_blwb0	2018-07-25T12:09:46Z	2018-07-25 12:09:46	NLP - Linear Models for Text Sentiment Analysis	In this video, we will talk about first text classification model on top of features that we have described. And let's continue with the sentiment classification. We can actually take the IMDB movie reviews dataset, that you can download, it is freely available. It contains 25,000 positive and 25,000 negative reviews. And how did that dataset appear? You can actually look at IMDB website and you can see that people write reviews there, and they actually also provide the number of stars from one star to ten star. They actually rate the movie and write the review. And if you take all those reviews from IMDB website, you can actually use that as a dataset for text classification because you have a text and you have a number of stars, and you can actually think of stars as sentiment. If we have at least seven stars, you can label it as positive sentiment. If it has at most four stars, that means that is a bad movie for a particular person and that is a negative sentiment. And that's how you get the dataset for sentiment classification for free. It contains at most 30 reviews per movie just to make it less biased for any particular movie. These dataset also provides a 50/50 train test split so that future researchers can use the same split and reproduce their results and enhance the model. For evaluation, you can use accuracy and that actually happens because we have the same number of positive and negative reviews. So our dataset is balanced in terms of the size of the classes so we can evaluate accuracy here. Okay, so let's start with first model. Let's takes features, let's take bag 1-grams with TF-IDF values. And in the result, we will have a matrix of features, 25,000 rows and 75,000 columns, and that is a pretty huge feature matrix. And what is more, it is extremely sparse. If you look at how many 0s are there, then you will see that 99.8% of all values in that matrix are 0s. So that actually applies some restrictions on the models that we can use on top of these features. And the model that is usable for these features is logistic regression, which works like the following. It tries to predict the probability of a review being a positive one given the features that we gave that model for that particular review. And the features that we use, let me remind you, is the vector of TF-IDF values. And what you actually can do is you can find the weight for every feature of that bag of force representation. You can multiply each value, each TF-IDF value by that weight, sum all of that things and pass it through a sigmoid activation function and that's how you get logistic regression model. And it's actually a linear classification model and what's good about that is since it's linear, it can handle sparse data. It's really fast to train and what's more, the weights that we get after the training can be interpreted. And let's look at that sigmoid graph at the bottom of the slide. If you have a linear combination that is close to 0, that means that sigmoid will output 0.5. So the probability of a review being positive is 0.5. So we really don't know whether it's positive or negative. But if that linear combination in the argument of our sigmoid function starts to become more and more positive, so it goes further away from zero. Then you see that the probability of a review being positive actually grows really fast. And that means that if we get the weight of our features that are positive, then those weights will likely correspond to the words that a positive. And if you take negative weights, they will correspond to the words that are negative like disgusting or awful.	28	Science & Technology	PT10M41S	641	2d	hd	false		1	14095	207	10	0	3
33	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	7YacOe4XwhY	2018-07-23T13:37:21Z	2018-07-23 13:37:21	Feature Extraction from Text (USING PYTHON)	Hi. In this lecture will transform tokens into features. And the best way to do that is Bag of Words. Let's count occurrences of a particular token in our text. The motivation is the following. We're actually looking for marker words like excellent or disappointed, and we want to detect those words, and make decisions based on absence or presence of that particular word, and how it might work. Let's take an example of three reviews like a good movie, not a good movie, did not like. Let's take all the possible words or tokens that we have in our documents. And for each such token, let's introduce a new feature or column that will correspond to that particular word. So, that is a pretty huge metrics of numbers, and how we translate our text into a vector in that metrics or row in that metrics. So, let's take for example good movie review. We have the word good, which is present in our text. So we put one in the column that corresponds to that word, then comes word movie, and we put one in the second column just to show that that word is actually seen in our text. We don't have any other words, so all the rest are zeroes. And that is a really long vector which is sparse in a sense that it has a lot of zeroes. And for not a good movie, it will have four ones, and all the rest of zeroes and so forth. This process is called text vectorization, because we actually replace the text with a huge vector of numbers, and each dimension of that vector corresponds to a certain token in our database. You can actually see that it has some problems. The first one is that we lose word order, because we can actually shuffle over words, and the representation on the right will stay the same. And that's why it's called bag of words, because it's a bag they're not ordered, and so they can come up in any order. And different problem is that counters are not normalized. Let's solve these two problems, and let's start with preserving some ordering. So how can we do that? Actually you can easily come to an idea that you should look at token pairs, triplets, or different combinations. These approach is also called as extracting n-grams. One gram stands for tokens, two gram stands for a token pair and so forth. So let's look how it might work. We have the same three reviews, and now we don't only have columns that correspond to tokens, but we have also columns that correspond to let's say token pairs. And our good movie review now translates into vector, which has one in a column corresponding to that token pair good movie, for movie for good and so forth. So, this way, we preserve some local word order, and we hope that that will help us to analyze this text better. The problems are obvious though. This representation can have too many features, because let's say you have 100,000 words in your database, and if you try to take the pairs of those words, then you can actually come up with a huge number that can exponentially grow with the number of consecutive words that you want to analyze. So that is a problem. And to overcome that problem, we can actually remove some n-grams. Let's remove n-grams from features based on their occurrence frequency in documents of our corpus. You can actually see that for high frequency n-grams, as well as for low frequency n-grams, we can show why we don't need those n-grams. For high frequency, if you take a text and take high frequency n-grams that is seen in almost all of the documents, and for English language that would be articles, and preposition, and stuff like that. Because they're just there for grammatical structure and they don't have much meaning. These are called stop-words, they won't help us to discriminate texts, and we can pretty easily remove them. Another story is low frequency n-grams, and if you look at low frequency n-grams, you actually find typos because people type with mistakes, or rare n-grams that's usually not seen in any other reviews. And both of them are bad for our model, because if we don't remove these tokens, then very likely we will overfeed, because that would be a very good feature for our future classifier that can just see that, okay, we have a review that has a typo, and we had only like two of those reviews, which had those typo, and it's pretty clear whether it's positive or negative. So, it can learn some independences that are actually not there and we don't really need them. And the last one is medium frequency n-grams, and those are really good n-grams, because they contain n-grams that are not stop-words, that are not typos and we actually look at them. And, the problem is there're a lot of medium frequency n-grams. And it proved to be useful to look at n-gram frequency in our corpus for filtering out bad n-grams. What if we can use the same frequency for ranking of medium frequency n-grams?	28	Science & Technology	PT14M24S	864	2d	hd	false	https://i.ytimg.com/vi/7YacOe4XwhY/maxresdefault.jpg	1	44646	755	9	0	26
34	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	nxhCyeRR75Q	2018-07-18T23:22:19Z	2018-07-18 23:22:19	NLP - Text Preprocessing and Text Classification (using Python)	Hi! My name is Andre and this week, we will focus on text classification problem. Although, the methods that we will overview can be applied to text regression as well, but that will be easier to keep in mind text classification problem. And for the example of such problem, we can take sentiment analysis. That is the problem when you have a text of review as an input, and as an output, you have to produce the class of sentiment. For example, it could be two classes like positive and negative. It could be more fine grained like positive, somewhat positive, neutral, somewhat negative, and negative, and so forth. And the example of positive review is the following. "The hotel is really beautiful. Very nice and helpful service at the front desk." So we read that and we understand that is a positive review. As for the negative review, "We had problems to get the Wi-Fi working. The pool area was occupied with young party animals, so the area wasn't fun for us." So, it's easy for us to read this text and to understand whether it has positive or negative sentiment but for computer that is much more difficult. And we'll first start with text preprocessing. And the first thing we have to ask ourselves, is what is text? You can think of text as a sequence, and it can be a sequence of different things. It can be a sequence of characters, that is a very low level representation of text. You can think of it as a sequence of words or maybe more high level features like, phrases like, "I don't really like", that could be a phrase, or a named entity like, the history of museum or the museum of history. And, it could be like bigger chunks like sentences or paragraphs and so forth. Let's start with words and let's denote what word is. It seems natural to think of a text as a sequence of words and you can think of a word as a meaningful sequence of characters. So, it has some meaning and it is usually like,if we take English language for example,it is usually easy to find the boundaries of words because in English we can split upa sentence by spaces or punctuation and all that is left are words.Let's look at the example,Friends, Romans, Countrymen, lend me your ears;so it has commas,it has a semicolon and it has spaces.And if we split them those,then we will get words that are ready for further analysis like Friends,Romans, Countrymen, and so forth.It could be more difficult in German,because in German, there are compound words which are written without spaces at all.And, the longest word that is still in use is the following,you can see it on the slide and it actually stands forinsurance companies which provide legal protection.So for the analysis of this text,it could be beneficial to split that compound word intoseparate words because every one of them actually makes sense.They're just written in such form that they don't have spaces.The Japanese language is a different story.	28	Science & Technology	PT14M31S	871	2d	hd	false	https://i.ytimg.com/vi/nxhCyeRR75Q/maxresdefault.jpg	1	37456	501	14	0	13
35	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	FVUiPk_wwoU	2018-07-18T14:51:20Z	2018-07-18 14:51:20	Interpreting the Fitted Line in Simple Linear Regression	Okay, so that represented a kind of high level overview about this module, as well as, other aspects that we're going to touch upon in this course. But now let's delve into a specific case of simple linear regression and talk about what this means. So going back to our flowchart, what we're gonna talk about now is specifically the machine learning model. So that's that highlighted green box and everything else is grayed out so you can forget about everything else for now. We're just talking about our model and what form it takes. So our simple linear regression model is just that. It's very simple. We're assuming we have just one input, which in this case is, square feet of the house and one output which is the house sales price and we're just gonna fit a line,. A very simple function here not that quadratic function or higher order polynomials we talked about before, just a very simple line. And what's the equation of a line? Well, it's just intercept plus slope times our variable of interest so that we're gonna say that's wo + w1x. And what this regression model then specifies is that each one of our observations yi is simply that function evaluated at xi. So that's w0 plus w1xI plus the error term which we called epsilon i. So this is our regression model, and to be clear, this error, epsilon i, is the distance from our specific observation back down to the line. Okay, so the parameters of this model Are w0 and w1 are intercept and slope and we call these the regression coefficients. So that summarizes our simple linear regression model. Very straight forward. Very simple. But we'll get to more complicated things later.	28	Science & Technology	PT21M19S	1279	2d	hd	false	https://i.ytimg.com/vi/FVUiPk_wwoU/maxresdefault.jpg	1	294	6	0	0	0
36	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	xsTmUtwUg9Q	2018-07-06T17:29:07Z	2018-07-06 17:29:07	Simple Linear Regression by Emily Fox	This is one of the best courses about Linear Regression! Emily Fox does a great job and explains Linear Regression in the simplest possible way. In the below, you read about the course's program in detail: In the Foundations course, we talked about how machine learning is about deriving intelligence from data. And in this course, the machine learning method that we're gonna focus on is regression. And in particular, what regression is gonna assume is that we have some features that are derived from our data that are the input to our regression model. And then our goal will be to predict some continuous valued output or response to the input. The way we're gonna do this is by learning a relationship between our inputs x and this output y. For example, maybe you're interested in how taking this machine learning specialization is going to pay off for you in the end. So you're sitting here, you're doing a lot of really hard work and you wonder where this is going to land you. Well, maybe a question you might be interested in is what will your salary be after taking this specialization. And so we can think about predicting what your salary is based on things like what your performance was in the various courses, the quality of your capstone project, how many forum responses you are participating in, and different features like this. So this would be the input to the regression model and the prediction, the output that we're trying to predict would be your expected salary at the end of this specialization. Another example is predicting the price of a stock. And to form this prediction maybe we expect that this would depend on the past history of the stock, as well as perhaps recent news events, in addition to the trends in other related commodities. Or maybe you tweeted something, and you wanna know how many people are gonna retweet what you tweeted. Well this might depend on how many followers you have, how many followers your followers have, local structure of your follower network, what hash tags you used, how many retweets you've had in the past, and other features like this. Another example that we're going to talk about in this course is a really cool example of reading your mind. Where you go and you get some kind of brain scan, could be FMRI or MEG and for our sake we're just going to think of it as producing an image of your brain even though the truth is it produces something more complicated. But we can think of all the different pixel intensities as inputs to a regression model where the goal of the output is to predict whether you felt happy or sad in response to something you were shown when you were getting that brain scan. So it's reading your mind because we want to guess how you're feeling just from an image of your brain. But in this course, we're gonna focus in on a case study of predicting house prices. So in particular, a question we're gonna ask is, what's the value of a given house? Maybe you wanna sell your house and you wanna figure out how much to list that house for. And so we're gonna derive this intelligence by looking at some data. And the data we're gonna look at include other house sales. So we're gonna have the sales price associated with a bunch of other houses, as well as the house attributes of these other houses, and from these inputs, the house attributes, we're gonna learn this relationship between house attributes and the output, which is the sales price, and use this learned model in order to make the prediction of the value of your house. And this course is all about how to form this relationship between the input and the output.	28	Science & Technology	PT16M46S	1006	2d	hd	false	https://i.ytimg.com/vi/xsTmUtwUg9Q/maxresdefault.jpg	1	1506	28	0	0	1
37	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rg4tDdAleSE	2018-07-04T15:17:12Z	2018-07-04 15:17:12	Practical Guide to Logistic Regression Analysis in R	Logistic regression is a method for fitting a regression curve, y = f(x) when y is a categorical variable. The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both. In this video, we will show how to perform Logistic Regression in R!	28	Science & Technology	PT9M40S	580	2d	hd	false	https://i.ytimg.com/vi/rg4tDdAleSE/maxresdefault.jpg	1	262	3	2	0	2
38	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	8dvmk49jSNE	2018-07-02T13:38:00Z	2018-07-02 13:38:00	Decision Tree Classifier implementation in R		28	Science & Technology	PT10M25S	625	2d	hd	false	https://i.ytimg.com/vi/8dvmk49jSNE/maxresdefault.jpg	1	515	3	0	0	0
39	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	0G6gW-w8ZQo	2018-06-26T20:20:03Z	2018-06-26 20:20:03	Random Forest Using R: Step by Step Tutorial	You can download the "Credit Card Dataset" from the below link: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients Learn Data Science & Machine Learning by doing! Hands On Experience Data Scientist has been ranked the number one job on Glassdoor and the average salary of a data scientist is over $120,000 in the United States according to Indeed! Data Science is a rewarding career that allows you to solve some of the world's most interesting problems! This course is designed for both complete beginners with no programming experience or experienced developers looking to make the jump to Data Science! This course is for those : 1. Who wants to be Data Scientist 2. Who are working as analyst / software developer but wants to be Data Scientist What is Data Science ? Data science is used to extract patterns or insights from data to predict future or to understand customer behavior and so on. Data science is a "concept to unify statistics, data analysis and their related methods" in order to "understand and analyze actual phenomena" with data Mining large amounts of structured and unstructured data to identify patterns can help an organization to reduce costs, increase efficiencies, recognize new market opportunities and increase the organization's competitive advantage. Some Data Science and machine learning Applications Netflix uses data science & machine learning to mine movie viewing patterns to understand what drives user interest, and uses that to make decisions on which Netflix original series to produce. Companies like Flipkart and Amazon uses data science and machine learning to understand the customer shopping behavior to do better recommendations. Gmail's spam filter uses data science (machine learning algorithm) to process incoming mail and determines if a message is junk or not.. Proctor & Gamble utilizes data science (machine learning ) models to more clearly understand future demand, which help plan for production levels more optimally. Why Programming Won't Work in some Cases?? Have you ever thought of the scenario where all the cars will be moving without a driver that means something like automated machines say for example automatic washing machine. But there is a difference. 1. For automatic washing machine,we can write programs for the washing machine functionality. 2. For automated cars without drivers in high traffic.Just imagine ,how complex and dangerous it will be when someone starts coding /programming for such functionalities.For cars to automate we would require something which is called "Machine Learning " In this course, we are first going to first discuss Data Structures,etc. in R like : 1. Vectors 2. Matrices 3. Data Frames 4. Factors 5. Numerical/Categorical Variables 6. List 7. How to convert matrix into data frame Programming in R Data Visualization Then implementation/working of machine learning models like 1. Linear Regression 2. Decision Tree 3. Random Forest 4.Neural Networks 5. Deep learning 6. H2o framework 7. Cross validation /How to avoid Over fitting 8. Dimensionality Reduction Techniques All the materials for this data science & machine learning course are FREE. You can download and install R, with simple commands on Windows, Linux, or Mac. This course focuses on "how to build and understand", not just "how to use".It's not about "remembering facts", it's about "seeing for yourself" via experimentation. It will teach you how to visualize what's happening in the model internally.	28	Science & Technology	PT32M52S	1972	2d	hd	false		1	7467	16	8	0	1
40	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	tO6hTI8CXaM	2018-05-13T23:29:08Z	2018-05-13 23:29:08	Deep Reinforcement Learning Essential Prerequisite Review	In this section we are going to review all the background knowledge you need to have in order to understand Deep Reinforcement Learning. This includes: ** Markov Decision Processes (MDPs) ** Dynamic Programming ** Monte Carlo ** Temporal difference learning ** Deep Learning ** Approximation Methods ** State Transition Probabilities Hope to enjoy it!	28	Science & Technology	PT31M13S	1873	2d	hd	false		1	718	16	0	0	0
41	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	CHfrCEflVxE	2018-05-12T20:11:29Z	2018-05-12 20:11:29	Deep Reinforcement Learning in Python - Introduction	The Complete Guide to Mastering Artificial Intelligence using Deep Learning and Neural Networks! Requirements: • Know reinforcement learning basics, MDPs, Dynamic Programming, Monte Carlo, TD Learning • Calculus and probability at the undergraduate level • Experience building machine learning models in Python and Numpy • Know how to build a feedforward, convolutional, and recurrent neural network using Theano and Tensorflow This course is all about the application of deep learning and neural networks to reinforcement learning. If you’ve taken my first reinforcement learning class, then you know that reinforcement learning is on the bleeding edge of what we can do with AI. Specifically, the combination of deep learning with reinforcement learning has led to AlphaGo beating a world champion in the strategy game Go, it has led to self-driving cars, and it has led to machines that can play video games at a superhuman level. Reinforcement learning has been around since the 70s but none of this has been possible until now. The world is changing at a very fast pace. The state of California is changing their regulations so that self-driving car companies can test their cars without a human in the car to supervise. We’ve seen that reinforcement learning is an entirely different kind of machine learning than supervised and unsupervised learning. Supervised and unsupervised machine learning algorithms are for analyzing and making predictions about data, whereas reinforcement learning is about training an agent to interact with an environment and maximize its reward. Unlike supervised and unsupervised learning algorithms, reinforcement learning agents have an impetus - they want to reach a goal. This is such a fascinating perspective, it can even make supervised / unsupervised machine learning and "data science" seem boring in hindsight. Why train a neural network to learn about the data in a database, when you can train a neural network to interact with the real-world? While deep reinforcement learning and AI has a lot of potential, it also carries with it huge risk.	28	Science & Technology	PT9M58S	598	2d	hd	false	https://i.ytimg.com/vi/CHfrCEflVxE/maxresdefault.jpg	1	1766	9	0	0	0
42	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	FP6hwIzsoCg	2018-04-06T16:15:54Z	2018-04-06 16:15:54	Deep Learning for Handwritten Digit Recognition- Part 2		28	Science & Technology	PT17M38S	1058	2d	hd	false	https://i.ytimg.com/vi/FP6hwIzsoCg/maxresdefault.jpg	1	18991	186	11	0	40
43	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	lbFEZAXzk0g	2018-03-15T20:06:55Z	2018-03-15 20:06:55	Deep Learning for Handwritten Digit Recognition - Part1		28	Science & Technology	PT14M30S	870	2d	hd	false		1	57659	700	39	0	75
44	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	MYrJx-xf2jY	2018-02-19T19:25:25Z	2018-02-19 19:25:25	What the F**k is Computer Vision?!		28	Science & Technology	PT18M10S	1090	2d	hd	false		1	406	8	1	0	3
45	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	FZBmO8Ld8H0	2018-02-07T15:12:32Z	2018-02-07 15:12:32	Applications of Deep Learning in Computer Vision - Artificial Neural Networks	In this video you are going to learn about Artificial Neural Networks (ANN) and Perceptrons. Then, we will use ANN in form of a Deep Learning algorithm to use in Computer Vision	28	Science & Technology	PT11M19S	679	2d	hd	false	https://i.ytimg.com/vi/FZBmO8Ld8H0/maxresdefault.jpg	1	417	9	1	0	5
46	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	R3nLFT-lSVg	2018-02-07T14:02:35Z	2018-02-07 14:02:35	Applications of Deep Learning in Computer Vision - An Introduction to the New Course	This is an introduction to the new course we are going to publish for machine learning lovers. In this new series you are going to learn about the application of Deep Learning in Computer Vision. I hope you enjoy it!	28	Science & Technology	PT1M53S	113	2d	hd	false	https://i.ytimg.com/vi/R3nLFT-lSVg/maxresdefault.jpg	1	1050	8	0	0	0
47	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rrOgPiqYu6s	2018-01-06T13:33:21Z	2018-01-06 13:33:21	Convolutional Weights as Image Features ( Deep Learning with TensorFlow)	*** Dropouts During Testing *** Final Model Accuracy *** Convolutional Weights as Image Features	28	Science & Technology	PT4M56S	296	2d	hd	false		1	1910	6	0	0	0
48	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rKfpEcA6hqQ	2017-12-17T14:56:28Z	2017-12-17 14:56:28	BREAKING NEWS: The Real Video of Navy F/A-18 Encounter with UFO	A video shows an encounter between a Navy F/A-18 Super Hornet and an unknown object. It was released by the Defense Department’s Advanced Aerospace Threat Identification Program.	28	Science & Technology	PT35S	35	2d	hd	false	https://i.ytimg.com/vi/rKfpEcA6hqQ/maxresdefault.jpg	1	11484	29	6	0	10
49	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	RJf98lxpFyc	2017-11-21T16:03:07Z	2017-11-21 16:03:07	A Deeper Convolutional Neural Network with TensorFlow	In this lesson you are going to learn: ** Adding a convolutional layer to the model ** Transitioning to a dense layer ** Implementing dropout training	28	Science & Technology	PT4M9S	249	2d	hd	false	https://i.ytimg.com/vi/RJf98lxpFyc/maxresdefault.jpg	1	516	7	1	0	0
50	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	M1JH4pSiG00	2017-10-16T13:41:06Z	2017-10-16 13:41:06	Deep Convolutional Neural Network with TensorFlow	** Setting up input and weights for convolution ** Adding convolution and pooling layers ** Evaluating complete convolutional neural network	28	Science & Technology	PT6M29S	389	2d	hd	false	https://i.ytimg.com/vi/M1JH4pSiG00/maxresdefault.jpg	1	654	12	1	0	0
51	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	YFoOhmNZiSw	2017-09-20T17:46:11Z	2017-09-20 17:46:11	Pooling Layer Application: Convolutional Neural Network with TensorFlow	In this video you are going to learn: ** Implementing max pooling in TensorFlow ** Flattening the output of a pooling layer ** Visually inspecting pooling output Hope to enjoy it!	28	Science & Technology	PT4M18S	258	2d	hd	false		1	3315	21	7	0	0
52	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	90FI9hlFYLI	2017-08-25T19:14:45Z	2017-08-25 19:14:45	Building Deep Convolutional Neural Network (CNN) with TensorFlow- Part 2 ( Understanding CNN)	** Understanding input shapes for TensorFlow ** Implementing the convolutional layer ** Verifying the convolution on an example	28	Science & Technology	PT6M56S	416	2d	hd	false		1	1888	15	0	0	4
53	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	FTNNfba5CJw	2017-08-09T19:16:44Z	2017-08-09 19:16:44	Building Deep Convolutional Neural Network (CNN) with TensorFlow- Part 1 ( Understanding CNN)	** Understanding Convolutional Neural Network as a Sliding Winwod ** Max Pooling Layers with Example ** Convolutional Nets Applied to Font Problem	28	Science & Technology	PT5M4S	304	2d	hd	false		1	3381	34	2	0	1
54	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	oXXh01SwLrs	2017-07-21T13:08:31Z	2017-07-21 13:08:31	Building Deep Multiple Hidden Layer Neural Network with TensorFlow	In this lesson you will learn how to: -- Build a deep neural network -- Choose number of layers and neurons -- Training a deep densely connected net Hope you all enjoy it!	28	Science & Technology	PT5M23S	323	2d	hd	false	https://i.ytimg.com/vi/oXXh01SwLrs/maxresdefault.jpg	1	5458	40	3	0	1
55	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	ReSVIkljYj4	2017-07-11T15:53:23Z	2017-07-11 15:53:23	Implementing Single Hidden Layer Neural Network with TensorFlow	In this video you are going to learn: - Implementing a Neural Network with Single Hidden Layer in Tensor Flow - Intuition of "Back-propagation" for model training - And training your first Neural Network Hope you guys enjoy it!	28	Science & Technology	PT5M6S	306	2d	hd	false	https://i.ytimg.com/vi/ReSVIkljYj4/maxresdefault.jpg	1	4391	16	2	0	2
56	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rzilPq7xXC4	2017-07-06T15:03:15Z	2017-07-06 15:03:15	Basic Neural Nets with TensorFlow	Hope you all enjoy this video!	28	Science & Technology	PT5M17S	317	2d	hd	false		1	976	11	0	0	0
57	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	Tb0JcgRX5Go	2017-06-29T17:39:47Z	2017-06-29 17:39:47	Logistic Regression Training : Deep Learning with TensorFlow	In this video you will learn how to train Logistic Regression in TensorFlow. Hope you guys enjoy it! :)	28	Science & Technology	PT4M54S	294	2d	hd	false	https://i.ytimg.com/vi/Tb0JcgRX5Go/maxresdefault.jpg	1	779	5	0	0	1
58	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	GDOA_VM8G5o	2017-06-17T18:57:00Z	2017-06-17 18:57:00	Logistic Regression Model Building with TensorFlow	In this tutorial you will learn about building a logistic regression learning model using TensorFlow.	28	Science & Technology	PT6M59S	419	2d	hd	false	https://i.ytimg.com/vi/GDOA_VM8G5o/maxresdefault.jpg	1	2212	6	4	0	8
59	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	vLdPACYiMHY	2017-05-24T21:25:20Z	2017-05-24 21:25:20	TensorFlow Basic Operations: Deep Learning with TensorFlow	In this video we are going to learn about the basic Tensor operations, we are going to build a TensorFlow graph, and also we will see how to fetch and feed intermediate computations. Enjoy it!	28	Science & Technology	PT5M32S	332	2d	hd	false	https://i.ytimg.com/vi/vLdPACYiMHY/maxresdefault.jpg	1	1321	15	2	0	3
60	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	b5xsuSfe0AY	2017-05-13T17:26:59Z	2017-05-13 17:26:59	Installing TensorFlow - Deep Learning with TensorFlow	In this lesson you are going to learn how to install the Tensor Flow and warm up to get started Deep Learning programming using TensorFlow!	28	Science & Technology	PT5M34S	334	2d	hd	false	https://i.ytimg.com/vi/b5xsuSfe0AY/maxresdefault.jpg	1	2492	14	4	0	2
61	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	oml-73YipmI	2017-05-01T13:03:51Z	2017-05-01 13:03:51	Creating Recurrent Layers in Theano and Keras - Deep Learning with Python		28	Science & Technology	PT6M29S	389	2d	hd	false	https://i.ytimg.com/vi/oml-73YipmI/maxresdefault.jpg	1	1574	10	0	0	3
62	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	488BKXyDQWU	2017-04-20T19:20:30Z	2017-04-20 19:20:30	Deep Learning for Automatic Image Captioning (Using Python)!		28	Science & Technology	PT4M41S	281	2d	hd	false	https://i.ytimg.com/vi/488BKXyDQWU/maxresdefault.jpg	1	5191	68	2	0	0
63	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	8ZWMQcd7KSo	2017-04-09T16:27:50Z	2017-04-09 16:27:50	Reusing Pre trained Models - Deep Learning with Python		28	Science & Technology	PT7M23S	443	2d	hd	false	https://i.ytimg.com/vi/8ZWMQcd7KSo/maxresdefault.jpg	1	11378	54	2	0	9
64	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	CcabLpr2qmE	2017-03-29T14:27:32Z	2017-03-29 14:27:32	For vs Scan loop in Theano (Deep Learning using Python)	The scan functions provides the basic functionality needed to do loops in Theano. Scan comes with many whistles and bells, which we will introduce by way of examples.	28	Science & Technology	PT5M19S	319	2d	hd	false	https://i.ytimg.com/vi/CcabLpr2qmE/maxresdefault.jpg	1	455	1	0	0	0
65	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	YkGieAgSWho	2017-03-24T17:44:04Z	2017-03-24 17:44:04	Loading Pre trained Models with Theano - Deep Learning with Python		28	Science & Technology	PT5M16S	316	2d	hd	false		1	790	1	0	0	0
66	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	09Q4pXQUk1c	2017-03-20T21:25:39Z	2017-03-20 21:25:39	Large Scale Datasets and Very Deep Neural Networks - Deep Learning with Python		28	Science & Technology	PT5M18S	318	2d	hd	false		1	698	3	0	0	0
67	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	49IOTCzoWQg	2017-03-16T18:20:54Z	2017-03-16 18:20:54	Fully Connected or Dense Layers - Deep Learning with Python		28	Science & Technology	PT4M47S	287	2d	hd	false		1	7602	29	4	0	0
68	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	q0hLRqBB2YY	2017-03-07T13:22:11Z	2017-03-07 13:22:11	Value of Perfect Information - Stanford University	Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. This course is the first in a sequence of three. It describes the two basic PGM representations: Bayesian Networks, which rely on a directed graph; and Markov networks, which use an undirected graph. The course discusses both the theoretical properties of these representations as well as their use in practice. The (highly recommended) honors track contains several hands-on assignments on how to represent some real-world problems. The course also presents some important extensions beyond the basic PGM representation, which allow more complex models to be encoded compactly.	28	Science & Technology	PT17M15S	1035	2d	sd	false		1	1771	9	2	0	1
69	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	cO_ZOgH60b8	2017-03-05T17:17:41Z	2017-03-05 17:17:41	The No Bullshit Guide to Convolutional Neural Networks and Pooling Layers in Python	Convolutional Neural Networks (CNN) are biologically-inspired variants of MLPs. From Hubel and Wiesel’s early work on the cat’s visual cortex, we know the visual cortex contains a complex arrangement of cells. These cells are sensitive to small sub-regions of the visual field, called a receptive field. The sub-regions are tiled to cover the entire visual field. These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images. Additionally, two basic cell types have been identified: Simple cells respond maximally to specific edge-like patterns within their receptive field. Complex cells have larger receptive fields and are locally invariant to the exact position of the pattern. The animal visual cortex being the most powerful visual processing system in existence, it seems natural to emulate its behavior. Hence, many neurally-inspired models can be found in the literature.	28	Science & Technology	PT6M40S	400	2d	hd	false		1	1075	6	0	0	0
70	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	Jy1Fkdg6npY	2017-02-28T13:51:04Z	2017-02-28 13:51:04	Keras Behind the Scenes - Deep Learning with Python	Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: -Allows for easy and fast prototyping (through total modularity, minimalism, and extensibility). -Supports both convolutional networks and recurrent networks, as well as combinations of the two. -Supports arbitrary connectivity schemes (including multi-input and multi-output training). -Runs seamlessly on CPU and GPU.	28	Science & Technology	PT5M25S	325	2d	hd	false		1	1164	3	0	0	2
71	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	Ggh1APzbWv8	2017-02-27T17:44:48Z	2017-02-27 17:44:48	Optimizing a Simple Model in Pure Theano (Deep Learning with Python)	Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language with its increasing number of libraries that are available in Python. The aim of deep learning is to develop deep neural networks by increasing and improving the number of training layers for each network, so that a machine learns more about the data until it’s as accurate as possible. Developers can avail the techniques provided by deep learning to accomplish complex machine learning tasks, and train AI networks to develop deep levels of perceptual recognition. Deep learning is the next step to machine learning with a more advanced implementation. Currently, it’s not established as an industry standard, but is heading in that direction and brings a strong promise of being a game changer when dealing with raw unstructured data. Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language processing. Developers can avail the benefits of building AI programs that, instead of using hand coded rules, learn from examples how to solve complicated tasks. With deep learning being used by many data scientists, deeper neural networks are evaluated for accurate results. This course takes you from basic calculus knowledge to understanding backpropagation and its application for training in neural networks for deep learning and understand automatic differentiation. Through the course, we will cover thorough training in convolutional, recurrent neural networks and build up the theory that focuses on supervised learning and integrate into your product offerings such as search, image recognition, and object processing. Also, we will examine the performance of the sentimental analysis model and will conclude with the introduction of Tensorflow. By the end of this course, you can start working with deep learning right away. This course will make you confident about its implementation in your current work as well as further research.	28	Science & Technology	PT4M41S	281	2d	hd	false		1	857	4	0	0	2
72	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	K6m-iOU3iqk	2017-02-27T17:44:48Z	2017-02-27 17:44:48	Understanding Deep Learning with Theano	Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language with its increasing number of libraries that are available in Python. The aim of deep learning is to develop deep neural networks by increasing and improving the number of training layers for each network, so that a machine learns more about the data until it’s as accurate as possible. Developers can avail the techniques provided by deep learning to accomplish complex machine learning tasks, and train AI networks to develop deep levels of perceptual recognition. Deep learning is the next step to machine learning with a more advanced implementation. Currently, it’s not established as an industry standard, but is heading in that direction and brings a strong promise of being a game changer when dealing with raw unstructured data. Deep learning is currently one of the best providers of solutions regarding problems in image recognition, speech recognition, object recognition, and natural language processing. Developers can avail the benefits of building AI programs that, instead of using hand coded rules, learn from examples how to solve complicated tasks. With deep learning being used by many data scientists, deeper neural networks are evaluated for accurate results. This course takes you from basic calculus knowledge to understanding backpropagation and its application for training in neural networks for deep learning and understand automatic differentiation. Through the course, we will cover thorough training in convolutional, recurrent neural networks and build up the theory that focuses on supervised learning and integrate into your product offerings such as search, image recognition, and object processing. Also, we will examine the performance of the sentimental analysis model and will conclude with the introduction of Tensorflow. By the end of this course, you can start working with deep learning right away. This course will make you confident about its implementation in your current work as well as further research.	28	Science & Technology	PT5M5S	305	2d	hd	false		1	1497	9	0	0	0
73	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	ZYNeOwxqOkY	2017-02-23T16:52:44Z	2017-02-23 16:52:44	Introduction to Backpropagation with Python		28	Science & Technology	PT5M24S	324	2d	hd	false		1	8134	24	10	0	2
74	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	R7loaENZJMQ	2017-02-17T16:40:19Z	2017-02-17 16:40:19	Day #3 - Deep Learning "Hello World"! Classifying the MNIST Data (Deep Learning with Python)	In this class, we are going to see how to get our first deep neural network trained with Python! Hope to Enjoy it!	28	Science & Technology	PT7M58S	478	2d	hd	false		1	2820	19	2	0	2
75	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	pHVbSZ25pvY	2017-02-16T17:25:36Z	2017-02-16 17:25:36	Day #2 - Open Source Python Libraries for Deep Learning	In day #2 of this course, some of the most popular deep learning libraries are presented. Hope to enjoy it!	28	Science & Technology	PT4M31S	271	2d	hd	false		1	2357	9	1	0	0
76	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	5xJY6UxFe0Y	2017-02-15T15:39:55Z	2017-02-15 15:39:55	A Step by Step Introduction to Deep Learning with Python: Day 1 - What is Deep Learning?	We are all surrounded by data. An enormous amount of it. It is not only stored on big computational servers all over the world, on the computers in our offices, and cell phones in our pockets and constantly being generated, but also being transferred at a high volume in the air. This picture in the below only signifies how you can imagine about it. Deep learning is good at identifying patterns. Imagine for example about the image in the below. When you look at it, you can see there are bright spots in the image, whereas a lot of spots are dark. If I was to feed this image to a deep learning image recognition kind of program it would be able to identify where the bright spots are. Hope you enjoy this series of Deep Learning course!	28	Science & Technology	PT4M9S	249	2d	hd	false		1	7251	28	6	0	1
77	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	HVbUD9aA_Ys	2017-02-13T13:27:13Z	2017-02-13 13:27:13	Deep Features - University of Washington	So, we've learned that deep neural networks are really cool, high accuracy tool, but they can be really hard to build and learn, and require lots and lots of data. So next, we're gonna talk about something really exciting. Which is called deep features, which allow you to build neural networks, even when you don't have a lot of data. So, if you go back to our data image classification pipeline, where we start with an image, we detected some features, or other representations, and we've had that to a simple classifier, like a linear classifier. The question here is can we somehow use the features that we learn through the neural network? Those cool ones at the corners, edges and even faces, to feed that classifier? .......	28	Science & Technology	PT6M45S	405	2d	hd	false	https://i.ytimg.com/vi/HVbUD9aA_Ys/maxresdefault.jpg	1	756	8	0	0	2
78	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	wCVf6bwG8gc	2017-02-13T13:27:10Z	2017-02-13 13:27:10	Examples of Deep Learning in Computer Vision - University of Washington	So I showed you some examples of neural networks in computer vision and doing classification. Is there a labrador retriever in this image? But they can do quite a bit more. So, for example, we can do image parsing. So in this example, for every picture in the image, you're trying to classify it and discover regions. So in the center top image, you see a region of sky, another region of grass, and so on. And this kind of image description, or is called scene understanding, is pretty cool, and you know networks again, provided significant gains.	28	Science & Technology	PT1M31S	91	2d	hd	false	https://i.ytimg.com/vi/wCVf6bwG8gc/maxresdefault.jpg	1	2320	5	0	0	1
79	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	WWpj5Qs9r18	2017-02-13T13:27:09Z	2017-02-13 13:27:09	Challenges of Deep learning - University of Washington	Now, neural networks provide some exciting results, however, they do come with some challenges. So, on the pro side, they really enable you to represent this non-linear complex features and they have impressive results, not just in computer vision, but in some other areas like speech recognition. So systems like Siri on the phone and others use the neural networks behind the scene, as well as some text analysis tasks. And its potential for much more impact in the wide range of areas.	28	Science & Technology	PT2M23S	143	2d	hd	false	https://i.ytimg.com/vi/WWpj5Qs9r18/maxresdefault.jpg	1	411	2	0	0	1
80	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	_6scoIuPJdE	2017-02-13T13:27:09Z	2017-02-13 13:27:09	Deep Learning Performance - University of Washington	Deep learning is exciting because it learns these complex features of images. And as we discussed earlier. They've had tremendous impact over the recent years in a variety of computer vision applications. Let me show you a couple of early examples. So, on the top of the slide here, what you see is an example of identifying traffic signs based on neural networks. So these are a data of German traffic signs and the idea is for every image, identify what sign it is. And they were able to get 99.5% accuracy using a deep neural network, which is pretty cool. On the bottom there, you see an example that came out of some work from Google on identifying the house numbers based on what's called Street View data. This is the data that Google uses driving around cars and photographing all sorts of streets around the world. And you see the images are pretty complex, and still they're able to get 97.8% accuracy on the per character level.	28	Science & Technology	PT3M6S	186	2d	hd	false	https://i.ytimg.com/vi/_6scoIuPJdE/maxresdefault.jpg	1	142	2	0	0	1
81	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	pyDHbXVaWmQ	2017-02-13T13:27:09Z	2017-02-13 13:27:09	Application of Deep Learning to Computer Vision - University of Washington	The first place where neural networks made a tremendous amount of difference, is in an area called computer vision, so analyzing images and videos. So let's see a few examples of how deep learning, or this big neural networks, can be applied to computer vision. So to do that, it's good to understand what image features are. So in computer vision, image features are kind of like local detectors that get combined to make a prediction. So let's say we take this particular image. Suppose that I want to predict whether this a face image or not a face image. I run the neural detector, let's say a nose detector, eye detector, another eye detector, a mouth detector, and if all of these fire, you can do it and using a little neural network, you can say this is a face, and that's our prediction.	28	Science & Technology	PT5M42S	342	2d	hd	false	https://i.ytimg.com/vi/pyDHbXVaWmQ/maxresdefault.jpg	1	884	5	1	0	1
82	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	2S9j1h6ckro	2017-02-13T13:14:40Z	2017-02-13 13:14:40	Demo of Deep Learning Model on ImageNet Data - University of Washington	So we saw that deep learning had a tremendous part in the ImageNet competition. Which allowed them to take 1.5 minute image string deeply on your network and get amazing performance to predict one of a thousand different categories. So let's go ahead and show you a little demo of what kind of categories we're talking about and how cool the predictions were. So here's an example. It was the AlexNet frame on that ImageNet data set, which we then employed as a service that can be queried from this website. And so every time I click on an image it gets sent to that service which actually runs on a GPU, so it's fast and it comes back for prediction. So if I click on this particular image here, it gets sent to a service that actually hosts in on Amazon AWS. It comes back for prediction here. It's hidden, but when I click on it, it tells me what prediction is. So if I show you this image, it might be unclear what that image is, but if I click on it, it says parking meter, it turns out to be the right label.	28	Science & Technology	PT2M58S	178	2d	hd	false	https://i.ytimg.com/vi/2S9j1h6ckro/maxresdefault.jpg	1	1340	3	1	0	1
83	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	M8qdcOxDxgA	2017-02-11T17:28:58Z	2017-02-11 17:28:58	Making Sense of the World with Deep Learning - Facebook	In supervised learning, each input sample is provided with a target label dur¬ing training. In this talk, I will describe how deep learning methods, which are algorithms vaguely inspired by how the brain works, can be trained to predict the label of unseen inputs for three diﬀerent applications: speech recognition, text understanding and generic object recognition in images. In the ﬁrst ap¬plication, the input is 100ms of speech and the output is a phone label of the sound, in the second case the input is a sequence of words and the output is the subsequent word, in the last case the input is an image and the output is the label of the category of the object in the image (e.g., “dog”). Although these applications are very diﬀerent from each other, the learning algorithm is very similar. These methods have yielded the most accurate prediction systems on a variety of tasks, and they have recently been deployed in several commercial systems (e.g., speech recognition on Android phones and image search on Baidu search engine, to name a few).	28	Science & Technology	PT33M15S	1995	2d	hd	false		1	308	4	0	0	0
84	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	V8qrVleGY5U	2017-02-11T17:17:33Z	2017-02-11 17:17:33	Deep Learning for Natural Language Processing	Machine learning is everywhere in today's NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations.	28	Science & Technology	PT20M7S	1207	2d	hd	false	https://i.ytimg.com/vi/V8qrVleGY5U/maxresdefault.jpg	1	21667	184	11	0	1
85	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	4ACDwbpUWeE	2017-02-11T16:52:34Z	2017-02-11 16:52:34	Types of Neural Network Architectures	In this video we are going to describe various kinds of architectures for neural networks. What I mean by an architecture, is the way in which the neurons are connected together. By far the commonest type of architecture in practical applications is a feet forward neural network where the information comes into the input units and flows in one direction through hidden layers until each reaches the output units. A much more interesting kind architecture is a recurrent neural network in which information can flow round in cycles. These networks can remember information for a long time. They can exhibit all sorts of interesting oscillations but they are much more difficult to train in part because they are so much more complicated in what they can do. Recently, however, people have made a lot of progress in training recurrent neural networks, and they can now do some fairly impressive things. The last kind of architecture that I'll describe is a symmetrically-connected network, one in which the weights are the same in both directions between two units...	28	Science & Technology	PT7M29S	449	2d	hd	false	https://i.ytimg.com/vi/4ACDwbpUWeE/maxresdefault.jpg	1	3638	38	2	0	0
86	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	dDv5PmzpSM0	2017-02-07T15:19:14Z	2017-02-07 15:19:14	Decision Theory: Utility Functions - Stanford University	When we talked about influence diagram we included in the influence diagram nodes that represent the agent's utility function and those utility functions, we said, indicate an agent's preferences regarding the state of the world or different aspects of the state of the world. What are these utility functions and where do they come from? utility functions are necessary for our ability to compare complex scenarios that involve uncertainty or risk. It's not difficult for a person to say that they prefer an outcome where they get four million to one where they prefer three million........	28	Science & Technology	PT18M16S	1096	2d	sd	false	https://i.ytimg.com/vi/dDv5PmzpSM0/maxresdefault.jpg	1	23584	167	6	0	7
87	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	JtF5-Ji8JrQ	2017-02-07T15:19:13Z	2017-02-07 15:19:13	Decision Theory: Maximum Expected Utility - Stanford University	We've show how probabilistic graphical models can be used for a variety of inference tasks like computing conditional probabilities or finding the map assignment. But often the thing that you actually want to do with a probability distribution is make decisions in the world. So, for example, if you're a doctor encountering a patient, it's not just enough to figure out what disease the patient has. Ultimately you need to decide what treatment to give the patient. How do we use a probability distribution and specifically a probabilistic graphical model in order to make the decisions? ......	28	Science & Technology	PT25M58S	1558	2d	sd	false	https://i.ytimg.com/vi/JtF5-Ji8JrQ/maxresdefault.jpg	1	3794	38	1	0	1
88	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rc3YDj5GiVM	2017-01-31T15:32:51Z	2017-01-31 15:32:51	Conditional Random Fields - Stanford University (By Daphne Koller)	One very important variant of Markov networks, that is probably at this point, more commonly used then other kinds, than anything that's not of this type is what's called a conditional random field. So a conditional random field, you can think of it as a, something that looks very much like a Markov network, but for a somewhat different purpose. So let's think about what we are trying to do here. This class of model is intended to deal with what we call task-specific prediction, that where we have a set of input variables for observed variables, X, we have a set of target variables that we're trying to predict y. And, the class of models is intended to, is designed for those cases where we always have the same types of variables is the instance variables in the same types of variables as the targets...	28	Science & Technology	PT22M23S	1343	2d	sd	false		1	64852	490	67	0	11
89	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	IzlYOX0wrz0	2017-01-31T15:17:48Z	2017-01-31 15:17:48	General Gibbs Distribution - Stanford University	now we're going to define a much more general notion, that is considerably more expressive than the Pairwise case. And that definition is called the Gibbs distribution. So in order to motivate the notion of a Gibbs distribution, let's look at the most expressive Markov network that we could possible define in the context of pairwise interactions. So here we have four random variables, a, b, c, d, and I've introduced all of the possible pairwise edges between them. And so the question is, that we'd like to ask ourselves is, is this good enough? So, is this fully expressive?	28	Science & Technology	PT15M53S	953	2d	sd	false		1	7200	41	5	0	3
90	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	KWNtPHVf2VQ	2017-01-31T15:17:48Z	2017-01-31 15:17:48	Pairwise Markov Networks - Stanford University	there's two main families of graphical models. There's those that are based on directed graphs, directed acyclic graphs and those that are based on undirected graphs. The undirected graphical models are typically called Markov networks, they're also called Markov random field. We're going to start by talking about the simplest subclass of those which is pairwise Markov networks and then we're going to generalize it.	28	Science & Technology	PT11M	660	2d	sd	false	https://i.ytimg.com/vi/KWNtPHVf2VQ/maxresdefault.jpg	1	4995	76	0	0	1
91	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	THmTZ7WOkbY	2017-01-28T16:23:09Z	2017-01-28 16:23:09	Towards Practical Machine Learning with Differential Privacy and Beyond	Machine learning (ML) has become one of the most powerful classes of tools for artificial intelligence, personalized web services and data science problems across fields. However, the use of ML on sensitive data sets involving medical, financial and behavioral data are greatly limited due to privacy concern. In this talk, we consider the problem of statistical learning with privacy constraints. Under Vapnik's general learning setting and the formalism of differential privacy (DP), we establish simple conditions that characterizes the private learnability, which reveals a mixture of positive and negative insight. We then identify generic methods that reuse existing randomness to effectively solve private learning in practice; and discuss a weaker notion of privacy — on-avg KL-privacy — that allows for orders-of-magnitude more favorable privacy-utility tradeoff, while preserving key properties of differential privacy. Moreover, we show that On-Average KL-Privacy is **equivalent** to generalization for a large class of commonly-used tools in statistics and machine learning that sample from Gibbs distributions---a class of distributions that arises naturally from the maximum entropy principle. Finally, I will describe a few exciting future directions that use statistics/machine learning tools to advance he state-of-the-art for privacy, and use privacy (and privacy inspired techniques) to formally address the problem of p-hacking in scientific discovery.	28	Science & Technology	PT1H17S	17	2d	sd	false		1	65	2	0	0	0
92	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	TIem2hUa4jo	2017-01-25T17:09:23Z	2017-01-25 17:09:23	Graphical Models: Overview of Template Models - Stanford University	topic is an important extension on the language on graphical models. And it's intended to deal with the very large class of cases. Where what we'd like to do is not just write down one kind of graphical model for a particular application. But rather, come up with something that is a general purpose representation that allows us to solve multiple problems using the same exact model.....	28	Science & Technology	PT10M56S	656	2d	sd	false		1	643	5	0	0	0
93	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	fRhaGgMjgso	2017-01-25T17:09:23Z	2017-01-25 17:09:23	Template Models: Plate Models - Stanford University	One very common kind of repeated structure occurs when we have multiple objects of the same type. So that where we want to have, all these different of the object. It's not copies of the objects, but objects of the same type all have a similar or in fact, the same probabilistic model. for reason that we'll talk about momentarily the most, one of the most common type of such models is called the Plate Model. Let's start by modelling repetition...	28	Science & Technology	PT20M9S	1209	2d	sd	false		1	376	3	0	0	0
94	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	lecy8kEjC3Q	2017-01-25T17:09:23Z	2017-01-25 17:09:23	Template Models: Dynamic Bayesian Networks (DBNs) - Stanford University Coursera	There are many classes of models that that allow us to represent in a single concise representation, a template over riched models that incorporate multiple copies of the same variable and also allow us to represent multiple models within as a byproduct of a single representation. But one of the most commonly used among those is for reasoning about template models where we have a system that evolves over time...	28	Science & Technology	PT23M3S	1383	2d	sd	false		1	10435	108	1	0	1
95	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	mNSQ-prhgsw	2017-01-25T17:09:23Z	2017-01-25 17:09:23	Template Models: Hidden Markov Models - Stanford University	One simple yet extraordinarily class of probabilistic temporal models is the class of hidden Markov models. Although these are models can be viewed as a subclass of dynamic Bayesian networks. We'll see that they have their own type of structure that makes them particularly useful for a broad range of applications...	28	Science & Technology	PT12M2S	722	2d	sd	false		1	57824	420	21	0	13
96	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	6xBU74VWEuE	2017-01-24T13:46:44Z	2017-01-24 13:46:44	Naive Bayes Classifier - Stanford University Course		28	Science & Technology	PT9M53S	593	2d	sd	false		1	5484	37	3	0	1
97	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	1RgkCkN6IM0	2017-01-23T13:53:54Z	2017-01-23 13:53:54	Semantics & Factorization - Stanford University		28	Science & Technology	PT17M21S	1041	2d	sd	false		1	2706	26	0	0	1
98	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	BLbvW6FVniU	2017-01-23T13:53:54Z	2017-01-23 13:53:54	Reasoning Patterns - Stanford University		28	Science & Technology	PT10M	600	2d	sd	false		1	1007	6	0	0	1
99	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	JSrNWurmyLU	2017-01-23T13:53:54Z	2017-01-23 13:53:54	Flow of Probabilistic Influence - Stanford University		28	Science & Technology	PT14M37S	877	2d	sd	false		1	622	5	0	0	0
100	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	MUMkrhrDmqQ	2017-01-10T21:14:55Z	2017-01-10 21:14:55	What Never Ending Learning (NELL) Really is? - Tom Mitchell	Lecture's slide: https://drive.google.com/open?id=0B_G-8vQI2_3QeENZbVptTmY1aDA	28	Science & Technology	PT55M56S	3356	2d	sd	false		1	683	12	0	0	0
101	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	rkbW3OpWP50	2017-01-10T13:56:45Z	2017-01-10 13:56:45	Lecture 18 320x240		22	People & Blogs	PT1H22M44S	1364	2d	sd	false		1	23	1	0	0	0
102	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	MHd4ueNolW0	2017-01-09T22:06:58Z	2017-01-09 22:06:58	Reinforcement Learning 2, by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MDPs_RL_04_28_2011.pdf and https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/FinalExamStudyTopics.pdf (Final study guide)	22	People & Blogs	PT1H18M30S	1110	2d	sd	false		1	268	2	0	0	0
103	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	CQZD_cyurgE	2017-01-09T22:02:10Z	2017-01-09 22:02:10	Reinforcement Learning I, by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MDPs_RL_04_26_2011-ann.pdf	22	People & Blogs	PT1H20M6S	1206	2d	sd	false		1	1195	10	1	0	0
104	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	4lcOx4zqb7g	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Kernel Methods and SVM's by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Kernels_SVM_04_7_2011-ann.pdf	22	People & Blogs	PT1H17M1S	1021	2d	sd	false		1	1250	1	1	0	3
105	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	LXb1aCZ6MAk	2017-01-09T21:58:16Z	2017-01-09 21:58:16	SVM's II	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Kernels_SVM2_04_12_2011-ann.pdf	22	People & Blogs	PT1H18M56S	1136	2d	sd	false		1	81	0	0	0	1
106	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	OMRlnKupsXM	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Semi-Supervised Learning by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/LabUnlab-3-17-2011.pdf	22	People & Blogs	PT1H16M36S	996	2d	sd	false		1	7749	46	2	0	3
107	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	ULG6XcfpEf4	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Machine Learning in Computational Biology, by Ziv Bar-Joseph	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Ziv_CompBio.pdf	22	People & Blogs	PT1H7M55S	475	2d	sd	false		1	481	4	0	0	0
108	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	YyI-S2--BYc	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Active Learning by Burr Settles	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/settles-2.active.pdf	22	People & Blogs	PT1H14M50S	890	2d	sd	false		1	1297	8	2	0	2
109	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	gyJiwXx3oRM	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Neural Networks and Gradient Descent by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/NNets-701-3_24_2011_ann.pdf	22	People & Blogs	PT1H16M34S	994	2d	sd	false		1	1074	3	2	0	0
110	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	kknRKIJ4ytg	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Learning Representations III by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DimensionalityReduction_04_5_2011_ann.pdf	22	People & Blogs	PT1H19M21S	1161	2d	sd	false		1	34	0	0	0	0
111	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	pWweh3XZGHk	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Learning Representations II , Deep Beliefe Networks by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DimensionalityReduction_03_29_2011_ann.pdf	22	People & Blogs	PT1H22M44S	1364	2d	sd	false		1	49	0	0	0	0
112	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	v-BsN-zKPcI	2017-01-09T21:58:16Z	2017-01-09 21:58:16	Computational Learning Theory by Tom Mitchell	Lecture's slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning3_3-15-2011_ann.pdf	22	People & Blogs	PT1H10M37S	637	2d	sd	false		1	2664	21	0	0	0
113	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	WLq45vajrBY	2017-01-09T20:26:34Z	2017-01-09 20:26:34	PAC Learning Review by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning1-2-24-2011-ann.pdf	22	People & Blogs	PT1H20M29S	1229	2d	sd	false		1	1657	10	1	0	2
114	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	IG4Eil4vpBs	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Graphical models 2, by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod2_2_15_2011-ann.pdf	22	People & Blogs	PT1H19M10S	1150	2d	sd	false		1	156	0	0	0	0
115	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	IPGdTJAORi0	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Graphical models 1, by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod1_2_8_2011-ann.pdf	22	People & Blogs	PT1H18M31S	1111	2d	sd	false		1	239	0	0	0	0
116	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	Qiq6CrGXEr0	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Graphical models 3, by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod3_2_17_2011-ann.pdf	22	People & Blogs	PT1H16M23S	983	2d	sd	false		1	84	0	0	0	0
117	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	TGPhb36nVCI	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Graphical models 4, by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GrMod4_2_22_2011-ann.pdf	22	People & Blogs	PT1H17M32S	1052	2d	sd	false		1	38	0	0	0	0
118	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	XdsgZ7Yczww	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Computational Learning Theory by Tom Mitchell	Lecture Slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/PAC-learning1-2-24-2011-ann.pdf	22	People & Blogs	PT1H20M49S	1249	2d	sd	false		1	1437	10	0	0	0
119	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	pQwF_qUYYR8	2017-01-09T20:24:29Z	2017-01-09 20:24:29	Linear Regression by Tom Mitchell	Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GenDiscr_2_1-2011.pdf	22	People & Blogs	PT1H17M4S	1024	2d	sd	false		1	275	1	0	0	0
120	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	513yuIqYd0A	2017-01-09T20:11:35Z	2017-01-09 20:11:35	Logistic Regression by Tom Mitchell	Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/LR_1-27-2011.pdf	22	People & Blogs	PT1H20M15S	1215	2d	sd	false		1	599	2	0	0	0
121	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	gnY4HSigW6k	2017-01-09T20:08:47Z	2017-01-09 20:08:47	Gaussian Bayes classifiers,	Lecture slide: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/GNB_1-25-2011.pdf	22	People & Blogs	PT1H19M52S	1192	2d	sd	false		1	809	2	3	0	2
122	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	qZHTxW_25rc	2017-01-09T20:05:34Z	2017-01-09 20:05:34	Naive Bayes by Tom Mitchell	In order to get the lecture slide go to the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/NBayes-1-20-2011-ann.pdf	22	People & Blogs	PT1H16M58S	1018	2d	sd	false		1	1106	4	0	0	0
123	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	XEEOV9Solpo	2017-01-09T20:04:07Z	2017-01-09 20:04:07	Probability and Estimation by Tom Mitchell	In order to get the lecture slide go to the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/MLE_MAP_1-18-11-ann.pdf	22	People & Blogs	PT1H25M23S	1523	2d	sd	false		1	253	2	0	0	0
124	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	EfNfNhWnCfs	2017-01-09T19:55:01Z	2017-01-09 19:55:01	Intro to Machine Learning- Decision Trees By Tom Mitchell	Get the slide from the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/DTreesAndOverfitting-1-11-2011_final.pdf	22	People & Blogs	PT1H19M58S	1198	2d	sd	false		1	2163	4	7	0	0
125	UChIaUcs3tho6XhyU6K6KMrw	Machine Learning TV	k-4YGQ4mZIM	2017-01-09T19:54:35Z	2017-01-09 19:54:35	Overfitting, Random variables and probabilities by Tom Mitchell	Get the slide from the following link: https://www.cs.cmu.edu/%7Etom/10701_sp11/slides/Overfitting_ProbReview-1-13-2011-ann.pdf	22	People & Blogs	PT1H18M28S	1108	2d	sd	false		1	363	5	0	0	0
