position,channelId,channelTitle,videoId,publishedAt,publishedAtSQL,videoTitle,videoDescription,videoCategoryId,videoCategoryLabel,duration,durationSec,dimension,definition,caption,thumbnail_maxres,licensedContent,viewCount,likeCount,dislikeCount,favoriteCount,commentCount
1,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,C50UNga8LUY,2016-12-20T10:24:08Z,20/12/16 10:24,Personalised Recommendations for Modes of Transport: A Sequence-based Approach,"Author: Gunjan Kumar, Insight Centre for Data Analytics Abstract: In this paper we consider the problem of recommending modes of transport to users in an urban setting. In particular, we build on our past work in which a general framework for activity recommendation is proposed. To model the personal preferences and habits of users, the framework uses a sequence-based approach to capture the order as well as the context associated with user activity patterns. Here, we extend this work by introducing a machine learning approach to learn and take into account the natural variations in the regularity and repetition of individual user behaviour that occur. We demonstrate the versatility of our recommendation framework by applying it to the transport domain, and an evaluation using a real-world (mode of transport) dataset demonstrates the efficacy of the approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M5S,545,2d,hd,FALSE,https://i.ytimg.com/vi/C50UNga8LUY/maxresdefault.jpg,,190,2,1,0,0
2,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,M5m1Fg2-5BU,2016-11-10T15:17:29Z,10/11/16 15:17,Generating Local Explanations of Network Anomalies via Score Decomposition,"Author: Timothy La Fond, Computer Science Department, Purdue University Abstract: An important application in network analysis is the detection of anomalous events in a network time series. These events could merely be times of interest in the network timeline or they could be examples of malicious activity or network malfunction. Once a set of events are identified by the anomaly detection algorithm, a more detailed examination of the graph at these times can reveal important details about the behavior of the network. In this paper we use the score decomposition of the global anomaly score of reported anomalies in several dynamic networks to identify the regions of most anomalous behavior and provide interpretations as to the nature of the anomalous events. We also define a new version of the Graph Edit Distance and Clustering Coefficient statistics which are better at finding the local explanations for anomalous behavior. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M27S,1107,2d,hd,FALSE,https://i.ytimg.com/vi/M5m1Fg2-5BU/maxresdefault.jpg,,165,1,0,0,0
3,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xVTZKokxrg0,2016-11-10T15:17:19Z,10/11/16 15:17,Detection of Cyber-Physical Faults and Intrusions from Physical Correlations,"Author: Andrey Lokhov, Los Alamos National Laboratory More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M55S,1315,2d,hd,FALSE,https://i.ytimg.com/vi/xVTZKokxrg0/maxresdefault.jpg,,206,2,0,0,0
4,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,kMs1_IYhLwE,2016-11-10T15:16:50Z,10/11/16 15:16,Fast and Accurate Kmeans Clustering with Outliers,"Author: Shalmoli Gupta, Department of Computer Science, University of Illinois at Urbana-Champaign More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M45S,1125,2d,hd,FALSE,https://i.ytimg.com/vi/kMs1_IYhLwE/maxresdefault.jpg,,2256,7,0,0,0
5,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Wvgxo4UTUCc,2016-11-10T15:16:19Z,10/11/16 15:16,Dealing with Class Imbalance using Thresholding,"Author: Rumi Ghosh, Robert Bosch LLC. Abstract: We propose thresholding as an approach to deal with class imbalance. We define the concept of thresholding as a process of determining a decision boundary in the presence of a tunable parameter. The threshold is the maximum value of this tunable parameter where the conditions of a certain decision are satisfied. We show that thresholding is applicable not only for linear classifiers but also for non-linear classifiers. We show that this is the implicit assumption for many approaches to deal with class imbalance in linear classifiers. We then extend this paradigm beyond linear classification and show how non-linear classification can be dealt with under this umbrella framework of thresholding. The proposed method can be used for outlier detection in many real-life scenarios like in manufacturing. In advanced manufacturing units, where the manufacturing process has matured over time, the number of instances (or parts) of the product that need to be rejected (based on a strict regime of quality tests) becomes relatively rare and are defined as outliers. How to detect these rare parts or outliers beforehand? How to detect combination of conditions leading to these outliers? These are the questions motivating our research. This paper focuses on prediction of outliers and conditions leading to outliers using classification. We address the problem of outlier detection using classification. The classes are good parts (those passing the quality tests) and bad parts (those failing the quality tests and can be considered as outliers). The rarity of outliers transforms this problem into a class-imbalanced classification problem. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M40S,1120,2d,hd,FALSE,https://i.ytimg.com/vi/Wvgxo4UTUCc/maxresdefault.jpg,,2786,30,1,0,0
6,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,15bikX-HVSA,2016-11-10T15:15:51Z,10/11/16 15:15,Beauty and Brains: Detecting Anomalous Pattern Co-Occurrences,"Author: Roel Bertens, Department of Information and Computing Sciences, Utrecht University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M55S,955,2d,hd,FALSE,https://i.ytimg.com/vi/15bikX-HVSA/maxresdefault.jpg,,181,1,0,0,0
7,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,rlcuNHzMYW0,2016-11-10T15:15:25Z,10/11/16 15:15,Interpretable Anomaly Detection for Monitoring of High Performance Computing Systems,"Author: Elisabeth Baseman, Los Alamos National Laboratory More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M2S,1202,2d,hd,FALSE,https://i.ytimg.com/vi/rlcuNHzMYW0/maxresdefault.jpg,,354,3,0,0,0
8,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,dVkoJEIudKA,2016-11-10T15:15:04Z,10/11/16 15:15,Moving from Anomalies to Known Phenomena,"Author: Jeff Schneider, School of Computer Science, Carnegie Mellon University Abstract: Basic anomaly detection finds data points that are unusual relative to an expected distribution and there are good methods for defining expected distributions and quantifying deviation from them. However, using anomaly detectors in operational systems has proven to be a challenge. Naive flagging of anomalous points can lead to an overwhelming number of false positives and result in detection thresholds being lowered to the point of effectively turning off the anomaly detector. I will discuss two improvements for anomaly detection systems. The first is to observe that we are often not interested in individually anomalous data points. Instead, there are underlying phenomena we want to identify and those phenomena affect large groups of data points. Anomaly detection over sets of data can produce more significant results. Second, anomaly detection should be in a closed loop process that generates additional labeled data, updated models, and ever fewer detections, thus reducing load on the user. We will illustrate these ideas with some scientific and commercial use cases. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M57S,3057,2d,hd,FALSE,https://i.ytimg.com/vi/dVkoJEIudKA/maxresdefault.jpg,,602,4,0,0,0
9,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,WcUtScVc2g8,2016-11-10T14:44:23Z,10/11/16 14:44,A Generative Model of Urban Activities from Cellular Data,"Author: Mogeng Yin, UC Berkeley Abstract: Activity based travel models are the main tools used to evaluate traffic conditions in the context of rapidly changing travel demand. However, data collection for activity based models is performed through travel surveys that are infrequent, expensive, and reflect the changes in transportation with significant delays. Thanks to the ubiquitous cell phone data, we see an opportunity to substantially complement these surveys with data extracted from network carrier mobile phone usage logs, such as call detail records (CDRs). Activity based travel demand models describe travel itineraries of individual users, namely (1) what activities users are participating in; (2) when users perform these activities; and (3) how users travel to the activity locales. In this paper, we first present a method of extracting user stay locations while not over-filtering short-term travel. Second, we apply Input-Output Hidden Markov Models (IO-HMMs) to reveal the activity patterns in real CDR data (with a focus on the San Francisco Bay Area regular commuters) collected by a major network carrier. No personally identifiable information (PII) was gathered or used in conducting this study. The mobility data that was analyzed was anonymous and aggregated in strict compliance with the carrier’s privacy policy. Our approach delivers actionable information to the practitioners in a form of a modular activity-based travel demand model, and captures the heterogeneous activity transition probabilities conditioned on spatial-temporal context. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M40S,1120,2d,hd,FALSE,https://i.ytimg.com/vi/WcUtScVc2g8/maxresdefault.jpg,,445,1,0,0,1
10,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Nv2kfpccZ64,2016-11-10T14:43:52Z,10/11/16 14:43,Deep learning for driving detection on mobile phones,"Author: Allen Tran, Metromile Inc. Abstract: Sensor based activity recognition is a critical component of mobile phone based applications aimed at driving detection. Current methodologies consist of hand-engineered features input into discriminative models, and experiments to date have been restricted to small scale studies of O(10) users. Here we show how convolutional neural networks can be used to learn features from raw and spectrogram sensor time series collected from the phone accelerometer and gyroscope. While with limited training data such an approach under performs existing models, we show that convolutional neural networks outperform currently used discriminative models when the training dataset size is sufficiently large. We also test performance of the model implemented on the Android platform and we validate our methodology using sensor data collected from over 2000 mobile phone users. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M7S,1447,2d,hd,FALSE,https://i.ytimg.com/vi/Nv2kfpccZ64/maxresdefault.jpg,,622,3,1,0,0
11,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yeDGP5yKzf8,2016-11-10T14:43:21Z,10/11/16 14:43,Bayesian optimization and its applications for autonomous vehicles,"Author: Jeff Schneider, School of Computer Science, Carnegie Mellon University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT47M58S,2878,2d,hd,FALSE,https://i.ytimg.com/vi/yeDGP5yKzf8/maxresdefault.jpg,,1029,11,0,0,1
12,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,GcR5bpvF8zo,2016-11-10T14:42:53Z,10/11/16 14:42,Online Traffic Speed Forecasting Considering Multiple Periodicities and Complex Patterns,"Author: Hsing-Kuo Pao, Department of Computer Science and Information Engineering, National Taiwan University Abstract: Intelligent Transportation Systems (ITS) has been developed to aid drivers and other road-users to make a better travel decision. In recent years, many research efforts have been devoted in this field. Being one kind of time-series data, we can analyze the traffic data following the general aspects of studying time-series, which contains the analysis of periodicity of many kinds. This work highlights the study on the (long-term) multiple periodicities that could be found in traffic data while also considers more specific aspects such as unexpected short-term patterns, spatial relationship and feature correlations. Thanks to the periodicity of traffic data, most experienced drivers can tell how the traffic state will be on the road with given specific time and location. We aim to propose an approach with many of the above aspects to reach a quality traffic speed forecasting. We choose Gaussian process regression as the base model to realize the approach. Given the forecasting that considers all the above aspects, we enjoy the speed forecasting performance with MAE equal to one to two mph at its peak performance for a challenging speed forecasting 30-minute ahead of the current time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M49S,1369,2d,hd,FALSE,https://i.ytimg.com/vi/GcR5bpvF8zo/maxresdefault.jpg,,362,0,0,0,0
13,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,EXUIZFl97Co,2016-11-10T14:42:23Z,10/11/16 14:42,Panel on Machine Learning for Large Scale Transportation Systems,"Panelists: Alexandre Bayen, Department of Electrical Engineering and Computer Sciences, UC Berkeley Chris Pouliot, NextEV Ltd. Jeff Schneider, School of Computer Science, Carnegie Mellon University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT30M29S,1829,2d,hd,FALSE,https://i.ytimg.com/vi/EXUIZFl97Co/maxresdefault.jpg,,429,6,0,0,0
14,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,w477bIC18Fk,2016-11-10T14:41:19Z,10/11/16 14:41,Improving Demand Prediction in Bike Sharing System by Learning Global Features,"Author: Ming Zeng, Carnegie Mellon University Abstract: A bike sharing system deploys bicycles at many open docking stations and makes them available to the public for shared use. These bikes can be checked-in and checked-out at any of the docking stations. Predicting daily visits is important for service providers to optimize bike allocation and station maintenance. In this paper, we formulate this prediction problem as a regression task. Through data analysis, we develop several features that are very helpful in predictions. Moreover, we demonstrate that there are significant differences among the patterns of visits at different stations. To improve prediction accuracy, we propose station-centric augmented with global feature transformation. The gradient boosting decision tree (GBDT) and neural network (NN) techniques are leveraged to extract global features. The experimental results demonstrate that the proposed model offers better prediction performance compared to two baseline approaches. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M16S,1576,2d,hd,FALSE,https://i.ytimg.com/vi/w477bIC18Fk/maxresdefault.jpg,,1352,3,3,0,0
15,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yIlnP8Q6pXw,2016-11-10T14:16:40Z,10/11/16 14:16,"Learning Multi-Layer Coarse-to-Fine Representations for Over 10,000 Image Categories","Author: Ji Zhang, Xi'an Jiaotong University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M13S,1333,2d,hd,FALSE,https://i.ytimg.com/vi/yIlnP8Q6pXw/maxresdefault.jpg,,75,0,0,0,0
16,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,fjK4KAAjlhs,2016-11-10T14:16:18Z,10/11/16 14:16,Effective Auto-Encoder for Unsupervised Sparse Representation,"Author: Elaheh Rashedi, Department of Computer Science, Wayne State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M10S,550,2d,hd,FALSE,https://i.ytimg.com/vi/fjK4KAAjlhs/maxresdefault.jpg,,463,4,1,0,0
17,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,rr2CUIGe5YY,2016-11-10T14:14:51Z,10/11/16 14:14,Film2Vec: A Feature-based Film Distributed Representation for Rating Prediction,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT11M9S,669,2d,hd,FALSE,https://i.ytimg.com/vi/rr2CUIGe5YY/maxresdefault.jpg,,163,1,1,0,0
18,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,rCZ134mqa7A,2016-11-10T14:14:28Z,10/11/16 14:14,Applying Deep Learning to Improve Maritime Situational Awareness,"Author: Kathy Tang, School of Informatics and Computing, Indiana University Bloomington More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M44S,884,2d,hd,FALSE,https://i.ytimg.com/vi/rCZ134mqa7A/maxresdefault.jpg,,333,2,0,0,0
19,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Nbu16m1p8m4,2016-11-10T14:14:08Z,10/11/16 14:14,Deep Learning for Financial Sentiment Analysis,"Author: Sahar Sohangir, Florida Atlantic University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M44S,884,2d,hd,FALSE,https://i.ytimg.com/vi/Nbu16m1p8m4/maxresdefault.jpg,,1273,16,1,0,2
20,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jYkVIWB9vq0,2016-11-10T14:13:43Z,10/11/16 14:13,Long-term face tracking in the wild using deep learning,"Author: Elaheh Rashedi, Department of Computer Science, Wayne State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M10S,730,2d,hd,FALSE,https://i.ytimg.com/vi/jYkVIWB9vq0/maxresdefault.jpg,,356,7,0,0,2
21,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ONShj2aJats,2016-11-10T14:13:14Z,10/11/16 14:13,Improving Deep Neural Network Design for New Text Data Representations,"Author: Joseph D. Prusa, Florida Atlantic University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M56S,1016,2d,hd,FALSE,https://i.ytimg.com/vi/ONShj2aJats/maxresdefault.jpg,,214,3,0,0,0
22,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZHVu4aAKvDg,2016-11-10T14:12:45Z,10/11/16 14:12,Deep Learning for Chemical Compound Stability Prediction,"Author: Ruoqian Liu, Department of Electrical Engineering and Computer Science, Northwestern University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M16S,976,2d,hd,FALSE,https://i.ytimg.com/vi/ZHVu4aAKvDg/maxresdefault.jpg,,331,2,0,0,0
23,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ERTDbELQLyQ,2016-11-10T14:11:43Z,10/11/16 14:11,Two Types of Big Data and Three Styles of Deep Learning for AI Applications,"Author: Li Deng, Microsoft Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT57M5S,3425,2d,hd,FALSE,https://i.ytimg.com/vi/ERTDbELQLyQ/maxresdefault.jpg,,210,3,1,0,0
24,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,V7GsgOWACDg,2016-11-10T14:10:54Z,10/11/16 14:10,Leveraging Multi-Layer Deep Features for Large-Scale Visual Recognition,"Author: Tianyi Zhao, University of Science and Technology of China More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M51S,591,2d,hd,FALSE,https://i.ytimg.com/vi/V7GsgOWACDg/maxresdefault.jpg,,310,1,1,0,0
25,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4pS1Ef8QhXM,2016-11-10T14:09:20Z,10/11/16 14:09,Contextual LSTM (CLSTM) models for Large-scale NLP tasks,"Author: Shalini Ghosh, Artificial Intelligence Center, SRI International More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M59S,1079,2d,hd,FALSE,https://i.ytimg.com/vi/4pS1Ef8QhXM/maxresdefault.jpg,,862,2,0,0,1
26,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,7dsStC8KUVg,2016-11-10T13:09:28Z,10/11/16 13:09,Automated Sports Broadcasting,"Author: Peter Carr, Disney Research Abstract: In team sports, players move in complex but somewhat predictable ways. With state-of-the-art computer vision and machine learning methods, it is now possible to collect large tracking datasets and learn patterns of how players move. In this talk, I will describe methods developed at Disney Research to track players automatically using computer vision, as well as machine learning techniques to drive robotic cameras so that games can be recorded automatically. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT41M34S,2494,2d,hd,FALSE,https://i.ytimg.com/vi/7dsStC8KUVg/maxresdefault.jpg,,209,2,0,0,0
27,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,sllmzFRcI6o,2016-11-10T13:09:09Z,10/11/16 13:09,Recognizing and Analyzing Ball Screen Defense in the NBA,"Author: Joel Brooks, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT Abstract: As the NBA’s go-to offensive play, determining how to defend the ball screen is among the most critical decisions faced by NBA coaching staffs. In this talk, we present the construction and application of a tool for automatically recognizing common defensive counters to ball screens. Using SportVU player tracking data and supervised machine learning techniques, we learn a classifier that labels ball screens according to how they were defended. Applied to data from five NBA seasons, our classifier identified the screen defense of over 300,000 screens in total. These labeled data enable novel analyses of defensive strategies. We present observations and trends at both the team and player levels. Our work is a step towards the construction of a coaching assistance tool for analyzing one of the game’s most important actions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT30M40S,1840,2d,hd,FALSE,https://i.ytimg.com/vi/sllmzFRcI6o/maxresdefault.jpg,,205,1,2,0,0
28,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,uA206jxinkE,2016-11-10T13:08:36Z,10/11/16 13:08,Making Strides in Quantifying and Understanding Soccer,"Author: Sarah Rudd, StatDNA, LLC Abstract: Soccer has a rich history of people using data in an attempt to gain a better understanding of what happened in a game. However, due to its fluid nature, the sport is often assumed to be difficult to quantify and analyze. This talk will highlight some of the progress that has been made in soccer analytics in recent years, including some of the advances being made thanks to rich, full-tracking datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT35M49S,2149,2d,hd,FALSE,https://i.ytimg.com/vi/uA206jxinkE/maxresdefault.jpg,,3101,29,1,0,0
29,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,sDvmzuSYPpI,2016-11-10T13:08:03Z,10/11/16 13:08,Poster Spotlight 2,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT11M30S,690,2d,hd,FALSE,https://i.ytimg.com/vi/sDvmzuSYPpI/maxresdefault.jpg,,123,0,0,0,0
30,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,AJupBlOcdMA,2016-11-10T13:07:54Z,10/11/16 13:07,Poster Spotlight 1,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT11M43S,703,2d,hd,FALSE,https://i.ytimg.com/vi/AJupBlOcdMA/maxresdefault.jpg,,252,1,0,0,0
31,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,npAoBhH_qGo,2016-11-10T13:07:36Z,10/11/16 13:07,Making an Idea Machine: Modular Architecture for a Scaleable Exploratory Data Analysis Platform,"Author: Jesse Paquette, tag.bio Abstract: Exploratory data analysis has been a core facilitator of discovery in genomics research over the last 20 years. The critical advancement in the field has been to put data analysis tools in the hands of domain experts: non-programmers and non-statisticians that have the capacity to hypothesize and discover in their field. Jesse Paquette and tag.bio have identified key aspects from the discovery process in genomics and used them to develop a modular, scalable exploratory data analysis platform that can be configured for a wide variety of data domains. Case studies presented will focus on applications of the tag.bio platform for discovery in sports. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT29M26S,1766,2d,hd,FALSE,https://i.ytimg.com/vi/npAoBhH_qGo/maxresdefault.jpg,,54,3,0,0,0
32,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gVswnF11Y9A,2016-11-10T13:06:51Z,10/11/16 13:06,AI in Sport: What does it mean and how will it change sport in the future?,"Panelists: Sarah Rudd, StatDNA, LLC Joel Brooks, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT Jesse Paquette, tag.bio Irfan Essa, College of Computing, Georgia Institute of Technology Peter Carr, Disney Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M10S,3010,2d,hd,FALSE,https://i.ytimg.com/vi/gVswnF11Y9A/maxresdefault.jpg,,738,2,0,0,0
33,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,sGB-3bRJ-G0,2016-11-10T13:06:10Z,10/11/16 13:06,Computational Video for Sports: Challenges for Large Scale Data Analysis,"Author: Irfan Essa, College of Computing, Georgia Institute of Technology Abstract: Video technologies have had a huge impact on sports. Most professional sports events are captured in video, are broadcast and are consumed by many. Video is also becoming a sensor used to measure and analyze athletic performances, overall games, and commonly used as an aide for judging calls made on the field. In this talk about, I will discuss some of the specific advances made in sports video analysis. Computer vision techniques are now used widely in sports analysis to extract data, insights, and inferences of much value. I will highlight some challenges as more and more of such data is becoming available, especially with growing pervasiveness of cameras. I will discuss how some foundational work in computer vision can be brought to bear on this growing problem of sports video analysis and showcase a few recent examples of our work on tracking, registration, and summarization. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M34S,2734,2d,hd,FALSE,https://i.ytimg.com/vi/sGB-3bRJ-G0/maxresdefault.jpg,,145,3,0,0,0
34,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Xi5Jo3R3SQ0,2016-11-10T13:05:34Z,10/11/16 13:05,Gameplay First: Data Science at Blizzard Entertainment,"Author: Chaitanya Chemudugunta, Blizzard Entertainment Inc. Abstract: With a focus on gameplay first, Blizzard Entertainment is known for developing premium games like World of Warcraft, Starcraft, Diablo, Hearthstone, Heroes of the Storm and Overwatch. Tens of millions of players login daily and interact with a variety of game features generating massive amounts of rich and diverse data streams. In this talk, I will provide a general overview of data science challenges at Blizzard and discuss two challenges in the area of game design. Specifically, I will discuss challenges and solutions for matchmaking in competitive games; and discuss how gameplay and player segmentation can be used to inform game balance. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT39M7S,2347,2d,hd,FALSE,https://i.ytimg.com/vi/Xi5Jo3R3SQ0/maxresdefault.jpg,,877,12,0,0,0
35,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NOUMgThZ5UE,2016-11-10T12:35:48Z,10/11/16 12:35,Actionable and Political Text Classification Using Word Embeddings and LSTM,"Author: Adithya Rao, Klout, Inc. Abstract: In this work, we apply word embeddings and neural networks with Long Short-Term Memory (LSTM) to text classification problems, where the classification criteria are decided by the context of the application. We examine two applications in particular. The first is that of Actionability, where we build models to classify social media messages from customers of service providers as Actionable or Non-Actionable. We build models for over 30 different languages for actionability, and most of the models achieve accuracy around 85%, with some reaching over 90% accuracy. We also show that using LSTM neural networks with word embeddings vastly outperform traditional techniques. Second, we explore classification of messages with respect to political leaning, where social media messages are classified as Democratic or Republican. The model is able to classify messages with a high accuracy of 87.57%. As part of our experiments, we vary different hyperparameters of the neural networks, and report the effect of such variation on the accuracy. These actionability models have been deployed to production and help company agents provide customer support by prioritizing which messages to respond to. The model for political leaning has been opened and made available for wider use. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT28M38S,1718,2d,hd,FALSE,https://i.ytimg.com/vi/NOUMgThZ5UE/maxresdefault.jpg,,1418,6,1,0,1
36,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,iSHbVIahofk,2016-11-10T12:35:45Z,10/11/16 12:35,Predicting Trust Relations Among Users in a Social Network,"Author: Nikhita Vedula, Department of Computer Science and Engineering, Ohio State University Abstract: Trust is a key concept in social networks, reflecting credibility and reliability for a multitude of participants and online data. Nevertheless, the majority of such networks lack explicit trust feedback. This motivates a mechanism to predict and manage trust relations automatically. We extract in an unsupervised manner local trust relationships between pairs of users from social networks derived from Twitter. We take into account factors of measurable influence between users, the impact of the structural topology of users in the network and the valence (sentiment) associated with the languagebased information shared by network members. We evaluate our user trust rankings over other members of the network against a metric of ground truth for both social media data and a non-social media dataset, and analyze how the inclusion of valence lends robustness and stability to our model of trust. Knowledge of trustworthy citizens in social networks is quite advantageous in accurately assessing the credibility of the information they provide via social media, for the purpose of emergency response and recovery efforts during a disaster or a catastrophic event. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M25S,1105,2d,hd,FALSE,https://i.ytimg.com/vi/iSHbVIahofk/maxresdefault.jpg,,392,4,0,0,0
37,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,N5yYIam02zo,2016-11-10T12:35:16Z,10/11/16 12:35,Social Influence and Sentiment Analysis,"Author: Jie Tang, Department of Computer Science and Technology, Tsinghua University Abstract: Social influence is the behavioral change of a person because of the perceived relationship with other people, organizations and society in general. Social influence has been a widely accepted phenomenon in social networks for decades. Many applications have been built based around the implicit notation of social influence between people, such as marketing, advertisement and recommendations. With the exponential growth of online social network services such as Facebook and Twitter, social influence can for the first time be measured over a large population. In this talk, I will present how we quantify the influential degree between users and also introduce the application of social influence to sentiment analysis in large social networks. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT56M24S,3384,2d,hd,FALSE,https://i.ytimg.com/vi/N5yYIam02zo/maxresdefault.jpg,,190,3,0,0,1
38,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TlXY053AGho,2016-11-10T12:34:19Z,10/11/16 12:34,AnonyMine: Mining anonymous social media posts using psycho-lingual and crowd-sourced dictionaries,"Author: Arindam Paul, Department of Electrical Engineering and Computer Science, Northwestern University Abstract: There is lot of research activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text. Social media websites have become increasingly popular for discussing uncomfortable topics. However, there are limited resources for mining and automatically labeling posts discussing self-disclosure. There is great incentive for a system which can be useful for monitoring emotional state of users, both for the research community as well as for mental health and business purposes. This paper presents a case where we leverage information from psycho-lingual and crowd-sourced dictionaries to create a system which can automatically predict anonymous posts about taboo topics on a social media site (Facebook Confessions). We achieve more than 80% accuracy for the most popular taboo topics, and an overall accuracy of 61.25 % across all taboo categories. We evaluate our system in two ways: a) comparing against human-annotated posts on another anonymous social media platform YikYak b) an evaluation against existing state-of-the-art models. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M54S,1314,2d,hd,FALSE,https://i.ytimg.com/vi/TlXY053AGho/maxresdefault.jpg,,147,0,0,0,0
39,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jakFE5MtzeM,2016-11-10T12:33:56Z,10/11/16 12:33,Improving Sentiment Classification of Social Media Posts through Data Refinements,"Author: Vita Markman, LinkedIn Corporation Abstract: Quality training data is essential for building high performance machine learning models. However, certain types of tasks such as opinion mining are inherently subjective, making it hard to elicit reliable judgements from human annotators. The problem is further exacerbated in situations where opinions are elicited on short text such as Tweets or micro reviews containing only one or two lines. The talk addresses various means of circumventing these challenges via automation of some annotation tasks as well as setting up multiple experiments for collecting human judgements. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M14S,3014,2d,hd,FALSE,https://i.ytimg.com/vi/jakFE5MtzeM/maxresdefault.jpg,,232,2,0,0,0
40,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,00URhb_PgM0,2016-11-10T11:58:34Z,10/11/16 11:58,Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models,"Author: Adam Perer, IBM Thomas J. Watson Research Center Abstract: Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these naïve estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M32S,1112,2d,hd,FALSE,https://i.ytimg.com/vi/00URhb_PgM0/maxresdefault.jpg,,158,0,0,0,0
41,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,bwR_e5f7oA0,2016-11-10T11:58:03Z,10/11/16 11:58,"At Last! Time Series Joins, Motifs, Discords and Shapelets at Interactive Speeds","Author: Eamonn Keogh, Department of Computer Science and Engineering, University of California, Riverside Abstract: Given the ubiquity of time series, the last decade has seen a flurry of activity in time series data mining. Some of the most useful and frequently used primitives “reason” about the shapes of subsequences found in longer time series. Examples include Time Series Joins, Motifs, Discords and Shapelets. These primitives have found significant adoption, however they are all run in batch mode. For most non-trivial datasets, you start the process; you go to lunch (or on a short vacation!) and examine the results when you get back. What if you could solve such problems in interactive time? Well, now you can! With a new data structure call the Matrix Profile, interactive data mining of large datasets has become possible for the first time, and as we shall demonstrate, it is a game changer. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M27S,1467,2d,hd,FALSE,https://i.ytimg.com/vi/bwR_e5f7oA0/maxresdefault.jpg,,937,7,2,0,2
42,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9ahg9D51mSA,2016-11-10T11:57:40Z,10/11/16 11:57,Predictive Interaction,"Author: Jeffrey Heer, Department of Computer Science and Engineering, University of Washington Abstract: How might we architect interactive systems that have better models of the tasks we're trying to perform, learn over time, help refine ambiguous user intents, and scale to large or repetitive workloads? In this talk I will present Predictive Interaction, a framework for interactive systems that shifts some of the burden of specification from users to algorithms, while preserving human guidance and expressive power. The central idea is to imbue software with domain-specific models of user tasks, which in turn power predictive methods to suggest a variety of possible actions. I will illustrate these concepts with examples drawn from widely-deployed systems for data transformation and visualization (with reported order-of-magnitude productivity gains) and then discuss associated design considerations and future research directions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT49M45S,2985,2d,hd,FALSE,https://i.ytimg.com/vi/9ahg9D51mSA/maxresdefault.jpg,,57,0,0,0,0
43,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Y054OfP37hE,2016-11-10T11:56:49Z,10/11/16 11:56,Regression Location and Scale Estimation with Application to Censoring Slides,"Author: Jerome H. Friedman, Department of Statistics, Stanford University Abstract: The aim of regression analysis in machine learning is to estimate the location of the distribution of an outcome variable y, given the joint values of a set of predictor variables x. This location estimate is then used as a prediction for the value of y at x. The accuracy of this prediction depends on the scale of the distribution of y at x, which in turn, usually depends on x (heteroscedasticity). A robust procedure is presented for jointly estimating both the location and scale of the distribution of y given x, as functions of x, under no assumptions concerning the relationship between the two functions. The scale function can then be used to access the accuracy of individual predictions, as well as to improve accuracy especially in the presence of censoring. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT54M34S,3274,2d,hd,FALSE,https://i.ytimg.com/vi/Y054OfP37hE/maxresdefault.jpg,,347,6,0,0,0
44,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8eODgQHLo7o,2016-11-10T11:56:07Z,10/11/16 11:56,Clustrophile: A Tool for Visual Clustering Analysis,"Author: Çağatay Demiralp, IBM Thomas J. Watson Research Center Abstract: While clustering is one of the most popular methods for data mining, analysts lack adequate tools for quick, iterative clustering analysis, which is essential for hypothesis generation and data reasoning. We introduce Clustrophile, an interactive tool for iteratively computing discrete and continuous data clusters, rapidly exploring different choices of clustering parameters, and reasoning about clustering instances in relation to data dimensions. Clustrophile combines three basic visualizations – a table of raw datasets, a scatter plot of planar projections, and a matrix diagram (heatmap) of discrete clusterings – through interaction and intermediate visual encoding. Clustrophile also contributes two spatial interaction techniques, forward projection and backward projection, and a visualization method, prolines, for reasoning about two-dimensional projections obtained through dimensionality reductions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M29S,809,2d,hd,FALSE,https://i.ytimg.com/vi/8eODgQHLo7o/maxresdefault.jpg,,141,1,0,0,0
45,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3GYHRlRHk3w,2016-11-10T11:54:38Z,10/11/16 11:54,On the Intuitiveness of Common Discretization Methods,"Author: Mario Boley, Cluster of Excellence Multimodal Computing and Interaction, Saarland University Abstract: Data discretization methods are usually evaluated in terms of technical criteria that are related to some specific data analysis goal like the preservation of variable interactions. In this paper, we provide a different evaluation principle that assesses the quality of a chosen discretization as the degree to which it coincides with human intuition. This is motivated from the setting of interactive exploratory data analysis where discretizations should be simple, self-explanatory, and fix across results in order to reduce the cognitive load on the user. We present a study design for measuring the intuitive discretization choices of a general human population for a set of discretization problems and present the results of a study trial that we performed with 153 respondents and four problem classes—each using the categories “low”, “normal”, and “high”. Through this trial, we evaluated eight discretization methods from three families: range-based discretization, count-based discretization, and clustering-based discretization. Our results partially confirm results from Cognitive Linguistics that assume prototype-based categorization, which is most closely resembled by clustering-based methods, as a predominant human discretization mechanism. They also show, however, an affinity of participants to sometimes compromise cluster quality in favor of approximating certain category proportions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M,1260,2d,hd,FALSE,https://i.ytimg.com/vi/3GYHRlRHk3w/maxresdefault.jpg,,72,0,0,0,0
46,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Z5CSOxcKBn0,2016-11-10T11:54:04Z,10/11/16 11:54,Expressive Query Construction through Direct Manipulation of Nested Relational Results,"Author: Eirik Bakke, Massachusetts Institute of Technology, MIT Abstract: Despite extensive research on visual query systems, the standard way to interact with relational databases remains to be through SQL queries and tailored form interfaces. We consider three requirements to be essential to a successful alternative: (1) query specification through direct manipulation of results, (2) the ability to view and modify any part of the current query without departing from the direct manipulation interface, and (3) SQL-like expressiveness. This paper presents the first visual query system to meet all three requirements in a single design. By directly manipulating nested relational results, and using spreadsheet idioms such as formulas and filters, the user can express a relationally complete set of query operators plus calculation, aggregation, outer joins, sorting, and nesting, while always remaining able to track and modify the state of the complete query. Our prototype gives the user an experience of responsive, incremental query building while pushing all actual query processing to the database layer. We evaluate our system with formative and controlled user studies on 28 spreadsheet users; the controlled study shows our system significantly outperforming Microsoft Access on the System Usability Scale. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M43S,1183,2d,hd,FALSE,https://i.ytimg.com/vi/Z5CSOxcKBn0/maxresdefault.jpg,,59,0,0,0,0
47,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_UsiwyXYgrk,2016-11-10T11:53:40Z,10/11/16 11:53,Better Machine Learning Through Data,"Author: Saleema Amershi, Microsoft Research Abstract: Machine learning is the product of both an algorithm and data. While machine learning research tends to focus on algorithmic advances, taking the data as given, machine learning practice is quite the opposite. Most of the influence practitioners have in using machine learning to build predictive models comes through interacting with data, including crafting the data used for training and examining results on new data to inform future iterations. In this talk, I will present tools and techniques we have been developing in the Machine Teaching Group at Microsoft Research to support the model building process. I will then discuss some of the open challenges and opportunities in improving the practice of machine learning. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT49M44S,2984,2d,hd,FALSE,https://i.ytimg.com/vi/_UsiwyXYgrk/maxresdefault.jpg,,131,2,0,0,0
48,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5oIHeqJwKcU,2016-11-10T11:53:19Z,10/11/16 11:53,Visual Quality Assessment of Subspace Clusterings,"Author: Michael Hund, Department of Computer and Information Science, University of Konstanz Abstract: The quality assessment of results of clustering algorithms is challenging as different cluster methodologies lead to different cluster characteristics and topologies. A further complication is that in high-dimensional data, subspace clustering adds to the complexity by detecting clusters in multiple different lower-dimensional projections. The quality assessment for (subspace) clustering is especially difficult if no benchmark data is available to compare the clustering results. In this research paper, we present SubEval, a novel subspace evaluation framework, which provides visual support for comparing quality criteria of subspace clusterings. We identify important aspects for evaluation of subspace clustering results and show how our system helps to derive quality assessments. SubEval allows assessing subspace cluster quality at three different granularity levels: (1) A global overview of similarity of clusters and estimated redundancy in cluster members and subspace dimensions. (2) A view of a selection of multiple clusters supports in-depth analysis of object distributions and potential cluster overlap. (3) The detail analysis of characteristics of individual clusters helps to understand the (non-)validity of a cluster. We demonstrate the usefulness of SubEval in two case studies focusing on the targeted algorithm- and domain scientists and show how the generated insights lead to a justified selection of an appropriate clustering algorithm and an improved parameter setting. Likewise, SubEval can be used for the understanding and improvement of newly developed subspace clustering algorithms. SubEval is part of SubVA, a novel open-source web-based framework for the visual analysis of different subspace analysis techniques. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M28S,1108,2d,hd,FALSE,https://i.ytimg.com/vi/5oIHeqJwKcU/maxresdefault.jpg,,90,2,0,0,0
49,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,M-iGd5v_rpg,2016-11-10T11:52:41Z,10/11/16 11:52,Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation,"Author: Charles Sutton, School of Informatics, University of Edinburgh Abstract: A good clustering can help a data analyst to explore and understand a data set, but what constitutes a good clustering may depend on domain-specific and application-specific criteria. These criteria can be difficult to formalize, even when it is easy for an analyst to know a good clustering when she sees one. We present a new approach to interactive clustering for data exploration, called Tinder, based on a particularly simple feedback mechanism, in which an analyst can choose to reject individual clusters and request new ones. The new clusters should be different from previously rejected clusters while still fitting the data well. We formalize this interaction in a novel Bayesian prior elicitation framework. In each iteration, the prior is adapted to account for all the previous feedback, and a new clustering is then produced from the posterior distribution. To achieve the computational efficiency necessary for an interactive setting, we propose an incremental optimization method over data minibatches using Lagrangian relaxation. Experiments demonstrate that Tinder can produce accurate and diverse clusterings. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M22S,1102,2d,hd,FALSE,https://i.ytimg.com/vi/M-iGd5v_rpg/maxresdefault.jpg,,173,3,0,0,0
50,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yI2BCH-jd_g,2016-11-10T11:52:07Z,10/11/16 11:52,Direct-Manipulation Visualization of Deep Networks,"Author: Daniel Smilkov, Google, Inc. Abstract: The recent successes of deep learning have led to a wave of interest from non-experts. Gaining an understanding of this technology, however, is difficult. While the theory is important, it is also helpful for novices to develop an intuitive feel for the effect of different hyperparameters and structural variations. We describe TensorFlow Playground, an interactive, open sourced visualization that allows users to experiment via direct manipulation rather than coding, enabling them to quickly build an intuition about neural nets More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M12S,792,2d,hd,FALSE,https://i.ytimg.com/vi/yI2BCH-jd_g/maxresdefault.jpg,,241,2,0,0,0
51,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0284KOYajoE,2016-11-10T11:03:51Z,10/11/16 11:03,A Unified Inference Framework for Enterprise Users: From Data Storage to Mail,"Author: Byungki Byun, Microsoft More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT4M7S,247,2d,hd,FALSE,https://i.ytimg.com/vi/0284KOYajoE/maxresdefault.jpg,,58,0,0,0,0
52,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4U_E3a35IOc,2016-11-10T11:03:12Z,10/11/16 11:03,Automated analytics: the organizational impact of analytics-as-a-service,"Author: Tine Van Calster, KU Leuven Abstract: Nowadays, many web-based applications are emerging that offer an easy-to-use platform for data analytics to the masses, which usually has fast processing times and large data storage capabilities. These tools fall under the term of 'analytics-as-a-service' (AaaS) and offer an alternative for expensive, in-house analytics infrastructures. Moreover, they might also prove to be interesting in situations where expertise in analytics is not readily available. Through an experimental study, this paper investigates which inexperienced users achieve the best analytical results when using this type of self-service analytics technology. These characteristics range from individual features to the user task approach and the type of task that the novice has to undertake. Furthermore, we also compare the quantitative performance of these inexperienced users to the scores of analytics experts who use the same tool. Our findings indicate that although an expert in analytics still outperforms a novice, we can list some recommendations for selecting the best candidate for performing an analytics task with AaaS, even though he/she does not necessarily have a lot of experience in the field. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT5M41S,341,2d,hd,FALSE,https://i.ytimg.com/vi/4U_E3a35IOc/maxresdefault.jpg,,101,1,0,0,0
53,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0d2SLBm9qPI,2016-11-10T11:02:17Z,10/11/16 11:02,Enterprise Intelligence: Putting the pieces together,"Author: Daniel Tunkelang, Etsy, Inc. Abstract: While consumer applications have become increasingly intelligent, I see three main challenges facing the enterprise. First, most enterprise data still lives in silos, whereas the intelligence comes from joining across data sets. Second, the enterprise suffers from weak signals — there’s little in the way of the labels or behavioral data that consumer application developers take for granted. Third, there’s an incentive problem: everyone promotes data reuse and knowledge sharing, but most organizations don’t reward it. Nonetheless, I see many reasons for hope. Open source and cloud computing have reduces the cost of developing intelligent applications. Consumerization of the enterprise has not only raised UI expectations, but also makes people expect greater interoperability. It’s hard to make broad prescriptions, but I suggested some general approaches in my talk. First, take advantage of opportunities to combine public and enterprise data. Second, invest in data standardization. Third, create better incentives to reward the development of reusable data assets. In short, we need to do a better job of putting the pieces together to enable enterprise intelligence. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT54M24S,3264,2d,hd,FALSE,https://i.ytimg.com/vi/0d2SLBm9qPI/maxresdefault.jpg,,62,0,0,0,0
54,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ov4TWe5jqz4,2016-11-10T11:01:34Z,10/11/16 11:01,Three tools for human in the loop data science,"Author: Aditya Parameswaran, Department of Computer Science, University of Illinois at Urbana-Champaign More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT43M26S,2606,2d,hd,FALSE,https://i.ytimg.com/vi/Ov4TWe5jqz4/maxresdefault.jpg,,296,3,0,0,0
55,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9EnUykV1H9I,2016-11-10T11:00:06Z,10/11/16 11:00,Panel on Enterprise Intelligence,"Moderator: Abhishek Gupta, LinkedIn Corporation Panelists: Igor Perisic, LinkedIn Corporation Josh Wills, Slack Technologies, Inc. George Karypis, Department of Computer Science and Engineering, University of Minnesota Bradford Cross, DCVC Management Co, LLC. Daniel Tunkelang, Etsy, Inc. Aditya Parameswaran, Department of Computer Science, University of Illinois at Urbana-Champaign More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT58M14S,3494,2d,hd,FALSE,https://i.ytimg.com/vi/9EnUykV1H9I/maxresdefault.jpg,,152,2,0,0,0
56,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NfedLJ0dZHM,2016-11-10T10:59:14Z,10/11/16 10:59,Attribute Extraction from Product Titles in eCommerce,"Author: Ajinkya More, Wal-Mart Stores, Inc. Abstract: This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute - brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT5M22S,322,2d,hd,FALSE,https://i.ytimg.com/vi/NfedLJ0dZHM/maxresdefault.jpg,,1437,17,0,0,0
57,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,mp1eNhF6geg,2016-11-10T10:58:37Z,10/11/16 10:58,Towards Data Quality Assessment in Online Advertising,"Author: Sahin Geyik, LinkedIn Corporation More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT5M37S,337,2d,hd,FALSE,https://i.ytimg.com/vi/mp1eNhF6geg/maxresdefault.jpg,,53,0,0,0,0
58,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,cBrHb9gz4Dk,2016-11-10T10:58:06Z,10/11/16 10:58,Rediscovering your customer base using web mining,"Author: Jeremie Dreyfuss, Intel Corporation More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT6M11S,371,2d,hd,FALSE,https://i.ytimg.com/vi/cBrHb9gz4Dk/maxresdefault.jpg,,22,0,0,0,0
59,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,g0EA6eAKIsY,2016-11-10T10:57:34Z,10/11/16 10:57,Multi-label classification and analysis of customer complaint logs,"Author: Tirthankar Dasgupta, Indian Institute of Technology Kharagpur More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT4M48S,288,2d,hd,FALSE,https://i.ytimg.com/vi/g0EA6eAKIsY/maxresdefault.jpg,,1271,3,6,0,0
60,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ra4A5GaL3VE,2016-11-09T15:35:43Z,9/11/16 15:35,Poster session,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT13M39S,819,2d,hd,FALSE,https://i.ytimg.com/vi/ra4A5GaL3VE/maxresdefault.jpg,,30,0,0,0,0
61,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,J9DlzO-W8t0,2016-11-09T15:35:17Z,9/11/16 15:35,Plantation Mapping in Southeast Asia,"Author: Xiaowei Jia, Department of Computer Science and Engineering, University of Minnesota More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M21S,801,2d,hd,FALSE,https://i.ytimg.com/vi/J9DlzO-W8t0/maxresdefault.jpg,,31,1,0,0,1
62,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Eko7x-omLDc,2016-11-09T15:34:32Z,9/11/16 15:34,Modeling the Food-Energy-Water Nexus in Critical Biodiverse Landscapes,"Author: Anuj Karpatne, Department of Computer Science and Engineering, University of Minnesota More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M56S,956,2d,hd,FALSE,https://i.ytimg.com/vi/Eko7x-omLDc/maxresdefault.jpg,,126,0,0,0,0
63,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tK98BtxK4jA,2016-11-09T15:33:48Z,9/11/16 15:33,SmartFarm: Improving Agriculture Sustainability Using Modern Information Technology,"Author: Chandra Krintz, Department of Computer Science, University of California, Santa Barbara More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M54S,774,2d,hd,FALSE,https://i.ytimg.com/vi/tK98BtxK4jA/maxresdefault.jpg,,95,0,0,0,0
64,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,POyYbTDjUx4,2016-11-09T15:29:30Z,9/11/16 15:29,Automated Vegetative Stage Phenotyping Analysis of Maize Plants using Visible Light Images,"Author: Sruti Das Choudhury, University of Nebraska-Lincoln More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M18S,798,2d,hd,FALSE,https://i.ytimg.com/vi/POyYbTDjUx4/maxresdefault.jpg,,116,3,1,0,0
65,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,iHUsas-m1bc,2016-11-09T15:28:28Z,9/11/16 15:28,Opportunities and Challenges for Remote Sensing in Agricultural Applications of Data Science,"Author: Melba Crawford, College of Engineering, Purdue University Abstract: Increases in global population, coupled with challenges of climate change require development of technologies to support increased food production throughout the entire supply chain – from plant breeding to delivery of agricultural products. Developments in remote sensing from space-based, airborne, and proximal sensing platforms, coupled with advanced capabilities in computational platforms and data analytics, are providing new opportunities for contributing solutions to address grand challenges related to food, energy, and water. Spaceborne platforms carrying new active and passive sensors are moving from complex, multi-purpose missions to lower cost, measurement specific constellations of small satellites. Advances in materials are leading to miniaturization and mass production of sensors and supporting instrumentation, resulting in advanced sensing from affordable autonomous vehicles. New algorithms to exploit the massive, multi-modality data sets and provide actionable information for agricultural applications from phenotyping to crop mapping and monitoring are being developed. An overview of recent contributions, as well opportunities and challenges for data science in analysis of multi-temporal, multi-scale multi-sensor remotely sensed data will be presented. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M38S,2738,2d,hd,FALSE,https://i.ytimg.com/vi/iHUsas-m1bc/maxresdefault.jpg,,1786,13,0,0,0
66,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3AMmuimFc-g,2016-11-09T15:27:46Z,9/11/16 15:27,A Bayesian Network approach to County-Level Corn Yield Prediction,"Author: Vikas Chawla, Department of Computer Science, Iowa State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M58S,898,2d,hd,FALSE,https://i.ytimg.com/vi/3AMmuimFc-g/maxresdefault.jpg,,381,5,0,0,0
67,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NlytO3U81T4,2016-11-09T15:26:59Z,9/11/16 15:26,"Satellite Image Analytics, Land Change and Food Security","Author: Sunandan Chakraborty, Courant Institute of Mathematical Sciences, New York University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M8S,968,2d,hd,FALSE,https://i.ytimg.com/vi/NlytO3U81T4/maxresdefault.jpg,,117,0,0,0,0
68,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,13WBfD15VV8,2016-11-09T15:26:20Z,9/11/16 15:26,An end-to-end convolutional selective autoencoder approach to Soybean Cyst Nematode eggs detection,"Author: Adedotun Akintayo, Iowa State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT11M44S,704,2d,hd,FALSE,https://i.ytimg.com/vi/13WBfD15VV8/maxresdefault.jpg,,132,0,0,0,0
69,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,lgoxw9W1dqs,2016-11-09T15:25:28Z,9/11/16 15:25,What spins the turbine?,"Author: Varvara Vetrova, Landcare Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M52S,952,2d,hd,FALSE,https://i.ytimg.com/vi/lgoxw9W1dqs/maxresdefault.jpg,,37,1,0,0,0
70,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,v1TRPSR9vaE,2016-11-09T15:24:50Z,9/11/16 15:24,How Much Water Does Turf Removal Save?,"Author: Christopher Tull, California Data Collaborative (CaDC) More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT10M5S,605,2d,hd,FALSE,https://i.ytimg.com/vi/v1TRPSR9vaE/maxresdefault.jpg,,14,0,0,0,0
71,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,W8EEPEBBuXs,2016-11-09T15:24:10Z,9/11/16 15:24,Automated Sorghum Phenotyping and Trait Development Platform,"Author: Mitch Tuinstra, Purdue University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M35S,755,2d,hd,FALSE,https://i.ytimg.com/vi/W8EEPEBBuXs/maxresdefault.jpg,,63,0,0,0,0
72,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VXiV1dTlRSU,2016-11-09T15:23:09Z,9/11/16 15:23,Predictive Phenomics of Plants,"Author: Patrick S. Schnable, Iowa State University Abstract: Our goal is to develop statistical models that will predict crop performance in diverse agronomic environments. Crop phenotypes such as yield and drought tolerance are controlled by genotype, environment (considered broadly) and their interaction (GxE). As a consequence of the next generation sequencing revolution genotyping data are now available for a wide diversity of accessions in each of the major crops. The necessary volumes of phenotypic data, however, remain limiting and our understanding of molecular basis of GxE is limited. To address this limitation, we are constructing new sensors and robots to automatically collect large volumes of phenotypic data. Two types of high-throughput, high-resolution, field-based phenotyping systems and new sensors will be described. These technologies will be introduced within the context of the Genomes to Fields Initiative. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectu",22,People & Blogs,PT41M11S,2471,2d,hd,FALSE,https://i.ytimg.com/vi/VXiV1dTlRSU/maxresdefault.jpg,,383,9,0,0,1
73,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0EUiAMzmE0c,2016-11-09T15:21:44Z,9/11/16 15:21,Estimating Phenotypic Traits From UAV Based RGB Imagery,"Author: Javier Ribera Prat, School of Electrical and Computer Engineering, Purdue University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT11M27S,687,2d,hd,FALSE,https://i.ytimg.com/vi/0EUiAMzmE0c/maxresdefault.jpg,,196,3,0,0,0
74,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aT-XWu1MNiI,2016-11-09T15:20:59Z,9/11/16 15:20,"A Knowledge Ecosystem for the Food, Energy, and Water System","Author: Praveen Rao, University of Missouri-Kansas City More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT10M58S,658,2d,hd,FALSE,https://i.ytimg.com/vi/aT-XWu1MNiI/maxresdefault.jpg,,39,1,0,0,0
75,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZfrRodD_658,2016-11-09T15:18:02Z,9/11/16 15:18,Predictive Modeling of Sorghum Phenotypes with Airborne Image Features,"Author: Karthikeyan Natesan Ramamurthy, IBM Thomas J. Watson Research Center More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M27S,867,2d,hd,FALSE,https://i.ytimg.com/vi/ZfrRodD_658/maxresdefault.jpg,,42,0,0,0,0
76,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,QcYkmmM4SEw,2016-11-09T15:17:16Z,9/11/16 15:17,"Fast, automated identification of tassels","Author: Nigel Lee, Iowa State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M12S,912,2d,hd,FALSE,https://i.ytimg.com/vi/QcYkmmM4SEw/maxresdefault.jpg,,93,1,0,0,0
77,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,EI8m9goUEVs,2016-11-09T14:16:58Z,9/11/16 14:16,Identifiability of Non-Gaussian Structural VAR Models for Subsampled and Mixed Frequency Time Series,"Author: Alex Tank, Department of Statistics, University of Washington Abstract: Causal inference in multivariate time series is confounded by subsampling in time between the true causal scale and the observed data sampling rate. In practice, this presents challenges for inferring causal interaction between time series due to differences in sampling rates across time series and generally low sampling rates due to technological limitations. To determine instantaneous and lagged effects between time series at the true causal scale, we take a model based approach based on structural vector autoregressive (SVAR) models. We show that when the underlying noise, or shocks, to the system are non-Gaussian, both the parameters of the true model and its causal structure are identifi- able from subsampled and mixed frequency data. Our work builds on the recent work of Gong et al. [1], who established the identifiability of VAR models from subsampled time series with non-Gaussian noise, but with no instantaneous interactions. Here, we generalize their work to the SVAR case to handle instantaneous interactions and, additionally, the multifrequency setting. The resulting approach provides a complete picture of identifiability in non-Gaussian SVAR models under arbitrary mixed frequency subsampling. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M3S,963,2d,hd,FALSE,https://i.ytimg.com/vi/EI8m9goUEVs/maxresdefault.jpg,,632,6,0,0,1
78,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,FuAkMnbnnOo,2016-11-09T14:16:58Z,9/11/16 14:16,Joint Probabilistic Inference of Causal Structure,"Author: Dhanya Sridhar, Department of Computer Science, University of California Santa Cruz Abstract: Causal directed acyclic graphical models (DAGs) are powerful reasoning tools in the study and estimation of cause and effect in scientific and socio-behavioral phenomena. In many domains where the cause and effect structure is unknown, a key challenge in studying causality with DAGs is learning the structure of causal graphs directly from observational data. Traditional approaches to causal structure discovery are categorized as constraint-based or score-based approaches. Score-based methods perform greedy search over the space of models whereas constraint-based methods iteratively prune and orient edges using structural and statistical constraints. However, both types of approaches rely on heuristics that introduce false positives and negatives. In our work, we cast causal structure discovery as an inference problem and propose a joint probabilistic approach for optimizing over model structures. We use a recently introduced and highly efficient probabilistic programming framework known as Probabilistic Soft Logic (PSL) to encode constraint-based structure search. With this novel probabilistic approach to structure discovery, we leverage multiple independence tests and avoid early pruning and variable ordering. We compare our method to the notable PC algorithm on a well-studied synthetic dataset and show improvements in accuracy of predicting causal edges. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M32S,1052,2d,hd,FALSE,https://i.ytimg.com/vi/FuAkMnbnnOo/maxresdefault.jpg,,189,3,0,0,0
79,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,kmckRBTb4rE,2016-11-09T14:16:58Z,9/11/16 14:16,Foundations of Causal Discovery,"Author: Frederick Eberhardt, Division of the Humanities and Social Sciences Abstract: The now widely used theory of causal graphical models considers causal relations among a set of statistical variables. The causal relations are represented in terms of a directed graph among the set of variables, and the task of causal discovery is to identify this causal structure on the basis of the probability distribution generated by the variables in the graph. I will provide an introduction and overview of some of the methods for causal discovery and present known identifiability results with a particular focus on the assumptions they depend on. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT59M30S,3570,2d,hd,FALSE,https://i.ytimg.com/vi/kmckRBTb4rE/maxresdefault.jpg,,728,12,1,0,2
80,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9FlnlaO2M20,2016-11-09T14:16:57Z,9/11/16 14:16,Identifiability and Transportability in Dynamic Causal Networks,"Author: Gilles Blondel, Universitat Politècnica de Catalunya Abstract: In this paper we propose a causal analog to the purely observational Dynamic Bayesian Networks, which we call Dynamic Causal Networks. We provide a sound and complete algorithm for the identification of causal effects in Dynamic Causal Networks, namely, for computing the effect of an intervention or experiment given a Dynamic Causal Network and probability distributions of passive observations of its variables, whenever possible. We note the existence of two types of hidden confounder variables that affect in substantially different ways the identification procedures, a distinction with no analog in either Dynamic Bayesian Networks or standard causal graphs. We further propose a procedure for the transportability of causal effects in Dynamic Causal Network settings, where the result of causal experiments in a source domain may be used for the identi- fication of causal effects in a target domain. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M15S,1035,2d,hd,FALSE,https://i.ytimg.com/vi/9FlnlaO2M20/maxresdefault.jpg,,241,2,1,0,0
81,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,iwSgsqhcS4k,2016-11-09T14:16:57Z,9/11/16 14:16,Evaluating Causal Models by Comparing Interventional Distributions,"Author: Dan Garant, College of Information and Computer Sciences, University of Massachusetts Amherst Abstract: The predominant method for evaluating the quality of causal models is to measure the graphical accuracy of the learned model structure. We present an alternative method for evaluating causal models that directly measures the accuracy of estimated interventional distributions. We contrast such distributional measures with structural measures, such as structural Hamming distance and structural intervention distance, showing that structural measures often correspond poorly to the accuracy of estimated interventional distributions. We use a number of real and synthetic datasets to illustrate various scenarios in which structural measures provide misleading results with respect to algorithm selection and parameter tuning, and we recommend that distributional measures become the new standard for evaluating causal models. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M54S,1314,2d,hd,FALSE,https://i.ytimg.com/vi/iwSgsqhcS4k/maxresdefault.jpg,,43,1,0,0,0
82,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hDaaRm5anfQ,2016-11-09T13:57:46Z,9/11/16 13:57,Urban Computing: Using Big Data to Tackle Urban Challenges,"Author: Yu Zheng, Microsoft Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M23S,983,2d,hd,FALSE,https://i.ytimg.com/vi/hDaaRm5anfQ/maxresdefault.jpg,,230,2,0,0,0
83,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TbS9rCasDzU,2016-11-09T13:57:15Z,9/11/16 13:57,Graph Processing and Mining in the Era of Big Data,"Author: Chengqi Zhang, University of Technology, Sydney More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M7S,1087,2d,hd,FALSE,https://i.ytimg.com/vi/TbS9rCasDzU/maxresdefault.jpg,,92,0,0,0,0
84,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZbWdPfsCqD0,2016-11-09T13:56:45Z,9/11/16 13:56,Mining Brain Networks,"Author: Philip S. Yu, Department of Computer Science, College of Engineering, University of Illinois at Chicago More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M18S,798,2d,hd,FALSE,https://i.ytimg.com/vi/ZbWdPfsCqD0/maxresdefault.jpg,,220,1,0,0,0
85,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,W0F802zt7dU,2016-11-09T13:55:33Z,9/11/16 13:55,People Analytics in Human Resources,"Author: Hui Xiong, Management Science and Information Systems Department, Rutgers, The State University of New Jersey More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M13S,913,2d,hd,FALSE,https://i.ytimg.com/vi/W0F802zt7dU/maxresdefault.jpg,,281,2,2,0,0
86,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,JychF6WgjWk,2016-11-09T13:54:43Z,9/11/16 13:54,Big Data Analytics in Science,"Author: Wei Wang, Computer Science Department, University of California, Los Angeles, UCLA More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M42S,1482,2d,hd,FALSE,https://i.ytimg.com/vi/JychF6WgjWk/maxresdefault.jpg,,942,4,2,0,0
87,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,90wCCsM2oTs,2016-11-09T13:53:22Z,9/11/16 13:53,AMiner: Toward Understanding Big Scholar Data,"Author: Jie Tang, Department of Computer Science and Technology, Tsinghua University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M44S,1424,2d,hd,FALSE,https://i.ytimg.com/vi/90wCCsM2oTs/maxresdefault.jpg,,174,3,0,0,0
88,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,t7q-FVHepnc,2016-11-09T13:53:10Z,9/11/16 13:53,Data Science Tools and Platforms for Business Applications,"Author: Jian Pei, School of Computing Science, Simon Fraser University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M29S,989,2d,hd,FALSE,https://i.ytimg.com/vi/t7q-FVHepnc/maxresdefault.jpg,,183,2,0,0,0
89,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wM99S4JnhtE,2016-11-09T13:49:59Z,9/11/16 13:49,Mining Structures from Massive Text Corpora,"Author: Jiawei Han, Department of Computer Science, University of Illinois at Urbana-Champaign More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT30M47S,1847,2d,hd,FALSE,https://i.ytimg.com/vi/wM99S4JnhtE/maxresdefault.jpg,,915,8,0,0,0
90,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,svx-0DQgRCk,2016-11-09T13:49:29Z,9/11/16 13:49,Network Representation: A Revisit in the Big Data Era,"Author: Peng Cui, Department of Computer Science and Technology, Tsinghua University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M30S,1410,2d,hd,FALSE,https://i.ytimg.com/vi/svx-0DQgRCk/maxresdefault.jpg,,493,2,0,0,0
91,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NSgPK06XgYU,2016-11-09T13:34:13Z,9/11/16 13:34,Bibliometric Ranking of Research Institutions,"Authors: Nachiappan Palaniappan, LatentView Analytics Mohan Manivannan, LatentView Analytics More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M38S,818,2d,hd,FALSE,https://i.ytimg.com/vi/NSgPK06XgYU/maxresdefault.jpg,,162,3,0,0,0
92,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,JsruGwy2LGQ,2016-11-09T13:32:53Z,9/11/16 13:32,Organizer’s Report,"Author: Hao Ma, Microsoft Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT43M49S,2629,2d,hd,FALSE,https://i.ytimg.com/vi/JsruGwy2LGQ/maxresdefault.jpg,,21,0,0,0,0
93,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,S7sss3OuJcU,2016-11-09T13:32:22Z,9/11/16 13:32,Finding Influential Institutions in Bibliographic Information Networks,"Author: Anubhav Gupta, Department of Computer Science and Automation (CSA), Indian Institute of Science Bangalore More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M27S,867,2d,hd,FALSE,https://i.ytimg.com/vi/S7sss3OuJcU/maxresdefault.jpg,,40,0,0,0,0
94,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Plx41Uu56a0,2016-11-09T13:31:48Z,9/11/16 13:31,What’s Next?,"Author: Kuansan Wang, Microsoft Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H3M7S,187,2d,hd,FALSE,https://i.ytimg.com/vi/Plx41Uu56a0/maxresdefault.jpg,,35,0,0,0,0
95,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Jyx-J3pU5pU,2016-11-09T13:31:45Z,9/11/16 13:31,A Simple Method to Predict Affiliation Ranking in KDDCup 2016,"Author: Ping-I Chou, Trend Micro Inc. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT4M36S,276,2d,hd,FALSE,https://i.ytimg.com/vi/Jyx-J3pU5pU/maxresdefault.jpg,,65,0,0,0,0
96,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4jjMnyaOlVY,2016-11-09T13:31:11Z,9/11/16 13:31,Prediction of Research Impact,"Author: Keita Yokoyama, NTT DOCOMO, Inc. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT3M8S,188,2d,hd,FALSE,https://i.ytimg.com/vi/4jjMnyaOlVY/maxresdefault.jpg,,97,0,0,0,0
97,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UnLUqsk1We4,2016-11-09T13:30:32Z,9/11/16 13:30,Predicting Institution-Level Paper Acceptance at Conferences,"Author: Jianjun Xie, CoreLogic More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M8S,908,2d,hd,FALSE,https://i.ytimg.com/vi/UnLUqsk1We4/maxresdefault.jpg,,27,0,0,0,0
98,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,V3UaEUoZ2SQ,2016-11-09T13:29:59Z,9/11/16 13:29,Ranking academic institutions on potential paper acceptance in upcoming conferences,"Author: Jobin Wilson, Flytxt More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M7S,1147,2d,hd,FALSE,https://i.ytimg.com/vi/V3UaEUoZ2SQ/maxresdefault.jpg,,69,3,1,0,0
99,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,AJ1VtgHOvZo,2016-11-09T13:28:37Z,9/11/16 13:28,Heterogenous Graph Mining for Measuring the Impact of Research Institutions,"Author: Shuang Qiu, University of Michigan More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT8M29S,509,2d,hd,FALSE,https://i.ytimg.com/vi/AJ1VtgHOvZo/maxresdefault.jpg,,85,0,0,0,1
100,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,siE5_b1DQ2Q,2016-11-09T13:28:04Z,9/11/16 13:28,Ranking Research Institutions Based On Relative Academic Conferences,"Author: Leili Tavabi, Intel Corporation",22,People & Blogs,PT5M34S,334,2d,hd,FALSE,https://i.ytimg.com/vi/siE5_b1DQ2Q/maxresdefault.jpg,,334,4,0,0,1
101,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,bnU2bcavc5c,2016-11-09T13:27:41Z,9/11/16 13:27,Regressing to Regression - Predicting Conference Paper Acceptance,"Author: Joel Schlosser, Georgia Institute of Technology",22,People & Blogs,PT13M37S,817,2d,hd,FALSE,https://i.ytimg.com/vi/bnU2bcavc5c/maxresdefault.jpg,,53,0,0,0,0
102,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xdmCGDNZacw,2016-11-09T13:27:12Z,9/11/16 13:27,Predicting the future relevance of research institutions,"Author: Vlad Sandulescu, Adform More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M39S,939,2d,hd,FALSE,https://i.ytimg.com/vi/xdmCGDNZacw/maxresdefault.jpg,,70,0,0,0,0
103,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,s4fwk9rhs10,2016-11-09T13:26:42Z,9/11/16 13:26,Feature Engineering and Ensemble Modeling for Paper Acceptance Rank Prediction,"Author: Yujie Qian, Tsinghua University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M11S,731,2d,hd,FALSE,https://i.ytimg.com/vi/s4fwk9rhs10/maxresdefault.jpg,,219,0,0,0,0
104,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ertcrgT0U8I,2016-11-09T13:26:10Z,9/11/16 13:26,Winners Panel,"Panelists: Yujie Qian, Tsinghua University Vlad Sandulescu, Adform Mohan Manivannan, LatentView Analytics More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT38M27S,2307,2d,hd,FALSE,https://i.ytimg.com/vi/ertcrgT0U8I/maxresdefault.jpg,,248,1,0,0,0
105,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,iMAALarIFdI,2016-11-09T13:02:26Z,9/11/16 13:02,Multi-Task Label Propagation with Dissimilarity Measures,"Author: Marco Frasca, Dipartimento di Scienze dell'Informazione, Università Degli Studi Di Milano Abstract: Multi-task algorithms typically use task similarity information as a bias to speed up learning. We argue that, when the classification problem is unbalanced, task dissimilarity information provides a more effective bias, as rare class labels tend to be better separated from the frequent class labels. In particular, we show that a multi-task extension of the label propagation algorithm for graph-based classification works much better on protein function prediction problems when the task relatedness information is represented using a dissimilarity matrix as opposed to a similarity matrix. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M26S,1646,2d,hd,FALSE,https://i.ytimg.com/vi/iMAALarIFdI/maxresdefault.jpg,,186,0,0,0,0
106,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MQZoD1St1Jo,2016-11-09T13:01:40Z,9/11/16 13:01,Multiple network alignment via multiMAGNA++,"Author: Vipin Vijayan, Department of Computer Science and Engineering, University of Notre Dame Abstract: Network alignment (NA) aims to find a node mapping between molecular networks of different species that identifies topologically or functionally similar network regions. Analogous to genomic sequence alignment, NA can be used to transfer biological knowledge from well- to poorly-studied species between aligned network regions. Pairwise NA (PNA) finds similar regions between two networks while multiple NA (MNA) can align more than two networks. We focus on MNA. Existing MNA methods aim to maximize total similarity over all aligned nodes (node conservation). Then, they evaluate alignment quality by measuring the amount of conserved edges, but only after the alignment is constructed. Directly optimizing edge conservation during alignment construction in addition to node conservation may result in superior alignments. Thus, we present a novel MNA approach called multiMAGNA++ that can achieve this. Indeed, multiMAGNA++ generally outperforms or is on par with the existing MNA methods, while often completing faster than the existing methods. That is, multiMAGNA++ scales well to larger network data and can be parallelized effectively. During method evaluation, we also introduce new MNA quality measures to allow for more complete alignment characterization as well as more fair MNA method comparison compared to using only the existing alignment quality measures. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M30S,1350,2d,hd,FALSE,https://i.ytimg.com/vi/MQZoD1St1Jo/maxresdefault.jpg,,100,0,0,0,0
107,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Mcud4yuH9dM,2016-11-09T13:01:03Z,9/11/16 13:01,Deep Learning for Connectomicss,"Author: Shuiwang Ji, School of Electrical Engineering and Computer Science, Washington State University Abstract: The importance of research that aims to unlock the mystery of the human brain has recently been recognized worldwide. In January 2013, the European Union selected the Human Brain Project to be one of its two flagship projects. In April 2013, the White House announced the Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative to generate a dynamic map of the brain. As these projects move forward, big data analytics will be playing increasingly important roles in converting big brain data into useful knowledge. A key challenge in analyzing brain data is to construct feature representations from brain images. In this talk, I will discuss our efforts in developing deep computational models for learning representations from brain data. In particular, I will provide details on how to use deep learning methods to elucidate the micro-scale brain connectomics among neurons. I will also show that our methods can be used in a number of diverse computational brain discovery tasks. Additionally, they may be used in other areas beyond brain analytics. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT47M12S,2832,2d,hd,FALSE,https://i.ytimg.com/vi/Mcud4yuH9dM/maxresdefault.jpg,,208,1,0,0,0
108,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ALu-46sI0KA,2016-11-09T13:00:24Z,9/11/16 13:00,Deep‐Learning: Investigating feed‐forward Deep Neural Networks,"Author: Jun (Luke) Huan, Department of Electrical Engineering and Computer Science, University of Kansas Abstract: In recent years, research in Artificial Neural Networks (ANNs) has resurged, now under the Deep-Learning umbrella, and grown extremely popular due to major breakthroughs in methodological and computing capabilities. Deep-Learning methods are part of representation-learning algorithms that attempt to extract and organize discriminative information from the data. Recently reported success of DL techniques in crowd-sourced QSARs and predictive toxicology competitions has showcased these methods as powerful tools for drug-discovery and toxicology research. Nevertheless, reported applications of Deep Learning techniques for modeling complex bioactivity data for small molecules remain still limited. In this talk I will present our recent work on optimizing feed-forward Deep Neural Nets (DNNs) hyper-parameters and performance evaluation of these methods as compared to shallow methods. In our study 48 DNNs, 24 Random Forest, 20 SVM and 6 Naïve Bayes arbitrary but reasonably selected configurations were compared employing 7 diverse bioactivity datasets assembled from ChEMBL repository combined with circular fingerprints as molecular descriptors. The non-parametric Wilcoxon paired singed-rank test was employed to compare the performance of DNN to RF, SVM and NB. Overall it was found that DNNs with 2 hidden layers, 2,000 neurons per each hidden layer, ReLU activation function and Dropout regularization technique achieved strong classification performance across all tested datasets. Our results demonstrate that DNNs are powerful modeling techniques for modeling complex bioactivity data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT51M29S,3089,2d,hd,FALSE,https://i.ytimg.com/vi/ALu-46sI0KA/maxresdefault.jpg,,133,0,0,0,0
109,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,QWuUCUhbf4Q,2016-11-09T12:58:32Z,9/11/16 12:58,Discovery of Functional Motifs from the Interface Region of Oligomeric Proteins,"Author: Mohammad Al Hasan, Indiana University-Purdue University Indianapolis Abstract: Studying the interface region of a protein complex pavesthe way for understanding its dynamics and functionalities.Existing works study a protein interface region by the com-position of its residues, by the geometry of the interfaceresidues, or by directly aligning interface regions. Very fewworks use graphs as the tool for modeling the interface re-gions. In this work, we use interface residues for formingnetworks from a set of protein structures, and then ﬁnd sub-graphs that are frequent in those networks. For ﬁnding suchsubgraphs, we use a scalable frequent subgraph mining al-gorithm, which can mine frequent sub-network patterns ofa speciﬁc size. We then discover the functional motif alongthe interface region of a given protein from those mined sub-graphs. In our experiment, we use PDB structures from twodimeric protein complexes: HIV-1 protease (329 structures)and triosephosphate isomerase (TIM) (86 structures). Theproposed frequent subgraph based approach discovers thegraphs representing the dimerization lock which is formed atthe base of the structure, in 323 of the 329 HIV-1 proteasestructures. Similarly, for 86 TIM structures, the approachdiscovers the dimerization lock formation in 50 structures.Our method captures the locking mechanism at the dimericinterface by taking into account the spatial positioning ofthe interfacial residues through graphs. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M52S,1252,2d,hd,FALSE,https://i.ytimg.com/vi/QWuUCUhbf4Q/maxresdefault.jpg,,180,0,0,0,0
110,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,kAFc01iDqOM,2016-11-09T12:37:13Z,9/11/16 12:37,Incremental Method for Spectral Clustering of Increasing Orders,"Author: Pin-Yu Chen, IBM Thomas J. Watson Research Center Abstract: The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) of a graph Laplacian matrix have been widely used for spectral clustering and community detection. However, in real-life applications the number of clusters or communities (say, K) is generally unknown a-priori. Consequently, the majority of the existing methods either choose K heuristically or they repeat the clustering method with different choices of K and accept the best clustering result. The first option, more often, yields suboptimal result, while the second option is computationally expensive. In this work, we propose an incremental method for constructing the eigenspectrum of the graph Laplacian matrix. This method leverages the eigenstructure of graph Laplacian matrix to obtain the K-th eigenpairs of the Laplacian matrix given a collection of all the $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix such that the batch eigenvalue decomposition problem transforms into an efficient sequential leading eigenpair computation problem. As a practical application, we consider user-guided spectral clustering. Specifically, we demonstrate that users can utilize the proposed incremental method for effective eigenpair computation and determining the desired number of clusters based on multiple clustering metrics. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M18S,558,2d,hd,FALSE,https://i.ytimg.com/vi/kAFc01iDqOM/maxresdefault.jpg,,170,0,0,0,0
111,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Xpx5RYNTQvg,2016-11-09T12:36:29Z,9/11/16 12:36,Serving a Billion Personalized News Feeds,"Author: Lars Backstrom, Facebook Abstract: Feed ranking's goal is to provide people with over a billion personalized experiences. We strive to provide the most compelling content to each person, personalized to them so that they are most likely to see the content that is most interesting to them. Similar to a newspaper, putting the right stories above the fold has always been critical to engaging customers and interesting them in the rest of the paper. In feed ranking, we face a similar challenge, but on a grander scale. Each time a person visits, we need to find the best piece of content out of all the available stories and put it at the top of feed where people are most likely to see it. To accomplish this, we do large-scale machine learning to model each person, figure out which friends, pages and topics they care about and pick the stories each particular person is interested in. In addition to the large-scale machine learning problems we work on, another primary area of research is understanding the value we are creating for people and making sure that our objective function is in alignment with what people want. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT39M50S,2390,2d,hd,FALSE,https://i.ytimg.com/vi/Xpx5RYNTQvg/maxresdefault.jpg,,8675,119,3,0,1
112,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-11-09T12:35:53Z,9/11/16 12:35,Communities and Anomalies in Attributed Networks,"Author: Leman Akoglu, Computer Science Department, Stony Brook University Abstract: Given a network in which nodes are associated with a list of attributes, how can we define and characterize communities? How can we spot anomalous communities and anomalies within communities? Networks have long been studied and focus has most recently shifted to 'networks with content'. Long-studied network questions, such as ranking, clustering, and similarity, are reconsidered for such networks, as the new information such as node/edge attributes and types help enrich the formulations and increase our understanding of real-world networks. In this talk, I will introduce our work on spotting anomalies in networks with node attributes. Our main approach to anomaly mining in attributed networks is through communities. In particular, we quantify the degree that a community can be characterized through (a subset of) attributes on which its members 'click'. We then use such a quantity as a 'normality' score, based on which we identify individual anomalous nodes inside communities as well as communities that are anomalous as a group of nodes due to their low normality. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT29M23S,1763,2d,hd,FALSE,https://i.ytimg.com/vi/-AwbFbjehpQ/maxresdefault.jpg,,352,3,0,0,0
113,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,uCyUcf-Stt8,2016-11-09T12:35:08Z,9/11/16 12:35,Exploiting the Computation Graph for Large Scale Distributed Machine Learning,"Author: S.V.N. Vishwanathan, Department of Computer Science, University of California Santa Cruz Abstract: Many machine learning algorithms minimize a regularized risk. It is well known that stochastic optimization algorithms are both theoretically and practically well motivated for regularized risk minimization. Unfortunately, stochastic optimization is not easy to parallelize. In this talk, we take a radically new approach and show that working with the saddle-point problem that arises out of the Lagrangian has a very specific computational graph structure which can be exploited to allow for a natural partitioning of the parameters across multiple processors. This allows us to derive a new parallel stochastic optimization algorithm for regularized risk minimization. Joint work with: Inderjit Dhillon, Cho-Jui Hsieh, Shihao Ji, Shin Matsushima, Parameshwaran Raman, Hsiang-Fu Yu, and Hyokun Yun. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT31M43S,1903,2d,hd,FALSE,https://i.ytimg.com/vi/uCyUcf-Stt8/maxresdefault.jpg,,209,1,0,0,0
114,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8XbHyCAXbtI,2016-11-09T12:34:21Z,9/11/16 12:34,Node Representation in Mining Heterogeneous Information Networks,"Author: Yizhou Sun, Computer Science Department, University of California, Los Angeles, UCLA Abstract: One of the challenges in mining information networks is the lack of intrinsic metric in representing nodes into a low dimensional space, which is essential in many mining tasks, such as recommendation and anomaly detection. Moreover, when coming to heterogeneous information networks, where nodes belong to different types and links represent different semantic meanings, it is even more challenging to represent nodes properly. In this talk, we will focus on two mining tasks, i.e., (1) content-based recommendation and (2) anomaly detection in heterogeneous categorical events, and introduce (1) how to represent nodes when different types of nodes and links are involved; and (2) how heterogeneous links play different roles in these tasks. Our results have demonstrated the superiority as well as the interpretability of these new methodologies. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT34M26S,2066,2d,hd,FALSE,https://i.ytimg.com/vi/8XbHyCAXbtI/maxresdefault.jpg,,2516,19,0,0,1
115,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8Q75cIT2g34,2016-11-09T12:33:43Z,9/11/16 12:33,Measuring Graph Proximity with Blink Model,"Author: Haifeng Qian, IBM Thomas J. Watson Research Center Abstract: This paper proposes a new graph proximity measure. This measure is a derivative of network reliability. By analyzing its properties and comparing it against other proximity measures through graph examples, we demonstrate that it is more consistent with human intuition than competitors. A new deterministic algorithm is developed to approximate this measure with practical complexity. Empirical evaluation by two link prediction benchmarks, one in coauthorship networks and one in Wikipedia, shows promising results. For example, a single parameterization of the proposed measure achieves accuracies that are 14-35% above the best accuracy for each graph of all predictors reported in the 2007 Liben-Nowell and Kleinberg survey. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT7M6S,426,2d,hd,FALSE,https://i.ytimg.com/vi/8Q75cIT2g34/maxresdefault.jpg,,48,2,0,0,0
116,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yUB09bBsiKQ,2016-11-09T12:33:00Z,9/11/16 12:33,Statistical Methods for Modeling Network Distributions,"Author: Jennifer Neville, Computer Science Department, Purdue University Abstract: The recent interest in analyzing the network structure of complex systems has fueled a large body of research on both models of network structure and algorithms to automatically discover patterns for use in predictive models. However, robust statistical models, which can accurately represent distributions over graph populations, and sample efficiently from those distributions, are critical to assess the evaluate the performance of analytic algorithms and the significance of discovered patterns. However, unlike metric spaces, the space of graphs exhibits a combinatorial structure that poses significant theoretical and practical challenges to accurate estimation and efficient sampling/inference. In this talk, I will discuss our recent work on modeling distributions of networks, both attributed and unattributed, and outline how the methods can be used for inference and evaluation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT41M55S,2515,2d,hd,FALSE,https://i.ytimg.com/vi/yUB09bBsiKQ/maxresdefault.jpg,,197,2,0,0,0
117,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,SKTNR5hBBnU,2016-11-09T12:32:14Z,9/11/16 12:32,Correctly Modeling Networks,"Author: Tamara Kolda, Sandia National Laboratories Abstract: Understanding and modeling go hand in hand – we develop models not only to make predictions but also to see where the models fail and there is more to do. Large-scale networks are immensely challenging to model mathematically. In this talk, we present our arguments for what features are important to measure and reproduce. In the undirected case, we show that graphs with high clustering coefficients (i.e., many triangles) must have dense Erdȍs-Rényi subgraphs. This is a key theoretical finding that may yield clues in understanding network structure. Following this line, we propose the Block Two-level Erdȍs-Rényi (BTER) model because it reproduces a given degree distribution and clustering coefficient profile (i.e., the triangle distribution), scales linearly in the number of edges, and is easily parallelized. We also consider the extension of this work to bipartite graphs, where we consider bipartite four-cycles, and propose a bipartite BTER (biBTER) model. These models can be used to generate artificial graphs that capture salient features of real graphs. We compare the artificial and real-world graphs so that we can understand where the models are accurate or not. Time permitting, we also explain how these models can be specified with very few parameters, which is useful for benchmarking purposes. We close with open questions for future investigations. This is joint work with S. Aksoy, A. Pinar, T. Plantenga, and C. Seshadhri. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT34M56S,2096,2d,hd,FALSE,https://i.ytimg.com/vi/SKTNR5hBBnU/maxresdefault.jpg,,139,0,0,0,0
118,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wwcniYFNrdI,2016-11-09T12:31:27Z,9/11/16 12:31,Scaling Overlapping Clustering,"Author: Kyle Kloster, NC State University Abstract: Identifying communities plays a central role in understanding the structure of large networks. As practitioners analyze progressively larger networks, it becomes increasingly important to understand the computational complexity of candidate algorithms. We examine the complexity of the link clustering algorithm for overlapping community detection. We provide new, tight bounds for the original implementation and propose modifications to reduce algorithmic complexity. These new bounds are a function of the number of wedges in the graph. Additionally, we demonstrate that for several community detection algorithms, wedges predict runtime better than commonly cited graph features. We conclude by proposing a method to reduce the wedges in a graph by removing high-degree vertices from the network, identifying communities with an optimized version of link clustering, and heuristically matching communities with the removed vertices as a post-processing step. We empirically demonstrate a large reduction in processing time on several common data sets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M5S,725,2d,hd,FALSE,https://i.ytimg.com/vi/wwcniYFNrdI/maxresdefault.jpg,,218,1,0,0,0
119,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gtwTsNedZJs,2016-11-09T12:29:35Z,9/11/16 12:29,Paper Spotlights 2,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT16M6S,966,2d,hd,FALSE,https://i.ytimg.com/vi/gtwTsNedZJs/maxresdefault.jpg,,14,0,0,0,0
120,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XIIpNl-_POA,2016-11-09T12:29:06Z,9/11/16 12:29,Paper Spotlights 1,More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/,22,People & Blogs,PT12M9S,729,2d,hd,FALSE,https://i.ytimg.com/vi/XIIpNl-_POA/maxresdefault.jpg,,32,0,0,0,0
121,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,QtEQ-tTqowI,2016-11-09T12:28:17Z,9/11/16 12:28,Distance-Based Influence in Networks: Computation and Maximization,"Author: Edith Cohen, Google, Inc. Abstract: A premise at a heart of network analysis is that entities in a network derive utilities from their connections. The {\em influence} of a seed set $S$ of nodes is defined as the sum over nodes $j$ of the {\em utility} of $S$ to $j$. {\em Distance-based} utility, which is a decreasing function of the distance from $S$ to $j$, was explored in several successful research threads from social network analysis and economics: Network formation games [Bloch and Jackson 2007], Reachability-based influence [Richardson and Domingos 2002; Kempe et al. 2003]; ``threshold'' influence [Gomez-Rodriguez et al. 2011]; and {\em closeness centrality} [Bavelas 1948]. We formulate a model that unifies and extends this previous work and address the two fundamental computational problems in this domain: {\em Influence oracles} and {\em influence maximization} (IM). An oracle performs some preprocessing, after which influence queries for arbitrary seed sets can be efficiently computed. With IM, we seek a set of nodes of a given size with maximum influence. Since the IM problem is computationally hard, we instead seek a {\em greedy sequence} of nodes, with each prefix having influence that is at least $1-1/e$ of that of the optimal seed set of the same size. We present the first highly scalable algorithms for both problems, providing statistical guarantees on approximation quality and near-linear worst-case bounds on the computation. We perform an experimental evaluation which demonstrates the effectiveness of our designs on networks with hundreds of millions of edges. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M,780,2d,hd,FALSE,https://i.ytimg.com/vi/QtEQ-tTqowI/maxresdefault.jpg,,215,1,0,0,0
122,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,25vQknkuvUs,2016-11-09T11:17:24Z,9/11/16 11:17,Distributed Learning Dynamics Convergence in Routing Games,"Author: Alexandre Bayen, Department of Electrical Engineering and Computer Sciences, UC Berkeley Abstract: With the emergence of smartphone based sensing for mobility as the main paradigm for sensing in the last decade, radically new information sets have become available for the driving public. This information enables commuters to make repeated decisions on a daily basis based on anticipated state of the network. This repeated decision-making process creates interesting patterns for the transportation network, in which users might (or might not) reach an equilibrium, depending on the information at their disposal (for example knowing or not what other users of the network are experiencing or doing). The present talk starts with a brief presentation of the state of the art in traffic monitoring, leading to a new results in routing games. Routing games offer a simple yet powerful model of congestion in traffic networks, both in transportation and communication systems. The congestion in such systems is affected by the combined decision of the agents (drivers or routers), so modeling the decision process of the agents is important, not only to estimate and predict the behavior of the system, but also to be able to control it. This decision process is often called learning, as agents ""learn"" information about the system or about the other agents. We propose and study different models of learning with the following requirement: the joint learning dynamics should converge asymptotically to the Nash equilibrium of the game. In particular, we focus on two important properties: Is the model robust to stochastic perturbations (such as measurement noise)? And does the model allow heterogeneous learning (different agents may follow different learning strategies)? We study these questions using tools from online learning theory and stochastic approximation theory. We then present experimental results obtained with an online gaming application in which distributed players can play the routing game: they connect to the web app and participate in the game, by iteratively making decisions about their routes and observing outcomes. We show preliminary results from data collected from the application. In particular, we propose and solve a model estimation problem to estimate the learning dynamics of the players, and compare the predictions of the model to the actual behavior of the players, and discuss extensions and open questions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT43M24S,2604,2d,hd,FALSE,https://i.ytimg.com/vi/25vQknkuvUs/maxresdefault.jpg,,54,0,0,0,0
123,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,JLcd4MGvpu8,2016-11-09T11:16:35Z,9/11/16 11:16,Robustness and Resilience of cities around the world,"Author: Sofiane Abbar, Qatar Computing Research Institute Abstract: The concept of city or urban resilience has emerged as one of the key challenges for the next decades. As a consequence, institutions like the United Nations or Rockefeller Foundation have embraced initiatives that increase or improve it. These efforts translate into funded programs both for action “on the ground” and to develop quantification of resilience, under the for of an index. Ironically, on the academic side there is no clear consensus regarding how resilience should be quantified, or what it exactly refers to in the urban context. Here we attempt to link both extremes providing an example of how to exploit large, publicly available, worldwide urban datasets, to produce objective insight into one of the possible dimensions of urban resilience. We do so via well-established methods in complexity science, such as percolation theory –which has a long tradition at providing valuable information on the vulnerability in complex systems. Our findings uncover large differences among studied cities, both regarding their infrastructural fragility and the imbalances in the distribution of critical services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M40S,820,2d,hd,FALSE,https://i.ytimg.com/vi/JLcd4MGvpu8/maxresdefault.jpg,,235,1,1,0,0
124,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8WXqK8HYM0M,2016-11-09T11:15:39Z,9/11/16 11:15,Reducing Inefficiencies in Taxi Systems,"Author: Chenguang Zhu, Microsoft Abstract: Taxi systems are perfect examples of supply-demand systems in which taxi vehicles and drivers constitute the supply side, while passengers hailing taxis are the demand side. However, various inefficiencies can be embedded within such a large-scale system, e.g. an excessive number of taxi vehicles, a shortage of taxi supplies after an event and long idle times with no passengers in taxis. These systemic ineffi- ciencies are often overlooked in previous literature, which focuses on taxi dispatching mechanisms to satisfy shortterm demand. In this paper, we address these inefficiencies and propose a novel model for the trip assignment problem based on network flow. Compared with existing methods, our model is much more scalable. This model is capable of assigning hundreds of thousands of trips to taxis over a long time interval, e.g. a shift of 12 hours. Furthermore, the trip assignment given by this model can effectively minimize the total number of required taxis while reducing incurred idle time. Experiments show that in our model, the number of required taxis to finish all observed trips in New York City is only 72% of the size of the current taxi fleet, while the average idle time incurred per taxi drops by 32%. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M10S,1150,2d,hd,FALSE,https://i.ytimg.com/vi/8WXqK8HYM0M/maxresdefault.jpg,,111,0,0,0,0
125,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YeH2nIQzYrY,2016-11-09T11:14:50Z,9/11/16 11:14,Urban Computing: Using Big Data to Sovle Urban Challenges Spearker,"Author: Yu Zheng, Microsoft Research Abstract: Urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in urban spaces, such as sensors, devices, vehicles, buildings, and human, to tackle the major issues that cities face, e.g. air pollution, increased energy consumption and traffic congestion. Urban computing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create win-win-win solutions that improve urban environment, human life quality, and city operation systems. This talk will present our recent research into urban computing, introducing the applications and technologies for integrating and deep mining heterogeneous data. Examples include large-scale dynamic ridesharing, identifying different functional regions (e.g., residential and commercial areas) in urban spaces, crowd sending city-wide gas consumption, and fine-grained air quality inference throughout a city. These examples have been published in top-tier conferences and journals recently, such as KDD, UbiComp, ICDE, received 4 best paper awards. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT46M29S,2789,2d,hd,FALSE,https://i.ytimg.com/vi/YeH2nIQzYrY/maxresdefault.jpg,,545,4,0,0,0
126,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wZVhfKf9e9I,2016-11-09T11:13:45Z,9/11/16 11:13,Estimating Evacuation Hotspots using GPS data,"Author: Takahiro Yabe, Institute of Industrial Science (ISS), University of Tokyo Abstract: Kumamoto prefecture, Japan, was hit by enormous (Magnitude 6.5, 7.3) earthquakes on 14th and 16th of April, 2016. As a result of the shocks, more than 10,000 buildings were severely damaged, and over 100,000 people had to evacuate away from their homes. After the earthquake, it took the authorities several days to grasp the locations where people were evacuating, which delayed the distribution of supply and rescue teams. This situation was made even harder since some people evacuated to places that were not designated as evacuation shelters. Conventional methods for grasping evacuation hotspots require field surveys, which take time and are also difficult to execute right after the hazard in the confusion. We propose a framework to efficiently estimate the evacuation hotspots using location data collected from mobile phones. To validate our framework, we estimated the locations that were congested with evacuees after the Kumamoto earthquake using GPS data collected by Yahoo! Japan. We also verified that our estimation results were very high, by checking the features located in each grid with high anomaly value. Moreover, for one of the non-designated evacuation hotspots, we accurately estimated the population transition of before and after the earthquake, which we validated using newspaper reports. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M59S,599,2d,hd,FALSE,https://i.ytimg.com/vi/wZVhfKf9e9I/maxresdefault.jpg,,55,0,0,0,0
127,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NJ8gOI7aFcg,2016-11-09T11:12:33Z,9/11/16 11:12,Urban Simulation and Visualization,"Author: Paul Waddell, College of Environmental Design, UC Berkeley More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M12S,3012,2d,hd,FALSE,https://i.ytimg.com/vi/NJ8gOI7aFcg/maxresdefault.jpg,,462,3,2,0,1
128,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BPkiG8QBmCI,2016-11-09T11:04:58Z,9/11/16 11:04,Disaggregating Appliance-Level Energy Consumption: A Probabilistic Framework,"Author: Sabina Tomkins, Jack Baskin School of Engineering, University of California Santa Cruz Abstract: In this work we propose a probabilistic disaggregation framework which can determine the energy consumption of individual electrical appliances from aggregate power readings. Our proposed framework uses probabilistic soft logic (PSL), to define a hinge-loss Markov random field (HL-MRF). Our method is novel in that it can integrate a diverse range of features, is highly scalable to any number of appliances, and makes less assumptions than existing methods. As the residential sector is responsible for over a third of all electricity demand, and delivering appliance level energy consumption information to consumers has been demonstrated to reduce electricity consumption, our framework has the potential to make a significant impact on energy savings. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M15S,555,2d,hd,FALSE,https://i.ytimg.com/vi/BPkiG8QBmCI/maxresdefault.jpg,,518,3,0,0,0
129,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,JIYfzI_GfPM,2016-11-09T11:04:01Z,9/11/16 11:04,Byproducts of Urban Infrastructure Interfaces: Evidence from Parking Compliance,"Author: Hyun Ah Song, Machine Learning Department, Carnegie Mellon University Abstract: The increased levels of urbanization have resulted in the demand for developing urban technologies that can realize the vision of smart cities, i.e., urban environments that are sustainable, livable and resilient. Electromechanical infrastructure is substituted by intelligent, cyber-physical infrastructure (e.g., coin-based ticket fare collectors are substituted by smart cards) in an effort to both reduce costs, increase efficiency as well as improve the user-friendliness of the system. Significant efforts and resources have been allocated in the area of public transportation, including the modernization of subway and bus networks. However, one of the most-discussed aspects of public transportation in our automobile-dominated cities is that of parking infrastructure. While research has concluded that appropriate pricing of metered parking zones is essential to allow local businesses to flourish and even reduce congestion, there is still a lot of hesitance on implementing the appropriate policies. Hence, parking zones are still significantly underpriced. The problem is further pronounced by poor enforcement. However, during the last years most of the coin-based parking meters are being substituted by “smart” meters that accept various types of payments (e.g., credit cards, mobile etc.). While these meters have been installed to mainly make parking payments more convenient to drivers, they appear to have important indirect benefits. In particular, in this study we use quasi-experimental techniques to analyze parking citation information from the city of Pittsburgh and we find that the installation of the new parking meters leads to increased compliance with parking rules. This can further have significant implication for the design of the urban infrastructure interfaces of the upcoming smart technology. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M,540,2d,hd,FALSE,https://i.ytimg.com/vi/JIYfzI_GfPM/maxresdefault.jpg,,23,0,0,0,0
130,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,FudfZGxr1qM,2016-11-09T11:02:55Z,9/11/16 11:02,On the Dominant Role of Returners’ Human Mobility Networks on Urban Energy Consumption,"Author: Neda Mohammadi, Virginia Polytechnic Institute and State University Abstract: As a result of population growth and urbanization, the interdependencies between infrastructure, services, and individuals in urban areas continue to increase. Urban areas already consume up to 80% of the world’s energy, and the expected population increase of nearly 70% by 2050 will drive a further rise in energy consumption. It is, therefore, vital for us to develop a better understanding of variabilities in human-related effects on buildings’ energy consumption within the urban spatial context in which they exist. Intra-city trips of urban population are undertaken as a result of individuals engaging in activities across various locations. However, people exhibit variations in their daily activities and the number of locations they visit over time. Here, we investigate the spatial interdependencies between human mobility networks of two distinct populations (i.e., returners and explorers) as an indicator of their daily activity patterns, as well as gas consumption to explore how variations in human mobility networks can be used to explain spatial fluctuations in energy use. We compare 2,015,339 positional records from an online social networking platform, Twitter, with energy consumption (gas) across 983 areas in Greater London over the course of a single month (May 2014). Our findings indicate a stronger statistically significant spatial dependency between human mobility networks of the returners and gas consumption, indicating domination of this population in urban energy use. This suggests that spatial fluctuations in urban energy consumption are governed by the structure of human mobility networks, among other factors. These results provide a clear picture of demand-side diversity and its drivers, establishing a foundation for human mobility-based predictive models for urban energy consumption. The relationship between energy consumption and human mobility is key to creating effective policies for urban areas, leading to more reliable predictions and effective management decisions about future patterns of energy use. Our findings will be of value to urban planners, researchers and policy-makers. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT8M5S,485,2d,hd,FALSE,https://i.ytimg.com/vi/FudfZGxr1qM/maxresdefault.jpg,,75,1,0,0,0
131,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UrBrzrdaAic,2016-11-09T11:02:05Z,9/11/16 11:02,Toward Semantic Understanding of Mobility Data,"Author: Zhenhui Jessie Li, College of Information Sciences and Technology, Pennsylvania State University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT36M32S,2192,2d,hd,FALSE,https://i.ytimg.com/vi/UrBrzrdaAic/maxresdefault.jpg,,322,2,0,0,0
132,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Q8_X0KV3FP8,2016-11-09T11:01:16Z,9/11/16 11:01,CityBES: A Web-based Platform to Support City-Scale Building Energy Efficiency,"Author: Tianzhen Hong, Lawrence Berkeley National Laboratory Abstract: Buildings in cities consume 30 to 70% of the cities’ total primary energy. Retrofitting the existing building stock to improve energy efficiency and reduce energy use is a key strategy for cities to reduce green-house-gas emissions and mitigate climate change. Planning and evaluating retrofit strategies for buildings requires a deep understanding of the physical characteristics, operating patterns, and energy use of the building stock. This is a challenge for city managers as data and tools are limited and disparate. This paper introduces a web-based data and computing platform, City Building Energy Saver (CityBES), which focuses on energy modeling and analysis of a city’s building stock to support district or city-scale efficiency programs. CityBES uses an international open data standard, CityGML, to represent and exchange 3D city models. CityBES employs EnergyPlus to simulate building energy use and savings from energy efficient retrofits. CityBES provides a suite of features for urban planners, city energy managers, building owners, utilities, energy consultants and researchers. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M26S,1046,2d,hd,FALSE,https://i.ytimg.com/vi/Q8_X0KV3FP8/maxresdefault.jpg,,455,10,0,0,0
133,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aM6NDkQyJMY,2016-11-09T11:00:21Z,9/11/16 11:00,An Active Learning Framework Incorporating User Input For Mining Urban Data,"Author: Dimitrios Gunopulos, Department of Informatics and Telecommunications, National and Kapodistrian University of Athens Panepistimiopolis Abstract: Analyzing and detecting events from ubiquitous sensors across the city has been an important goal in recent years. Different techniques that are able to automatically detect events by monitoring urban sensor’s data have been efficiently applied in several smart cities to improve the citizens everyday life. However, the analysis of such voluminous data streams often interferes with several constraints that arise in smart cities scenarios. For example it is impossible to hire human oracles that will monitor each data stream continuously to provide knowledge to these models and to annotate past instances. Thus, the development of novel techniques is required in order to build efficient supervised learning models that will be able to cope with urban data deluge. Our approach makes the following contributions: (i) we formulate the problem of building supervised learning models efficiently by incorporating streaming input from urban data, and (ii) we present a novel framework that is able to cope with the restrictions that arise in the event detection of streaming urban data, requiring labels from carefully selected instances. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M1S,841,2d,hd,FALSE,https://i.ytimg.com/vi/aM6NDkQyJMY/maxresdefault.jpg,,131,0,0,0,0
134,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,SXNVNYhr5Io,2016-11-09T10:53:57Z,9/11/16 10:53,Monitoring Manhattan's traffic from 5 cameras?,"Author: Siheng Chen, Machine Learning Department, Carnegie Mellon University Abstract: Is it possible to monitor the entire traffic in Manhattan at a few road intersections? In this paper, we propose a series of novel graph data processing techniques to handle complex and nonsmooth traffic data. Then, we validate our proposed techniques on Manhattan’s taxi pickups during the years of 2014 and 2015. We are able to approximately recover the taxi-pickup activities in Manhattan by taking samples at only 5 selected intersections. We believe that the same techniques can be applied to recover other types of traffic data. The advantages of our methods are (a) quality: we are able to recover the taxi-pickup activities in entire Manhattan with small error from only 5 selected intersections; (b) scalability: we use a tree structure and principle component analysis to make this method efficient for large- scale graphs. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M49S,769,2d,hd,FALSE,https://i.ytimg.com/vi/SXNVNYhr5Io/maxresdefault.jpg,,138,2,0,0,0
135,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,bIRzZB86B-s,2016-11-09T10:32:30Z,9/11/16 10:32,Scalable SDE Filtering and Inference with Apache Spark,"Author: Harish S. Bhat, University of California Merced Abstract: In this paper, we consider the problem of Bayesian filtering and inference for time series data modeled as noisy, discrete-time observations of a stochastic differential equation (SDE) with undetermined parameters. We develop a Metropolis algorithm to sample from the high-dimensional joint posterior density of all SDE parameters and state time series. Our approach relies on an innovative density tracking by quadrature (DTQ) method to compute the likelihood of the SDE, the part of the posterior that requires the most computational effort to evaluate. As we show, the DTQ method lends itself to a natural implementation using Scala and Apache Spark, an open source framework for scalable data mining. We study the performance and scalability of our algorithm on filtering and inference problems for both regularly and irregularly spaced time series. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M24S,744,2d,hd,FALSE,https://i.ytimg.com/vi/bIRzZB86B-s/maxresdefault.jpg,,67,1,0,0,0
136,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XvkPXRD4ess,2016-11-09T10:31:52Z,9/11/16 10:31,Exact and Estimation of Local Edge­centric Graphlet Counts,"Author: Nesreen K. Ahmed, Intel Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M49S,1069,2d,hd,FALSE,https://i.ytimg.com/vi/XvkPXRD4ess/maxresdefault.jpg,,71,0,0,0,0
137,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hrx4m-kT6WQ,2016-11-09T10:30:38Z,9/11/16 10:30,Large Scale CVR Prediction through Dynamic Transfer Learning of Global and Local Features,"Author: Hongxia Yang, Yahoo! Inc. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M2S,962,2d,hd,FALSE,https://i.ytimg.com/vi/hrx4m-kT6WQ/maxresdefault.jpg,,205,1,0,0,0
138,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,z-P7QzXTrrY,2016-11-09T10:29:59Z,9/11/16 10:29,Inside the Atoms: Mining a Network of Networks and Beyond,"Author: Hanghang Tong, School of Computing, Informatics and Decision Systems Engineering, Arizona State University Abstract: Networks (i.e., graphs) appears in many high-impact applications. Often these networks are collected from different sources, at different times, at different granularities. In this talk, I will present our recent work on mining such multiple networks. First, we will present two models - one on modeling a set of inter-connected networks (NoN); and the other on modeling a set of inter-connected co-evolving time series (NoT). For both models, we will show that by treating networks as context, we are able to model more complicate real-world applications. Second, we will present some algorithmic examples on how to do mining with such new models, including ranking, imputation and prediction. Finally, we will demonstrate the effectiveness of our new models and algorithms in some applications, including bioinformatics, and sensor networks. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT28M40S,1720,2d,hd,FALSE,https://i.ytimg.com/vi/z-P7QzXTrrY/maxresdefault.jpg,,309,3,1,0,0
139,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_0W1Fi1atqE,2016-11-09T10:29:16Z,9/11/16 10:29,Disease Propagation in Social Networks: A Novel Study of Infection Genesis and Spread on Twitter,"Author: Manan Shah, The Harker School Abstract: The CDC diagnoses millions of cases of infectious diseases annually with observed disease curves peaking around mid-December and lulling in August and September. While this provides an accurate depiction of disease spread, its compilation takes too long for up-to-date monitoring. The ability to generate real-time disease distributions is important in identifying outbreaks and facilitating instant communication between authorities and health-care providers. We have attempted to characterize disease propagation using Twitter, expanding upon Google’s 2008 Flu Trends project. Our novel contribution is the development of a pipeline based model incorporating natural language processing and machine learning. The correlation coefficient between the Twitter disease distribution obtained via our approach and CDC data was 0.98. Our model further identified disease outbreaks that were not prevalent in the CDC distribution such as the parotitis outbreak in late 2014 that large hospital samples failed to identify. We additionally develop a differential equation based disease simulation (known as SEIR) in order to further validate our Twitter disease distribution model. Our model has the potential to greatly assist in the creation of an early-warning infection system by identifying disease outbreaks in real-time using the ever-growing social media sphere, representing a unique and powerful benefit to society. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M35S,755,2d,hd,FALSE,https://i.ytimg.com/vi/_0W1Fi1atqE/maxresdefault.jpg,,138,0,0,0,0
140,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,SgIDoUT1gaU,2016-11-09T10:28:30Z,9/11/16 10:28,"A Sublinear, Massive-­scale Look-­alike Audience Extension System","Author: Qiang Ma, Yahoo! Inc. Abstract: Look-alike audience extension is a practically effective way to customize high-performance audience in on-line advertising. With look-alike audience extension system, any advertiser can easily generate a set of customized audience by just providing a list of existing customers without knowing the detailed targetable attributes in a sophisticated advertising system. In this paper, we present our newly developed graph-based look-alike system in Yahoo! advertising platform which provides look-alike audiences for thousands of campaigns. Extensive experiments have been conducted to compare our look-alike model with three other existing look-alike systems using billions of users and millions of user features. The experiment results show that our developed graph-based method with nearest-neighbor filtering outperforms other methods in comparison by more than 50% regarding conversion rate in app-install ad campaigns. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M20S,920,2d,hd,FALSE,https://i.ytimg.com/vi/SgIDoUT1gaU/maxresdefault.jpg,,268,1,0,0,1
141,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jhiFn8REjyQ,2016-11-09T10:27:27Z,9/11/16 10:27,Contextual embedding for distributed representations of entities in a text corpus,"Author: Md Abdul Kader, Department of Computer Science, University of Texas at El Paso More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT10M22S,622,2d,hd,FALSE,https://i.ytimg.com/vi/jhiFn8REjyQ/maxresdefault.jpg,,117,0,0,0,0
142,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,J73TKGUEqCk,2016-11-09T10:22:55Z,9/11/16 10:22,From Practice to Theory in Learning from Massive Data,"Author: Charles Elkan, Department of Computer Science and Engineering, UC San Diego Abstract: This talk will discuss examples of how Amazon applies machine learning to large-scale data, and open research questions inspired by these applications. One important question is how to distinguish between users that can be influenced, versus those who are merely likely to respond. Another question is how to measure and maximize the long-term benefit of movie and other recommendations. A third question, is how to share data while provably protecting the privacy of users. Note: Information in the talk is already public, and opinions expressed will be strictly personal. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT40M1S,2401,2d,hd,FALSE,https://i.ytimg.com/vi/J73TKGUEqCk/maxresdefault.jpg,,424,6,2,0,1
143,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,cfw9OO_UtWc,2016-11-09T10:21:55Z,9/11/16 10:21,Foundations for Scaling ML in Apache Spark,"Author: Joseph K. Bradley, Databricks Inc. Abstract: Apache Spark has become the most active open source Big Data project, and its Machine Learning library MLlib has seen rapid growth in usage. A critical aspect of MLlib and Spark is the ability to scale: the same code used on a laptop can scale to 100’s or 1000’s of machines. This talk will describe ongoing and future efforts to make MLlib even faster and more scalable by integrating with two key initiatives in Spark. The first is Catalyst, the query optimizer underlying DataFrames and Datasets. The second is Tungsten, the project for approaching bare-metal speeds in Spark via memory management, cache-awareness, and code generation. This talk will discuss the goals, the challenges, and the benefits for MLlib users and developers. More generally, we will reflect on the importance of integrating ML with the many other aspects of big data analysis. About MLlib: MLlib is a general Machine Learning library providing many ML algorithms, feature transformers, and tools for model tuning and building workflows. The library benefits from integration with the rest of Apache Spark (SQL, streaming, Graph, core), which facilitates ETL, streaming, and deployment. It is used in both ad hoc analysis and production deployments throughout academia and industry. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M6S,1626,2d,hd,FALSE,https://i.ytimg.com/vi/cfw9OO_UtWc/maxresdefault.jpg,,79,1,0,0,0
144,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XBfO562xDhc,2016-11-09T10:21:15Z,9/11/16 10:21,FPGASVM: A Framework for Accelerating Kernelized Support Vector Machine Training using FPGAs,"Author: Mudhar Bin Rabieah, Department of Electrical and Electronic Engineering, Imperial College London More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M11S,971,2d,hd,FALSE,https://i.ytimg.com/vi/XBfO562xDhc/maxresdefault.jpg,,158,4,0,0,0
145,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,LwezHLVGb6A,2016-11-09T10:00:59Z,9/11/16 10:00,Scalable Clustering of Correlated Time Series using Expectation Propagation,"Author: Christopher Aicher, Department of Statistics, University of Washington Abstract: We are interested in finding clusters of time series such that series within a cluster are correlated and series between clusters are independent. Existing Bayesian methods for inferring correlated clusters of time series either: (i) require conditioning on latent variables to decouple time series, but results in slow mixing or (ii) require calculating a collapsed likelihood, but with computation scaling cubically with the number of time series per cluster. To infer the latent cluster assignments efficiently, we consider approximate methods that trade exactness for scalability. Our main contribution is the development of an expectation propagation based approximation for the collapsed likelihood approach. Our empirical results on synthetic data show our methods scale linearly instead of cubically, while maintaining competitive accuracy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M55S,1255,2d,hd,FALSE,https://i.ytimg.com/vi/LwezHLVGb6A/maxresdefault.jpg,,915,6,1,0,2
146,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gpBIESNqU3w,2016-11-09T10:00:01Z,9/11/16 10:00,Using Time Series Techniques to Forecast and Analyze Wake and Sleep Behavior,"Author: Jennifer A. Williams, School of Electrical Engineering and Computer Science, Washington State University Abstract: Smart home technologies provide numerous benefits for providing healthcare to individuals in a non-invasive manner. Our goal of this research is to use smart home technology to assist people recovering from injuries or coping with disabilities to live independently. In this paper, we propose an algorithmic method, Behavior Forecasting (BF), to model and forecast both the wake and sleep behaviors that are exhibited by an individual. The BF method consists of (1) detecting wake/sleep cycles, (2) defining numeric values that reflect wake behavior and numeric values that reflect sleep behavior, (3) forecasting the numeric wake and sleep values based on past behavior, (4) analyzing the effect of wake behavior on sleep by using wake behaviors when forecasting for the next sleep behavior observed, and vice versa, and (5) improving the performance of score prediction by using both past wake and past sleep scores. We evaluate the performance of our BF method with data collected from CASAS smart homes. We found that incorporating time series techniques such as a periodogram improves the detection of regular sleep and wake cycles. We also found that regardless of the utilized forecasting method, we can model wake behavior and sleep behavior with the minimum accuracy of 87%. These results suggest that we can effectively model wake and sleep behaviors in a smart environment. Furthermore, that future wake behavior can be determined from sleep behaviors and vice versa. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M45S,1245,2d,hd,FALSE,https://i.ytimg.com/vi/gpBIESNqU3w/maxresdefault.jpg,,142,2,0,0,0
147,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Tc-JVDsJf2c,2016-11-09T09:58:55Z,9/11/16 9:58,Short-term Time Series Forecasting with Regression Automata,"Author: Massimo Chenal, Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg Abstract: We present regression automata (RA), which are novel type syntactic models for time series forecasting. Building on top of conventional state-merging algorithms for identifying automata, RA use numeric data in addition to symbolic values and make predictions based on this data in a regression fashion. We apply our model to the problem of hourly wind speed and wind power forecasting. Our results show that RA outperform other state-of-the-art approaches for predicting both wind speed and power generation. In both cases, short-term predictions are used for resource allocation and infrastructure load balancing. For those critical tasks, the ability to inspect and interpret the generative model RA provide is an additional benefit. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT11M49S,709,2d,hd,FALSE,https://i.ytimg.com/vi/Tc-JVDsJf2c/maxresdefault.jpg,,566,6,0,0,0
148,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,RomX4HYNsmI,2016-11-09T09:58:01Z,9/11/16 9:58,Granger Causality Networks for Categorical Time Series,"Author: Alex Tank, Department of Statistics, University of Washington Abstract: We present two model-based methods for learning Granger causality networks for multivariate categorical time series. Our first proposal is based on the mixture transition distribution (MTD) model. Traditionally, MTD is plagued by a nonconvex objective, non-identifiability, and presence of many local optima. To circumvent these problems, we recast inference in the MTD as a convex problem. The new formulation facilitates the application of MTD to high-dimensional multivariate time series. Our second proposal is based on a multi-output logistic autoregressive model, which while a straightforward extension, has not been previously applied to the analysis of multivariate categorial time series. We investigate identifiability conditions of both methods, devise novel optimization algorithms for the MTD, and compare the MTD and mLTD in simulated experiments. Our approach simultaneously provides a comparison of methods for network inference in categorical time series and opens the door to modern, regularized inference in MTD model. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M2S,1022,2d,hd,FALSE,https://i.ytimg.com/vi/RomX4HYNsmI/maxresdefault.jpg,,1675,9,2,0,4
149,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,68qPrNs-1AY,2016-11-09T09:56:52Z,9/11/16 9:56,"Dynamic Processes over Information Networks Representation, Modeling, Learning and Inference","Author: Le Song, College of Computing, Georgia Institute of Technology Abstract: Nowadays, large-scale human activity data from online social platforms, such as Twitter, Facebook, Reddit, Stackoverflow, Wikipedia and Yelp, are becoming increasing available and in increasing spatial and temporal resolutions. Such data provide great opportunities for understanding and modeling both macroscopic (network level) and microscopic (node-level) patterns in human dynamics. Such data have also fueled the increasing efforts on developing realistic representations and models as well as learning, inference and control algorithms to understand, predict, control and distill knowledge from these dynamic processes over networks. It has emerged as a trend to take a bottom-up approach which starts by considering the stochastic mechanism driving the behavior of each node in a network to later produce global, macroscopic patterns at a network level. However, this bottom-up approach also raises significant modeling, algorithmic and computational challenges. In this talk, I will present machine learning framework for representing, modeling, and performing learning and inference for human activity data. The framework leverage methods from temporal point process theory, probabilistic graphical models and optimization, and often produce state-of-the-art results on various modeling and time-sensitive inference tasks. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M36S,3036,2d,hd,FALSE,https://i.ytimg.com/vi/68qPrNs-1AY/maxresdefault.jpg,,104,1,0,0,0
150,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pg9ygUbXySk,2016-11-09T09:56:09Z,9/11/16 9:56,On the Effect of Endpoints on Dynamic Time Warping,"Author: Diego Furtado Silva, University of São Paulo Abstract: While there exist a plethora of classification algorithms for most data types, there is an increasing acceptance that the unique properties of time series mean that the combination of nearest neighbor classifiers and Dynamic Time Warping (DTW) is very competitive across a host of domains, from medicine to astronomy to environmental sensors. While there has been significant progress in improving the efficiency and effectiveness of DTW in recent years, in this work we demonstrate that an underappreciated issue can significantly degrade the accuracy of DTW in real-world deployments. This issue has probably escaped the attention of the very active time series research community because of its reliance on static highly contrived benchmark datasets, rather than real world dynamic datasets where the problem tends to manifest itself. In essence, the issue is that DTW’s eponymous invariance to warping is only true for the main “body” of the two time series being compared. However, for the “head” and “tail” of the time series, the DTW algorithm affords no warping invariance. The effect of this is that tiny differences at the beginning or end of the time series (which may be either consequential or simply the result of poor “cropping”) will tend to contribute disproportionally to the estimated similarity, producing incorrect classifications. In this work, we show that this effect is real, and reduces the performance of the algorithm. We further show that we can fix the issue with a subtle redesign of the DTW algorithm, and that we can learn an appropriate setting for the extra parameter we introduced. We further demonstrate that our generalization is amiable to all the optimizations that make DTW tractable for large datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M28S,1288,2d,hd,FALSE,https://i.ytimg.com/vi/pg9ygUbXySk/maxresdefault.jpg,,653,1,2,0,1
151,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3dbPfCehS3U,2016-11-09T09:55:18Z,9/11/16 9:55,New Time Series Methods for Flu Forecasting,"Author: Naren Ramakrishnan, Department of Computer Science, Virginia Polytechnic Institute and State University Abstract: There has been recent concerted interest in computational methods for forecasting the flu, spurred by competitions organized by agencies like the CDC and IARPA. The CDC competition aimed to forecast flu seasonal characteristics in the US and the IARPA Open Source Indicators (OSI) forecasting tournament was focused on disease forecasting (flu and rare diseases) in countries of Latin America. The speaker's team was declared the winner in the IARPA OSI competition and this task will communicate our lessons learned about what goes into a successful flu forecasting engine, how to evaluate its performance, and how best to ensure its relevance to public health policy and planning purposes. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT41M8S,2468,2d,hd,FALSE,https://i.ytimg.com/vi/3dbPfCehS3U/maxresdefault.jpg,,137,2,0,0,0
152,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,7DsbLaRWcIM,2016-11-09T09:54:32Z,9/11/16 9:54,Sparse plus low-rank graphical models of time series for functional connectivity in MEG,"Author: Rahul Nadkarni, Department of Computer Science and Engineering, University of Washington Abstract: Inferring graphical models from high dimensional observations has become an important problem in machine learning and statistics because of its importance in a variety of application domains. One such application is inferring functional connectivity between brain regions from neuroimaging data such as magnetoencephalograpy (MEG) recordings that produce signals with good temporal and spatial resolution. Unfortunately, existing techniques to learn graphical models that have been applied to neuroimaging data have assumed the data to be i.i.d. over time, ignoring key temporal dynamics. Additionally, the signals that arise from neuroimaging data do not exist in isolation as the brain is performing many tasks simultaneously so that most existing methods can introduce spurious connections. We address these issues by introducing a method to learn Gaussian graphical models between multiple time series with latent processes. In addition, we allow for heterogeneity between different groups of MEG recordings by using a hierarchical penalty. The proposed methods are formulated as convex optimization problems that we efficiently solve by developing an alternating directions method of multipliers algorithm. We evaluate the proposed model on synthetic data as well as on global stock index returns and a real MEG data set. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M29S,1469,2d,hd,FALSE,https://i.ytimg.com/vi/7DsbLaRWcIM/maxresdefault.jpg,,217,1,0,0,0
153,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hiKpS7NTf6Y,2016-11-09T09:53:34Z,9/11/16 9:53,The Great Time Series Classification Bake Off,"Author: Jason Lines, University of East Anglia Abstract: In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 20 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M56S,1316,2d,hd,FALSE,https://i.ytimg.com/vi/hiKpS7NTf6Y/maxresdefault.jpg,,1512,15,9,0,2
154,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Aqp0z1lkDFU,2016-11-09T09:51:33Z,9/11/16 9:51,Stream Data Mining: A Big Data Perspective,"Author: Latifur Khan, Department of Computer Science, Erik Jonsson School of Engineering & Computer Science, The University of Texas at Dallas Abstract: Data streams are continuous flows of data. Examples of data streams include network traffic, sensor data, call center records and so on. Data streams demonstrate several unique properties that together conform to the characteristics of big data (i.e., volume, velocity, variety and veracity) and add challenges to data stream mining. In this talk we will present an organized picture on how to handle various data mining techniques in data streams. Most existing data stream classification techniques ignore one important aspect of stream data: arrival of a novel class. We address this issue and propose a data stream classification technique that integrates a novel class detection mechanism into traditional classifiers, enabling automatic detection of novel classes before the true labels of the novel class instances arrive. Novel class detection problem becomes more challenging in the presence of concept-drift, when the underlying data distributions evolve in streams. In this talk we will show how to make fast and correct classification decisions under this constraint with limited labeled training data and apply them to real benchmark data. In addition, we will present a number of stream classification applications such as adaptive malicious code detection, website fingerprinting, evolving insider threat detection and textual stream classification. This research was funded in part by NSF, NASA, Air Force Office of Scientific Research (AFOSR) and Raytheon. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M30S,2730,2d,hd,FALSE,https://i.ytimg.com/vi/Aqp0z1lkDFU/maxresdefault.jpg,,1107,6,2,0,4
155,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,G5SlUEHvCBc,2016-11-09T09:50:27Z,9/11/16 9:50,Parallel News-Article Traffic Forecasting with ADMM,"Author: Stratis Ioannidis, Department of Electrical and Computer Engineering, Northeastern University Abstract: Predicting the traffic of an article, as measured by page views, is of great importance to content providers. Articles with increased traffic can improve advertising revenue and expand a provider’s user base. We propose a broadly applicable methodology incorporating meta-data and joint forecasting across articles, that involves solving a large optimization problem through the Alternating Directions Method of Multipliers (ADMM). We implement our solution using Spark, and evaluate it over a large corpus of articles and forecasting models. Our results demonstrate that our featurebased forecasting is both scalable as well as highly accurate, significantly improving forecasting predictions compared to traditional forecasting models. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M56S,1256,2d,hd,FALSE,https://i.ytimg.com/vi/G5SlUEHvCBc/maxresdefault.jpg,,84,0,0,0,0
156,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,GHubRSvMqrk,2016-11-09T09:49:18Z,9/11/16 9:49,Space-Time Clustering with Stability Probe while Riding Downhill,"Author: Xin Huang, The University of Texas at Dallas Abstract: We propose a new data-driven procedure for optimal selection of tuning parameters in dynamic clustering algorithms, using the notion of stability probe. Due to the shape of the stability probe dynamics, we refer to the new clustering stability procedure as Downhill Riding (DR). We study final sample performance of DR in conjunction with DBSCAN and TRUST in application to clustering synthetic times series and yearly temperature records in Central Germany. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M32S,1052,2d,hd,FALSE,https://i.ytimg.com/vi/GHubRSvMqrk/maxresdefault.jpg,,38,0,0,0,0
157,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ak99QfrlaTo,2016-11-09T09:48:30Z,9/11/16 9:48,Time Lag Concerned Dynamic Dependency Network Structure Learning,"Author: Lei Han, Department of Statistics, Rutgers, The State University of New Jersey Abstract: Characterizing and understanding the structure and the evolution of networks is an important problem for many different fields. While in the real-world networks, especially the spatial networks, the time lags cost to propagate influences from one node to another tend to vary over both space and time due to the different space distances and propagation speeds between nodes. Thus time lag plays an essential role in interpreting the temporal causal dependency among nodes and also brings a big challenge in network structure learning. However most of the previous researches aiming to learn the dynamic network structure only treat the time lag as a predefined constant, which may miss important information or include noisy information if the time lag is set too small or too large. In this paper, we propose a dynamic Bayesian model which simultaneously integrates two usually separate tasks, i.e. learning the dynamic dependency network structure and estimating time lags, within one unified framework. Besides, we propose a novel weight kernel approach for time series segmenting and sampling via leveraging samples from adjacent segments to avoid the sample scarcity and an effective Bayesian scheme cooperated with RJMCMC and EP algorithms for parameter inference. To our knowledge, this is the first practical work for dynamic network structure learning concerned with adaptive time lag estimation. Extensive empirical evaluations are conducted on both synthetic and two real-world datasets, and the results demonstrate that our proposed model is superior to the traditional methods in learning the network structure and the temporal dependency. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M52S,772,2d,hd,FALSE,https://i.ytimg.com/vi/Ak99QfrlaTo/maxresdefault.jpg,,43,0,0,0,0
158,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,cAZSU5Na8fM,2016-11-09T09:45:49Z,9/11/16 9:45,Evaluating Improvements to the Shapelet Transform,"Author: Aaron Bostrom, University of East Anglia Abstract: The Shapelet tree algorithm was proposed in 2009 as a novel way to find phase independent subsequences which could be used for time series classification. The shapelet discovery algorithm is O(n2m4), where n is the number of cases, and m is the length of the series. Several methods have sought to increase the speed of finding shapelets. The ShapeletTransform reduces the finding to a single pass, and FastShapelets smooths and reduces the series lengths through PAA and SAX. However neither of these techniques can enumerate all shapelets on the largest of the datasets present in the UCR repository. We first evaluate whether the FastShapelet algorithm is better as a transform, and secondly provide a contract classifier for the shapelet transform, by calculating the number of fundamental operations we can estimate the run time of the algorithm, and sample the data to fulfil this contract. We found that whilst the FastShapeletTransform does drastically reduce the operation count of finding shapelets it is not significantly better than FastShapelets, nor can it compete with the ShapeletTransform. The factory method for sampling the data is competitive with the ShapeletTransform and in some cases we see minor improvements despite being much faster. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M57S,1317,2d,hd,FALSE,https://i.ytimg.com/vi/cAZSU5Na8fM/maxresdefault.jpg,,307,1,0,0,0
159,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aK82myaNFAA,2016-11-08T14:09:27Z,8/11/16 14:09,Prognostics of Combustion Instabilities from Hi-speed Flame Video,"Author: Adedotun Akintayo, Iowa State University Abstract: The thermo-acoustic instabilities arising in combustion processes cause significant deterioration and safety issues in various human-engineered systems such as land and air based gas turbine engines. The phenomenon is described as selfsustaining, large amplitude pressure oscillations that shows varying spatial scales of periodic coherent vortex structure shedding. Early detection and close monitoring of combustion instability are the keys to extending the remaining useful life (RUL) of any gas turbine engine. However, such impending instability to a stable combustion is extremely difficult to detect only from pressure data due to its sudden (bifurcation-type) nature. This paper proposes an endto-end deep convolutional selective autoencoder approach to capture the rich information in hi-speed flame video for instability prognostics. In this context, an autoencoder is trained to selectively mask stable flame and allow unstable flame image frames. The network identifies subtle instability features as a combustion process makes transition from stable to unstable region. The proposed framework is validated on a set of real data collected from a laboratory scale combustor over varied operating conditions. As a result, the deep learning tool-chain can perform as an early detection framework for combustion instabilities that will have a transformative impact on the safety and performance of modern engines. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT10M16S,616,2d,hd,FALSE,https://i.ytimg.com/vi/aK82myaNFAA/maxresdefault.jpg,,221,0,0,0,0
160,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,PyASaUwFTxQ,2016-11-08T14:08:52Z,8/11/16 14:08,Custom Large-Scale Application Management for Verizon Use Cases,"Author: Santanu Das, Verizon Communications More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M15S,1575,2d,hd,FALSE,https://i.ytimg.com/vi/PyASaUwFTxQ/maxresdefault.jpg,,20,0,0,0,0
161,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,2NkjsA1aMSA,2016-11-08T14:07:12Z,8/11/16 14:07,Sequential Hypothesis Tests for Markov Models of Time-Series Data,"Author: Nurali Virani, Department of Mechanical and Nuclear Engineering, Pennsylvania State University Abstract: This paper presents new results on sequential hypothesis tests for Markov models of time series data. In particular, a technique for sequential hypothesis testing for Markov models inferred using concepts of symbolic dynamics is developed. These models are created by discretizing the phase space of a dynamical system and the system dynamics is approximated as a finite memory Markov chain on the discrete state-space. We present sequential update rules for log-likelihood ratio statistic of Markov models under the setting of binary hypothesis testing and analyze the stochastic evolution of this statistic. The proposed technique allows us to choose a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The study is motivated by time-critical detection problems with physical systems, where a temporal model is trained and a fast reliable decision with large volumes of streaming data is desired during operation. The proposed technique is first illustrated through a simulation example. Furthermore, the ideas are tested on pressure time-series data obtained from a laboratory-scale swirl stabilized combustor, where some controlled protocols are used to induce instability. The proposed framework is used to detect and estimate onset of instability during combustion. We compare the performance with maximum-likelihood classifier and show that the proposed technique gives reliable detection of instability using fewer observations. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M10S,910,2d,hd,FALSE,https://i.ytimg.com/vi/2NkjsA1aMSA/maxresdefault.jpg,,979,1,1,0,1
162,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HI1tAw3iVEg,2016-11-08T14:06:17Z,8/11/16 14:06,A Classifier Development Process for Mechanical Health Diagnostics on US Army Rotorcraft,"Author: Andrew W. Wilson Abstract: Due to various historical events, the Aviation Engineering Directorate (AED) of the United States Army has a unique, large data set describing the mechanical health of rotorcraft systems. This data set includes detailed information regarding non-critical failures and wear over a significant period and number of aircraft, each of which is instrumented to take measurements of mechanical vibrations and other parameters every flight. Attempts to utilize this data led AED to investigate the efficacy of machine learning and knowledge discovery from data (KDD) techniques. This paper outlines a tool–termed the Crawler–which AED developed to automate the process of creating diagnostic classifiers, and its application to two specific problems of interest: improving the usability and performance of a deployed gearbox health classifier, and rapidly developing a model to search sensor data for a newly identified fault mode. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT9M43S,583,2d,hd,FALSE,https://i.ytimg.com/vi/HI1tAw3iVEg/maxresdefault.jpg,,40,0,0,0,0
163,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,s8kHQJzRuZI,2016-11-08T14:05:31Z,8/11/16 14:05,Temporal Learning in Video Data Using Deep Learning and Gaussian Processes,"Author: Abhishek Srivastav, General Electric Company Abstract: This paper presents an approach for data-driven modeling of hidden, stationary temporal dynamics in sequential images or vidoes using deep learning and Bayesian non-parametric techniques. In particular, a Deep Convolutional Neural Network (CNN) is used to extract spatial features in an unsupervised fashion from individual images and then, a Gaussian process is used to model the temporal dynamics of the spatial features extracted by the Deep CNN. By decomposing the spatial and temporal components and utilizing the strengths of deep learning and Gaussian processes for the respective sub-problems, we are able to construct a model that is able to capture complex spatio-temporal phenomenon while using relatively small number of free parameters (or hyperparameters). The proposed approach is tested on high-speed grey-scale video data obtained of combustion flames in a swirl-stabilized combustor, where certain protocols are used to induce instability in combustion process. The proposed approach is then used to detect and predict the transition of the combustion process from stable to unstable regime. It is demonstrated that the proposed approach is able to detect unstable flame conditions using very few frames from high-speed video. This is useful as early detection of unstable combustion can lead to better control strategies to mitigate instability. Results from the proposed approach are compared and contrasted with several baselines and recent work in this area, the performance of the proposed approach is found to be significantly better in terms of detection accuracy, model complexity and lead-time to detection. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT11M58S,718,2d,hd,FALSE,https://i.ytimg.com/vi/s8kHQJzRuZI/maxresdefault.jpg,,449,2,0,0,0
164,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,d2dxBp-jvg0,2016-11-08T14:04:19Z,8/11/16 14:04,Activity Recognition in Prognostics and Health Monitoring (PHM) Related Service Environment,"Author: Soumalya Sarkar, United Technology Research Center (UTRC) Abstract: It is important to track the cognitive activity of service personnel in a Prognostics and Health Monitoring (PHM) related training or operation environment. EEG data is a good candidate for cognitive activity recognition. Analyzing EEG data in an unconstrained (natural) environment is a challenging task due to multiple reasons such as low signal-to-noise ratio, transient nature, lack of baseline and uncontrolled mixing of various tasks. This paper proposes a framework based on deep learning using both deep belief network (DBN) and deep convolutional neural network (DCNN) that monitors cognitive activity by fusing multiple non-collocated EEG probes and also selects a smaller sensor suite for a lean data collection system. Validation on realistic data along with comparison with benchmark machine learning techniques are performed. It is observed via sensor selection that a significantly smaller EEG sensor suite can perform at a comparable accuracy as the original sensor suite. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M35S,815,2d,hd,FALSE,https://i.ytimg.com/vi/d2dxBp-jvg0/maxresdefault.jpg,,154,0,0,0,0
165,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,u-df8vd-ZRE,2016-11-08T14:02:55Z,8/11/16 14:02,Applying Deep Learning for Prognostic Health Monitoring of Aerospace and Building Systems,"Author: Kishore K. Reddy, United Technologies Research Center Abstract: Data-driven prognostics are instrumental in enabling anomaly detection, sensor estimation and prediction in prognostics and health management (PHM) systems. Recent advances in machine learning techniques such as deep learning (DL) has rejuvenated data-driven analysis in PHM. DL algorithms have been successful due to the presence of large volumes of data and its ability to learn the features during the learning process. The performance improvement is significant from the features learnt from DL techniques as compared to the hand crafted features. This paper proposes using deep belief networks (DBN) and deep auto encoders (DAE) in three different aerospace and building systems applications: (i) estimation of fuel flow rate in jet engines, (ii) fault detection in elevator cab doors using smart phone, and (iii) prediction of chiller power consumption in heating, ventilation, and air conditioning (HVAC) systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M26S,746,2d,hd,FALSE,https://i.ytimg.com/vi/u-df8vd-ZRE/maxresdefault.jpg,,1915,6,0,0,0
166,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9HHzmxDS0KM,2016-11-08T14:00:29Z,8/11/16 14:00,Bridging the gap between domain experts and machine learning,"Panelists: Dragos Margineantu, Boeing Integrated Defense Systems, Boeing Company Michael Giering, United Technologies Research Center Achalesh Pandey, GE Global Research More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT57M1S,3421,2d,hd,FALSE,https://i.ytimg.com/vi/9HHzmxDS0KM/maxresdefault.jpg,,140,0,0,0,0
167,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,j-5ejCfCKuY,2016-11-08T13:54:31Z,8/11/16 13:54,A Framework of Combining Deep Learning and Survival Analysis for Asset Health Management,"Author: Linxia Liao, General Electric Company Abstract: We propose a method to integrate feature extraction and prediction as a single optimization task by stacking a threelayer model as a deep learning structure. The first layer of the deep structure is a Long Short Term Memory (LSTM) model which deals with the sequential input data from a group of assets. The output of the LSTM model is followed by mean-pooling, and the result is fed to the second layer. The second layer is a neural network layer, which further learns the feature representation. The output of the second layer is connected to a survival model as the third layer for predicting asset health condition. The parameters of the three-layer model are optimized together via stochastic gradient decent. The proposed method was tested on a small dataset collected from a fleet of mining haul trucks. The model resulted in the “individualized” failure probability representation for assessing the health condition of each individual asset, which well separates the in-service and failed trucks. The proposed method was also tested on a large open source hard drive dataset, and it showed promising result. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M32S,872,2d,hd,FALSE,https://i.ytimg.com/vi/j-5ejCfCKuY/maxresdefault.jpg,,1169,9,0,0,0
168,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Div67MF1hQM,2016-11-08T13:53:23Z,8/11/16 13:53,Multi-Sensor Prognostics using an Unsupervised Health Index based on an LSTM Encoder-Decoder,"Author: Pankaj Malhotra, Tata Consultancy Services Ltd Abstract: Many approaches for estimation of Remaining Useful Life (RUL) of a machine, using its operational sensor data, make assumptions about how a system degrades or a fault evolves, e.g., exponential degradation. However, in many domains degradation may not follow a pattern. We propose a Long Short Term Memory based Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI) for a system using multi-sensor time-series data. LSTM-ED is trained to reconstruct the time-series corresponding to healthy state of a system. The reconstruction error is used to compute HI which is then used for RUL estimation. We evaluate our approach on publicly available Turbofan Engine and Milling Machine datasets. We also present results on a real-world industry dataset from a pulverizer mill where we find significant correlation between LSTM-ED based HI and maintenance costs. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT14M18S,858,2d,hd,FALSE,https://i.ytimg.com/vi/Div67MF1hQM/maxresdefault.jpg,,645,6,0,0,2
169,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XZ7Hx2pts7I,2016-11-08T13:26:58Z,8/11/16 13:26,Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping,"Aauthor: Christian Bracher, Zalando SE Abstract: We present a method to determine Fashion DNA, coordinate vectors locating fashion items in an abstract space. Our approach is based on a deep neural network architecture that ingests curated article information such as tags and images, and is trained to predict sales for a large set of frequent customers. In the process, a dual space of customer style preferences naturally arises. Interpretation of the metric of these spaces is straightforward: The product of Fashion DNA and customer style vectors yields the forecast purchase likelihood for the customer–item pair, while the angle between Fashion DNA vectors is a measure of item similarity. Importantly, our models are able to generate unbiased purchase probabilities for fashion items based solely on article information, even in absence of sales data, thus circumventing the “cold–start problem” of collaborative recommendation approaches. Likewise, it generalizes easily and reliably to customers outside the training set. We experiment with Fashion DNA models based on visual and/or tag item data, evaluate their recommendation power, and discuss the resulting article similarities. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M49S,1069,2d,hd,FALSE,https://i.ytimg.com/vi/XZ7Hx2pts7I/maxresdefault.jpg,,105,0,0,0,0
170,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,1m0UHOXpwmc,2016-11-08T13:26:17Z,8/11/16 13:26,"Challenges of quantifying fashion data: creativity, art and emotions","Authors: Jinah Oh, Academy of Art University, San Francisco Elena Eberhard, Academy of Art University, San Francisco Abstract: Fashion is a field at the border of art and industry, combining elements of creative spontaneity in a unexpected ways, based on various sources of inspiration. It takes a human to create a clothing and a celebrity to make it fashionable. Real fashion world, designers and creative consumers (street fashion) provide an eclectic ever-changing content that science and technology are trying to optimize in order to increase sales and decrease the waste of over-production. In this talk we provide an overview of fashion big data problems: forecasting fashion trends, influencer analytics, visual search, natural language processing, style recommendation algorithms and the need to understand the natural life-cycle of a fashion garment before applying science in order to accelerate or alter it. Also, we will share some examples of collaboration projects between giants of technology and academics exploring the potential of quantifying fashion data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT42M19S,2539,2d,hd,FALSE,https://i.ytimg.com/vi/1m0UHOXpwmc/maxresdefault.jpg,,290,4,0,0,0
171,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aTB38biOBoE,2016-11-08T13:25:04Z,8/11/16 13:25,Making fashion recommendations with human-in-the-loop machine learning,"Author: Brad Klingenberg, Stitch Fix, Inc. Abstract: Most recommendation algorithms produce results without human intervention. Especially in hard-to-quantify domains like fashion combining algorithms with expert human curation can make recommendations more effective. But it can also complicate traditional approaches to training and evaluating algorithms. In this talk I will share lessons from making personalized fashion recommendations with humans in the loop at Stitch Fix, where we commit to our recommendations through the physical delivery of merchandise to clients. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M53S,1253,2d,hd,FALSE,https://i.ytimg.com/vi/aTB38biOBoE/maxresdefault.jpg,,707,7,0,0,0
172,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_4Z_pzOBgpI,2016-11-08T13:23:48Z,8/11/16 13:23,Recommendation and Opinion Mining with Visual Signals,"Author: Julian McAuley, Department of Computer Science and Engineering, UC San Diego Abstract: Building personalized systems for fashion recommendation presents several challenges due to the complicated semantics of people's preferences and styles. One challenge is simply the need to deal with sparse, long-tailed datasets, where new content is constantly introduced and recommendation is inherently a cold-start problem. Another challenge is the need to model visual signals, where the semantics of what makes items ""attractive"" are incredibly subtle. Finally, there is the need to model temporal dynamics that account for how fashion continually (and rapidly) evolves. In this talk we'll see how traditional recommendation approaches can be extended to explicitly account for the visual appearance of the items being recommended, in order to overcome these challenges and make visually- and stylistically-aware recommendations. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M36S,996,2d,hd,FALSE,https://i.ytimg.com/vi/_4Z_pzOBgpI/maxresdefault.jpg,,235,1,0,0,0
173,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_HAj06WCaaY,2016-11-08T13:21:34Z,8/11/16 13:21,Joint multi-modal representations for e-commerce catalog search,"Author: Amrita Saha, IBM India Research Lab Abstract: In many visual domains (like fashion, furniture etc.) the search for products on online platforms is highly driven by visual attributes. Conventional search requires that all items in the catalog are manually tagged with all possible attribute values which is not scalable. In this paper we propose a novel paradigm for multi-modal catalog search via joint representations. The user provides a search query in natural language (e.g.,pink floral top) and the returned results are of a different modality (that is the set of images of pink floral tops). Specifically we use a correlational autoencoder based model to learn the joint representation for both the image and its corresponding description such that the two representations are embedded in the same space and as close as possible to each other. These representations are learnt over a large curated fashion dataset of over 700 thousand images crawled from multiple fashion e-commerce portals. Our experimental results show that these representations are a viable alternative for searching large fashion catalogs without manual tagging. The same representations can also be used for visual search, image tagging, and query expansion. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M37S,997,2d,hd,FALSE,https://i.ytimg.com/vi/_HAj06WCaaY/maxresdefault.jpg,,111,1,0,0,0
174,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UV44oINAm00,2016-11-08T13:20:38Z,8/11/16 13:20,Detection of fashion trends and seasonal cycles,"Author: Roberto Sanchis-Ojeda, Stitch Fix, Inc. Abstract: In this contribution we describe a new approach to detecting seasonal and fashion trends, by statistically modeling how clients’ reaction to style units change with time. In our framework, client reactions are required to take the form of binary outcome variables (e.g., buy vs. do not buy, click vs. do not click). Client behavior can then be studied with generalized linear models and mixed-effect models that include temporal features. We discuss how the coefficients of such models inform which styles are going in or out of season or fashion and demonstrate these methods using simulated data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M21S,981,2d,hd,FALSE,https://i.ytimg.com/vi/UV44oINAm00/maxresdefault.jpg,,374,1,0,0,0
175,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4D1wG9dg8bw,2016-11-08T13:19:11Z,8/11/16 13:19,Decoding Fashion Contexts Using Word Embeddings,"Author: Deepak Warrier, Myntra Designs Private Ltd. Abstract: Personalisation in e-commerce hinges on dynamically uncovering the user’s context via his/her interactions on the portal. The harder the context identification, lesser is the effectiveness of personalisation. Our work attempts to uncover and understand the user’s context to effectively render personalisation for fashion ecommerce. We highlight fashion-domain specific gaps with typical implementations of personalised recommendation systems and present an alternate approach. Our approach hinges on user sessions (clickstream) as a proxy to the context and explores “session vector” as an atomic unit for personalization. The approach to learn context vector incorporates both the fashion product (style) attributes and the users’ browsing signals. We establish various possible user contexts (product clusters) and a style can have a fuzzy membership into multiple contexts. We predict the user’s context using the skip-gram model with negative sampling introduced by Mikolov et al [1]. We are able to decode the context with a high accuracy even for non-coherent sessions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M23S,1463,2d,hd,FALSE,https://i.ytimg.com/vi/4D1wG9dg8bw/maxresdefault.jpg,,653,16,0,0,0
176,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,25SrZbzFvb0,2016-11-08T13:17:45Z,8/11/16 13:17,Visual Product Discovery,"Author: Phil Long, Sentient Technologies USA LLC Abstract: We describe a system, Sentient Aware, that allows a user to interactively navigate through a catalog by viewing and clicking on images of products. When a user clicks on a product, she receives a new set of products to browse that is enriched for products that are similar to the clicked product. This continues, allowing the user to define an increasingly refined set of products, solely by expressing preferences between images of products. We describe the design of Sentient Aware, including its rationale, and some experiments. We also discuss limitations of our model of the problem, and potential alternatives. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M34S,1234,2d,hd,FALSE,https://i.ytimg.com/vi/25SrZbzFvb0/maxresdefault.jpg,,253,1,0,0,0
177,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,I870ganKZco,2016-10-10T17:14:02Z,10/10/16 17:14,Point-of-Interest Recommendations: Learning Potential Check-ins from Friends,"Author: Huayu Li, University of North Carolina at Charlotte Abstract: The emergence of Location-based Social Network (LBSN) services provides a wonderful opportunity to build personalized Point-of-Interest (POI) recommender systems. Although a personalized POI recommender system can signiﬁcantly facilitate users’ outdoor activities, it faces many challenging problems, such as the hardness to model user’s POI decision making process and the diﬃculty to address data sparsity and user/location cold-start problem. To cope with these challenges, we deﬁne three types of friends (i.e., social friends, location friends, and neighboring friends) in LBSN, and develop a two-step framework to leverage the information of friends to improve POI recommendation accuracy and address cold-start problem. Speciﬁcally, we ﬁrst propose to learn a set of potential locations that each individual’s friends have checked-in before and this individual is most interested in. Then we incorporate three types of check-ins (i.e., observed check-ins, potential check-ins and other unobserved check-ins) into matrix factorization model using two diﬀerent loss functions (i.e., the square error based loss and the ranking error based loss). To evaluate the proposed model, we conduct extensive experiments with many state-of-the-art baseline methods and evaluation metrics on two real-world data sets. The experimental results demonstrate the eﬀectiveness of our methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M53S,1193,2d,hd,FALSE,https://i.ytimg.com/vi/I870ganKZco/maxresdefault.jpg,,692,7,0,0,2
178,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ezqq9oE-icY,2016-10-10T17:13:23Z,10/10/16 17:13,QUINT: On Query-Specific Optimal Networks,"Author: Liangyue Li, School of Computing, Informatics and Decision Systems Engineering, Arizona State University Abstract: Measuring node proximity on large scale networks is a fundamental building block in many application domains, ranging from computer vision, e-commerce, social networks, software engineering, disaster management to biology and epidemiology. The state of the art (e.g., random walk based methods) typically assumes the input network is given a priori, with the known network topology and the associated edge weights. A few recent works aim to further infer the optimal edge weights based on the side information. This paper generalizes the challenge in multiple dimensions, aiming to learn optimal networks for node proximity measures. First (optimization scope), our proposed formulation explores a much larger parameter space, so that it is able to simultaneously infer the optimal network topology and the associated edge weights. This is important as a noisy or missing edge could greatly mislead the network node proximity measures. Second (optimization granularity), while all the existing works assume one common optimal network, be it given as the input or learned by the algorithms, exists for all queries, our method performs optimization at a much ﬁner granularity, essentially being able to infer an optimal network that is speciﬁc to a given query. Third (optimization eﬃciency), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user preference set. We perform extensive empirical evaluations on a diverse set of 10+ real networks, which show that the proposed algorithms (1) consistently outperform the existing methods on all six commonly used metrics; (2) empirically scale sub-linearly to billion-scale networks and (3) respond in a fraction of a second. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M5S,1325,2d,hd,FALSE,https://i.ytimg.com/vi/Ezqq9oE-icY/maxresdefault.jpg,,45,0,0,0,0
179,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,22FdVcAEtWw,2016-10-10T17:12:22Z,10/10/16 17:12,Communication Efficient Distributed Kernel Principal Component Analysis,"Author: Yingyu Liang, Department of Computer Science, Princeton University Abstract: Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing k principal components with relative error ε over s workers has communication cost Õ(sρκ/ε + sκ^2/ε^3) words, where ρ is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets. The experimental results showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M36S,1296,2d,hd,FALSE,https://i.ytimg.com/vi/22FdVcAEtWw/maxresdefault.jpg,,391,3,0,0,0
180,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,zRyj_Hvfxu8,2016-10-10T17:11:40Z,10/10/16 17:11,Unified Point-of-Interest Recommendation with Temporal Interval Assessment,"Author: Yanchi Liu, Rutgers, The State University of New Jersey Abstract: Point-of-interest (POI) recommendation, which helps mobile users explore new places, has become an important location-based service. Existing approaches for POI recommendation have been mainly focused on exploiting the information about user preferences, social inﬂuence, and geographical inﬂuence. However, these approaches cannot handle the scenario where users are expecting to have POI recommendation for a speciﬁc time period. To this end, in this paper, we propose a uniﬁed recommender system, named the ‘Where and When to gO’ (WWO) recommender system, to integrate the user interests and their evolving sequential preferences with temporal interval assessment. As a result, the WWO system can make recommendations dynamically for a speciﬁc time period and the traditional POI recommender system can be treated as the special case of the WWO system by setting this time period long enough. Speciﬁcally, to quantify users’ sequential preferences, we consider the distributions of the temporal intervals between dependent POIs in the historical check-in sequences. Then, to estimate the distributions with only sparse observations, we develop the low-rank graph construction model, which identiﬁes a set of bi-weighted graph bases so as to learn the static user preferences and the dynamic sequential preferences in a coherent way. Finally, we evaluate the proposed approach using real-world data sets from several location-based social networks (LBSNs). The experimental results show that our method outperforms the state-of-the-art approaches for POI recom-mendation in terms of various metrics, such as F-measure and NDCG, with a signiﬁcant margin. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M6S,1326,2d,hd,FALSE,https://i.ytimg.com/vi/zRyj_Hvfxu8/maxresdefault.jpg,,294,4,0,0,0
181,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZB6UpW-5eRw,2016-10-10T17:10:57Z,10/10/16 17:10,Rebalancing Bike Sharing Systems: A Multi-source Data Smart Optimization,"Author: Junming Liu, Rutgers, The State University of New Jersey Abstract: Bike sharing systems, aiming at providing the missing links in public transportation systems, are becoming popular in urban cities. A key to success for a bike sharing systems is the eﬀectiveness of rebalancing operations, that is, the eﬀort-s of restoring the number of bikes in each station to its target value by routing vehicles through pick-up and drop-oﬀ operations. There are two major issues for this bike rebalancing problem: the determination of station inventory target level and the large scale multiple capacitated vehicle routing optimization with outlier stations. The key challenges include demand prediction accuracy for inventory target level determination, and an eﬀective optimizer for vehicle routing with hundreds of stations. To this end, in this paper, we develop a Meteorology Similarity Weighted K-Nearest-Neighbor (M-SWK) regressor to predict the station pick-up demand based on large-scale historic trip records. Based on further analysis on the station network constructed by station-station connections and the trip duration, we propose an inter station bike transition (ISBT) model to predict the station drop-oﬀ demand. Then, we provide a mixed integer nonlinear programming (MINLP) formulation of multiple capacitated bike routing problem with the objective of minimizing total travel distance. To solve it, we propose an Adaptive Capacity Constrained K-centers Clustering (AdaCCKC) algorithm to separate outlier stations (the demands of these stations are very large and make the optimization infeasible) and group the rest stations into clusters within which one vehicle is scheduled to redistribute bikes between stations. In this way, the large scale multiple vehicle routing problem is reduced to inner cluster one vehicle routing problem with guaranteed feasible solutions. Finally, the extensive experimental results on the NYC Citi Bike system show the advantages of our approach for bike demand prediction and large-scale bike rebalancing optimization. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M56S,1136,2d,hd,FALSE,https://i.ytimg.com/vi/ZB6UpW-5eRw/maxresdefault.jpg,,529,8,0,0,1
182,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NiPEdnMS1F0,2016-10-10T17:10:15Z,10/10/16 17:10,AnyDBC: An Efficient Anytime Density-based Clustering Algorithm for Very Large Complex,"Author: Son Thai Mai, Department of Computer Science, University of Aarhus Abstract: The density-based clustering algorithm DBSCAN is a state-of-the-art data clustering technique with numerous applications in many ﬁelds. However, its O(n2) time complexity still remains a severe weakness. In this paper, we propose a novel anytime approach to cope with this problem by reducing both the range query and the label propagation time of DBSCAN. Our algorithm, called AnyDBC, compresses the data into smaller density-connected subsets called primitive clusters and labels objects based on connected components of these primitive clusters for reducing the label propagation time. Moreover, instead of passively performing the range query for all objects like existing techniques, AnyDBC iteratively and actively learns the current cluster structure of the data and selects a few most promising objects for reﬁning clusters at each iteration. Thus, in the end, it performs substantially fewer range queries compared to DBSCAN while still guaranteeing the exact ﬁnal result of DBSCAN. Experiments show speedup factors of orders of magnitude com-pared to DBSCAN and its fastest variants on very large real and synthetic complex datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M39S,1299,2d,hd,FALSE,https://i.ytimg.com/vi/NiPEdnMS1F0/maxresdefault.jpg,,345,0,0,0,0
183,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,lF11k2s1uWw,2016-10-10T17:09:29Z,10/10/16 17:09,Fast Memory-efficient Anomaly Detection in Streaming Heterogeneous Graphs,"Author: Emaad Manzoor, Carnegie Mellon University Abstract: Given a stream of heterogeneous graphs containing different types of nodes and edges, how can we spot anomalous ones in real-time while consuming bounded memory? This problem is motivated by and generalizes from its application in security to host-level advanced persistent threat (APT) detection. We propose StreamSpot, a clustering based anomaly detection approach that addresses challenges in two key fronts: (1) heterogeneity, and (2) streaming nature. We introduce a new similarity function for heterogeneous graphs that compares two graphs based on their relative frequency of local substructures, represented as short strings. This function lends itself to a vector representation of a graph, which is (a) fast to compute, and (b) amenable to a sketched version with bounded size that preserves similarity. StreamSpot exhibits desirable properties that a streaming application requires—it is (i) fully-streaming; processing the stream one edge at a time as it arrives, (ii) memory-eﬃcient; requiring constant space for the sketches and the clustering, (iii) fast; taking constant time to update the graph sketches and the cluster summaries that can process over 100K edges per second, and (iv) online; scoring and ﬂagging anomalies in real time. Experiments on datasets containing simulated system-call ﬂow graphs from normal browser activity and various attack scenarios (ground truth) show that StreamSpot is high-performance; achieving above 95% detection accuracy with small delay, as well as competitive time and memory usage. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M24S,1284,2d,hd,FALSE,https://i.ytimg.com/vi/lF11k2s1uWw/maxresdefault.jpg,,571,7,1,0,0
184,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hK2z41nsYZk,2016-10-10T17:08:48Z,10/10/16 17:08,Regime Shifts in Streams: Real-time Forecasting of Co-evolving Time Sequences,"Author: Yasuko Matsubara, Graduate School of Science and Technology, Kumamoto University Abstract: Given a large, online stream of multiple co-evolving event sequences, such as sensor data and Web-click logs, that contains various types of non-linear dynamic evolving patterns of different durations, how can we efﬁciently and effectively capture important patterns? How do we go about forecasting long-term future events? In this paper, we present REGIMECAST, an efﬁcient and effective method for forecasting co-evolving data streams. REGIME-CAST is designed as an adaptive non-linear dynamical system, which is inspired by the concept of “regime shifts” in natural dynamical systems. Our method has the following properties: (a) Effective: it operates on large data streams, captures important patterns and performs long-term forecasting; (b) Adaptive: it automatically and incrementally recognizes the latent trends and dynamic evolution patterns (i.e., regimes) that are unknown in advance; (c) Scalable: it is fast and the computation cost does not depend on the length of data streams; (d) Any-time: it provides a response at any time and generates long-range future events. Extensive experiments on real datasets demonstrate that REGIME-CAST does indeed make long-range forecasts, and it outperforms state-of-the-art competitors as regards accuracy and speed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M33S,1473,2d,hd,FALSE,https://i.ytimg.com/vi/hK2z41nsYZk/maxresdefault.jpg,,286,2,0,0,0
185,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,z6ow4s_NBgA,2016-10-10T17:08:03Z,10/10/16 17:08,Semi-Markov Switching Vector Autoregressive Model-based Anomaly Detection in Aviation Systems,"Author: Bryan Matthews, NASA Ames Research Center Abstract: In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are ﬂights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous ﬂight segments, due to mechanical, environmental, or human factors in order to identifying operationally signiﬁcant events and highlight potential safety risks. For this purpose, we propose a framework which represents each ﬂight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model’s prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M11S,1091,2d,hd,FALSE,https://i.ytimg.com/vi/z6ow4s_NBgA/maxresdefault.jpg,,741,3,1,0,0
186,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BQ3lUgctsks,2016-10-10T17:07:17Z,10/10/16 17:07,Skinny-dip: Clustering in a Sea of Noise,"Author: Samuel Maurus, Helmholtz Zentrum München - German Research Center for Environmental Health Abstract: Can we ﬁnd heterogeneous clusters hidden in data sets with 80% noise? Although such settings occur in the real-world, we struggle to ﬁnd methods from the abundance of clustering techniques that perform well with noise at this level. Indeed, perhaps this is enough of a departure from classical cluster-ing to warrant its study as a separate problem. In this paper we present SkinnyDip which, based on Hartigan’s elegant dip test of unimodality, represents an intriguing approach to clustering with an attractive set of properties. Speciﬁcally, SkinnyDip is highly noise-robust, practically parameter-free and completely deterministic. SkinnyDip never performs multivariate distance calculations, but rather employs in-sightful recursion based on “dips” into univariate projections of the data. It is able to detect a range of cluster shapes and densities, assuming only that each cluster admits a unimodal shape. Practically, its run-time grows linearly with the data. Finally, for high-dimensional data, continuity properties of the dip enable SkinnyDip to exploit multimodal projection pursuit in order to ﬁnd an appropriate basis for clustering. Although not without its limitations, SkinnyDip compares favorably to a variety of clustering approaches on synthetic and real data, particularly in high-noise settings. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M50S,1550,2d,hd,FALSE,https://i.ytimg.com/vi/BQ3lUgctsks/maxresdefault.jpg,,271,1,0,0,0
187,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,zlo7ca9b8pI,2016-10-10T17:06:38Z,10/10/16 17:06,Overcoming key weaknesses of Distance-based Neighbourhood Methods,"Author: Kai Ming Ting, Federation University Australia Abstract: This paper introduces the ﬁrst generic version of data dependent dissimilarity and shows that it provides a better closest match than distance measures for three existing algorithms in clustering, anomaly detection and multi-label classiﬁcation. For each algorithm, we show that by simply replacing the distance measure with the data dependent dissimilarity measure, it overcomes a key weakness of the otherwise unchanged algorithm. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M37S,1417,2d,hd,FALSE,https://i.ytimg.com/vi/zlo7ca9b8pI/maxresdefault.jpg,,60,0,0,0,0
188,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9o1bb5xdK70,2016-10-10T17:05:32Z,10/10/16 17:05,Continuous Experience-aware Language Model,"Author: Subhabrata Mukherjee, Max Planck Institute for Informatics, Max Planck Institute Abstract: Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience, and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also out-performs state-of-the-art methods for predicting item ratings. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M50S,1490,2d,hd,FALSE,https://i.ytimg.com/vi/9o1bb5xdK70/maxresdefault.jpg,,36,0,0,0,0
189,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BCiB9OECo4w,2016-10-10T17:04:56Z,10/10/16 17:04,Structural Neighborhood based Classification of Nodes in a Network,"Author: Sharad Nandanwar, Indian Institute of Science Bangalore Abstract: Classification of entities based on the underlying network structure is an important problem. Networks encountered in practice are sparse and have many missing and noisy links. Statistical learning techniques have been used in intra-network classification; however, they typically exploit only the local neighborhood, so may not perform well. In this paper, we propose a novel structural neighborhood-based classifier learning using a random walk. For classifying a node, we take a random walk from the node and make a decision based on how nodes in the respective kth-level neighborhood are labeled. We observe that random walks of short length are helpful in classification. Emphasizing role of longer random walks may cause the underlying Markov chain to converge to a stationary distribution. Considering this, we take a lazy random walk based approach with variable termination probability for each node, based on the node’s structural properties including its degree. Our experimental study on real world datasets demonstrates the superiority of the proposed approach over the existing state-of-the-art approaches. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT12M36S,756,2d,hd,FALSE,https://i.ytimg.com/vi/BCiB9OECo4w/maxresdefault.jpg,,263,1,0,0,0
190,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,w7pqa9_UlRw,2016-10-10T17:04:15Z,10/10/16 17:04,Modeling Precursors for Event Forecasting via Nested Multi-Instance Learning,"Author: Yue Ning, Department of Computer Science, Virginia Polytechnic Institute and State University Abstract: Forecasting large-scale societal events like civil unrest movements, disease outbreaks, and elections is an important and challenging problem. From the perspective of human analysts and policy makers, forecasting algorithms must not only make accurate predictions but must also provide sup-porting evidence, e.g., the causal factors related to the event of interest. We develop a novel multiple instance learning based approach that jointly tackles the problem of identifying evidence-based precursors and forecasts events into the future. Speciﬁcally, given a collection of streaming news articles from multiple sources we develop a nested multiple instance learning approach to forecast signiﬁcant societal events such as protests. Using data from three countries in Latin America, we demonstrate how our approach is able to consistently identify news articles considered as precursors for protests. Our empirical evaluation demonstrates the strengths of our proposed approach in ﬁltering candidate precursors, in forecasting the occurrence of events with a lead time advantage and in accurately predicting the characteristics of civil unrest events. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M15S,1395,2d,hd,FALSE,https://i.ytimg.com/vi/w7pqa9_UlRw/maxresdefault.jpg,,155,0,0,0,0
191,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3VMcwTDg4QY,2016-10-10T17:03:22Z,10/10/16 17:03,"The Limits of Popularity-Based Recommendations, and the Role of Social Ties","Author: Alessandro Panconesi, Sapienza University of Rome Abstract: In this paper we introduce a mathematical model that captures some of the salient features of recommender systems that are based on popularity and that try to exploit social ties among the users. We show that, under very general conditions, the market always converges to a steady state, for which we are able to give an explicit form. Thanks to this we can tell rather precisely how much a market is altered by a recommendation system, and determine the power of users to influence others. Our theoretical results are complemented by experiments with real world social networks showing that social graphs prevent large market distortions in spite of the presence of highly influential users. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M3S,1563,2d,hd,FALSE,https://i.ytimg.com/vi/3VMcwTDg4QY/maxresdefault.jpg,,147,1,0,0,1
192,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,COV37FrtAjY,2016-10-10T17:02:22Z,10/10/16 17:02,PTE: Enumerating Trillion Triangles On Distributed Systems,"Author: Ha-Myung Park, KAIST - Korea Advanced Institute of Science and Technology Abstract: How can we enumerate triangles from an enormous graph with billions of vertices and edges? Triangle enumeration is an important task for graph data analysis with many applications including identifying suspicious users in social networks, detecting web spams, ﬁnding communities, etc. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suﬀer from the massive shufﬂed data resulting in a very long processing time. In this paper, we propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural ineﬃciency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuﬄed data, total work, and network read. Experimental results show that PTE provides up to 47× faster performance than re-cent distributed algorithms on real world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges, which any previous triangle computation algorithm fail to process. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M29S,989,2d,hd,FALSE,https://i.ytimg.com/vi/COV37FrtAjY/maxresdefault.jpg,,177,3,0,0,0
193,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,33p-uR5swy0,2016-10-10T17:01:38Z,10/10/16 17:01,Robust Large-Scale Machine Learning in the Cloud,"Author: Steffen Rendle, Google, Inc. Abstract: The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of ma-chines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD’s cost eﬀectiveness and scalability. Using Google’s internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the ‘large-scale’ Netﬂix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M2S,1502,2d,hd,FALSE,https://i.ytimg.com/vi/33p-uR5swy0/maxresdefault.jpg,,396,4,0,0,0
194,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,KP7-JtFMLo4,2016-10-10T17:00:58Z,10/10/16 17:00,Why Should I Trust you? Explaining the Predictions of Any Classifier,"Author: Marco Tulio Ribeiro, Department of Computer Science and Engineering, University of Washington Abstract: Despite widespread adoption, machine learning models re- main mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M26S,1466,2d,hd,FALSE,https://i.ytimg.com/vi/KP7-JtFMLo4/maxresdefault.jpg,,16993,267,2,0,7
195,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,m2AAGEA5tqw,2016-10-10T17:00:17Z,10/10/16 17:00,ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages,"Author: Matteo Riondato, Two Sigma Investments, LP Abstract: We present ABRA, a suite of algorithms to compute and maintain probabilistically-guaranteed, high-quality, approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, this is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime and number of samples, state-of-the-art algorithms with the same quality guarantees. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M6S,1266,2d,hd,FALSE,https://i.ytimg.com/vi/m2AAGEA5tqw/maxresdefault.jpg,,139,0,0,0,1
196,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_CmPLW7mbOs,2016-10-10T16:59:36Z,10/10/16 16:59,Sampling of Attributed Networks From Hierarchical Generative Models,"Author: Pablo Robles Granda, Department of Computer Science, Purdue University Abstract: Network sampling is a widely used procedure in social net-work analysis where a random network is sampled from a generative network model (GNM). Recently proposed GNMs, allow generation of networks with more realistic structural characteristics than earlier ones. This facilitates tasks such as hypothesis testing and sensitivity analysis. However, sampling of networks with correlated vertex attributes remains a challenging problem. While the recent work of [16] has provided a promising approach for attributed-network sampling, the approach was developed for use with relatively simple GNMs and does not work well with more complex hierarchical GNMs (which can model the range of characteristics and variation observed in real world networks more accurately). In contrast to simple GNMs where the probability mass is spread throughout the space of edges more evenly, hierarchical GNMs concentrate the mass to smaller regions of the space to reﬂect dependencies among edges in the network—this produces more realistic network characteristics, but also makes it more diﬃcult to identify candidate networks from the sampling space. In this paper, we propose a novel sampling method, CSAG, to sample from hierarchical GNMs and generate networks with correlated attributes. CSAG constrains every step of the sampling process to consider the structure of the GNM—in order to bias the search to regions of the space with higher likelihood. We implemented CSAG using mixed Kronecker Product Graph Models and evaluated our approach on three real-world datasets. The results show that CSAG jointly models the correlation and structure of the networks better than the state of the art. Speciﬁcally, CSAG maintains the variability of the underlying GNM while pro-viding a ≥ 5X reduction in attribute correlation error. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M52S,1432,2d,hd,FALSE,https://i.ytimg.com/vi/_CmPLW7mbOs/maxresdefault.jpg,,107,0,0,0,0
197,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MPWVBynYpj4,2016-10-10T16:58:55Z,10/10/16 16:58,Goal-Directed Inductive Matrix Completion,"Author: Si Si, Department of Computer Science, University of Texas at Austin Abstract: Matrix completion (MC) with additional information has found wide applicability in several machine learning applications. Among algorithms for solving such problems, Inductive Matrix Completion(IMC) has drawn a considerable amount of attention, not only for its well established theoretical guarantees but also for its superior performance in various real-world applications. However, IMC based methods usually place very strong constraints on the quality of the features (side information) to ensure accurate recovery, which might not be met in practice. In this paper, we pro-pose Goal-directed Inductive Matrix Completion(GIMC) to learn a nonlinear mapping of the features so that they satisfy the required properties. A key distinction between GIMC and IMC is that the feature mapping is learnt in a supervised manner, deviating from the traditional approach of un-supervised feature learning followed by model training. We establish the superiority of our method on several popular machine learning applications including multi-label learning, multi-class classiﬁcation, and semi-supervised clustering. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M42S,1242,2d,hd,FALSE,https://i.ytimg.com/vi/MPWVBynYpj4/maxresdefault.jpg,,186,1,0,0,0
198,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,q7CQNtjizmQ,2016-10-10T16:57:29Z,10/10/16 16:57,Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data,"Author: Payam Siyari, Georgia Institute of Technology Abstract: Data represented as strings abounds in biology, linguistics, document mining, web search and many other ﬁelds. Such data often have a hierarchical structure, either because they were artiﬁcially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as Lexis, that produces an optimized hierarchical representation of a given set of “target” strings. The resulting hierarchy, “Lexis-DAG”, shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-hardness for two cost formulations, we propose an eﬃcient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the “core” of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M15S,1335,2d,hd,FALSE,https://i.ytimg.com/vi/q7CQNtjizmQ/maxresdefault.jpg,,157,1,0,0,0
199,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xk6EJhlCVrc,2016-10-10T16:55:09Z,10/10/16 16:55,Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches,"Author: Daniel Ting, Facebook Abstract: Estimating the cardinality of unions and intersections of sets is a problem of interest in OLAP. Large data applications often require the use of approximate methods based on small sketches of the data. We give new estimators for the cardinality of unions and intersection and show they approximate an optimal estimation procedure. These estimators enable the improved accuracy of the streaming MinCount sketch to be exploited in distributed settings. Both theoretical and empirical results demonstrate substantial improvements over existing methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M58S,1618,2d,hd,FALSE,https://i.ytimg.com/vi/xk6EJhlCVrc/maxresdefault.jpg,,231,4,0,0,0
200,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3nYTU6vwy1s,2016-10-10T16:54:31Z,10/10/16 16:54,Just One More: Modeling Binge Watching Behavior,"Author: William Trouleau, École Polytechnique Fédérale de Lausanne Abstract: Easy accessibility can often lead to over-consumption, as seen in food and alcohol habits. On video on-demand (VOD) services, this has recently been referred to as “binge watching”, where potentially entire seasons of TV shows are consumed in a single viewing session. While a user viewership model may reveal this binging behavior, creating an accurate model has several challenges, including censored data, deviations in the population, and the need to consider external inﬂuences on consumption habits. In this paper, we introduce a novel statistical mixture model that incorporates these factors and presents a “ﬁrst of its kind” characterization of viewer consumption behavior using a real-world dataset that includes playback data from a VOD service. From our modeling, we tackle various predictive tasks to infer the consumption decisions of a user in a viewing session, including estimating the number of episodes they watch and classifying if they continue watching another episode. Using these insights, we then identify binge watching sessions based on deviation from normal viewing behavior. We observe different types of binging behavior, that binge watchers often view certain content out-of-order, and that binge watching is not a consistent behavior among our users. These insights and our ﬁndings have application in VOD revenue generation, consumer health applications, and customer retention analysis. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M10S,970,2d,hd,FALSE,https://i.ytimg.com/vi/3nYTU6vwy1s/maxresdefault.jpg,,148,3,0,0,0
201,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,b9Afq18oFkI,2016-10-10T16:52:41Z,10/10/16 16:52,Dynamic Clustering of Streaming Short Documents,"Author: Weinan Zhang, Department of Computer Science and Engineering, Shanghai Jiao Tong University Abstract: Clustering technology has found numerous applications in mining textual data. It was shown to enhance the performance of retrieval systems in various different ways, such as identifying different query aspects in search result diversification, improving smoothing in the context of language modeling, matching queries with documents in a latent topic space in ad-hoc retrieval, summarizing documents etc. The vast majority of clustering methods have been developed under the assumption of a static corpus of long (and hence textually rich) documents. Little attention has been given to streaming corpora of short text, which is the predominant type of data in Web 2.0 applications, such as social media, forums, and blogs. In this paper, we consider the problem of dynamically clustering a streaming corpus of short documents. The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time. To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster-based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M2S,962,2d,hd,FALSE,https://i.ytimg.com/vi/b9Afq18oFkI/maxresdefault.jpg,,622,2,0,0,0
202,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,WqSDXI04IOM,2016-10-10T16:51:55Z,10/10/16 16:51,Partial Label Learning via Feature-Aware Disambiguation,"Author: Xiangnan Kong, Department of Computer Science, Worcester Polytechnic Institute Abstract: Partial label learning deals with the problem where each training example is represented by a feature vector while associated with a set of candidate labels, among which only one label is valid. To learn from such ambiguous labeling information, the key is to try to disambiguate the candidate label sets of partial label training examples. Existing disambiguation strategies work by either identifying the ground-truth label iteratively or treating each candidate label equally. Nonetheless, the disambiguation process is generally conducted by focusing on manipulating the label space, and thus ignores making full use of potentially useful information from the feature space. In this paper, a novel two-stage approach is proposed to learning from partial label examples based on feature-aware disambiguation. In the ﬁrst stage, the manifold structure of feature space is utilized to generate normalized labeling conﬁdences over candidate label set. In the second stage, the predictive model is learned by performing regularized multi-output regression over the generated labeling conﬁdences. Extensive experiments on artiﬁcial as well as real-world partial label data sets clearly validate the superiority of the proposed feature-aware disambiguation approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M58S,1138,2d,hd,FALSE,https://i.ytimg.com/vi/WqSDXI04IOM/maxresdefault.jpg,,187,2,0,0,0
203,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,SHLWSQkvnL4,2016-10-10T16:45:32Z,10/10/16 16:45,Structural Deep Network Embedding,"Author: Daixin Wang, Tsinghua University Abstract: Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link pre-diction and visualization. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M25S,1345,2d,hd,FALSE,https://i.ytimg.com/vi/SHLWSQkvnL4/maxresdefault.jpg,,1714,10,2,0,5
204,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,umDde5zr6ns,2016-10-10T16:44:48Z,10/10/16 16:44,Structured Doubly Stochastic Matrix for Graph Based Clustering,"Author: Xiaoqian Wang, Department of Computer Science and Engineering, University of Texas at Arlington Abstract: As one of the most signiﬁcant machine learning topics, clustering has been extensively employed in various kinds of area. Its prevalent application in scientiﬁc research as well as industrial practice has drawn high attention in this day and age. A multitude of clustering methods have been developed, among which the graph based clustering method using the afﬁnity matrix has been laid great ephasis on. Recent research work used the doubly stochastic matrix to normalize the input afﬁnity matrix and enhance the graph based clustering models. Although the doubly stochastic matrix can improve the clustering performance, the clustering structure in the doubly stochastic matrix is not clear as expected. Thus, post-processing step is required to extract the ﬁnal clustering results, which may not be optimal. To address this problem, in this paper, we propose a novel convex model to learn the structured doubly stochastic matrix by imposing low-rank constraint on the graph Laplacian matrix. Our new structured doubly stochastic matrix can explicitly uncover the clustering structure and encode the probabilities of pair-wise data points to be connected, such that the clustering results are enhanced. An efﬁcient optimization algorithm is derived to solve our new objective. Also, we provide theoretical discussions that when the input differs, our method possesses interesting connections with K-means and spectral graph cut models respectively. We conduct experiments on both synthetic and bench-mark datasets to validate the performance of our proposed method. The empirical results demonstrate that our model provides an approach to better solving the K-mean clustering problem. By using the cluster indicator provided by our model as initialization, K-means converges to a smaller objective function value with better clustering performance. Moreover, we compare the clustering performance of our model with spectral clustering and related double stochastic model. On all datasets, our method performs equally or better than the related methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M30S,1050,2d,hd,FALSE,https://i.ytimg.com/vi/umDde5zr6ns/maxresdefault.jpg,,625,5,1,0,1
205,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xOdSCu9wdwY,2016-10-10T16:42:26Z,10/10/16 16:42,Targeted Topic Modeling for Focused Analysis,"Author: Shuai Wang, Department of Computer Science, University of Illinois at Chicago Abstract: One of the overarching tasks of document analysis is to ﬁnd what topics people talk about. One of the main techniques for this purpose is topic modeling. So far many models have been proposed. However, the existing models typically per-form full analysis on the whole data to ﬁnd all topics. This is certainly useful, but in practice we found that the user almost always also wants to perform more detailed analyses on some speciﬁc aspects, which we refer to as targets (or targeted aspects). Current full-analysis models are not suitable for such analyses as their generated topics are often too coarse and may not even be on target. For example, given a set of tweets about e-cigarette, one may want to ﬁnd out what topics under discussion are speciﬁcally related to children. Likewise, given a collection of online reviews about a camera, a consumer or camera manufacturer may be interested in ﬁnding out all topics about the camera’s screen, the targeted aspect. As we will see in our experiments, current full topic models are ineﬀective for such targeted analyses. This paper studies this problem and proposes a novel targeted topic model (TTM) to enable focused analyses on any speciﬁc aspect of interest. Our experimental results demonstrate the eﬀectiveness of the TTM. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M56S,1496,2d,hd,FALSE,https://i.ytimg.com/vi/xOdSCu9wdwY/maxresdefault.jpg,,154,0,0,0,0
206,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,T-4etdFcyYw,2016-10-10T16:41:27Z,10/10/16 16:41,A multiple test correction for streams and cascades of statistical hypothesis tests,"Author: Geoff Webb, Faculty of Information Technology, Monash University Abstract: Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, i.e. rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be predetermined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even the number of hypotheses, can be known in advance. This paper introduces Subfamilywise Multiple Testing, a multiple-testing correction that can be used in applications for which there are repeated pools of null hypotheses from each of which a single null hypothesis is to be rejected and neither the specific hypotheses nor their number are known until the final rejection decision is completed. To demonstrate the importance and relevance of this work to current machine learning problems, we further refine the theory to the problem of model selection and show how to use Subfamilywise Multiple Testing for learning graphical models. We assess its ability to discover graphical models on more than 7,000 datasets, studying the ability of Subfamilywise Multiple Testing to outperform the state of the art on data with varying size and dimensionality, as well as with varying density and power of the present correlations. Subfam-ilywise Multiple Testing provides a significant improvement in statistical efficiency, often requiring only half as much data to discover the same model, while strictly controlling FWER. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT28M51S,1731,2d,hd,FALSE,https://i.ytimg.com/vi/T-4etdFcyYw/maxresdefault.jpg,,255,0,0,0,0
207,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HD4hGvT-97g,2016-10-10T16:40:48Z,10/10/16 16:40,Revisiting Random Binning Feature: Fast Convergence and Strong Parallelizability,"Author: Lingfei Wu, Department of Computer Science, College of William & Mary Abstract: Kernel method has been developed as one of the standard approaches for nonlinear learning, which however, does not scale to large data set due to its quadratic complexity in the number of samples. A number of kernel approximation methods have thus been proposed in the recent years, among which the random features method gains much popularity due to its simplicity and direct reduction of non-linear problem to a linear one. Different random feature functions have since been proposed to approximate a variety of kernel functions. Among them the Random Binning (RB) feature, proposed in the first random-feature paper [21], has drawn much less attention than the Random Fourier (RF) feature proposed also in [21]. In this work, we observe that the RB features, with right choice of optimization solver, could be orders-of-magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy. We thus propose the first analysis of RB from the perspective of optimization, which by interpreting RB as a Randomized Block Coordinate Descent in the infinite-dimensional space, gives a faster convergence rate compared to that of other random features. In particular, we show that by drawing R random grids with at least κ number of non-empty bins per grid in expectation, RB method achieves a convergence rate of O(1/(κR)), which not only sharpens its O(1/√R) rate from Monte Carlo analysis, but also shows a κ times speedup over other random features under the same analysis framework. In addition, we demonstrate another advantage of RB in the L1-regularized set-ting, where unlike other random features, a RB-based Coordinate Descent solver can be parallelized with guaranteed speedup proportional to κ. Our extensive experiments demonstrate the superior performance of the RB features over other random features and ker-nel approximation methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M47S,1427,2d,hd,FALSE,https://i.ytimg.com/vi/HD4hGvT-97g/maxresdefault.jpg,,220,1,1,0,0
208,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,E8J1jolkriY,2016-10-10T16:29:26Z,10/10/16 16:29,Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective,"Author: Tong Xu, Department of Computer Science and Technology, East China Normal University Abstract: With recent advances in mobile and sensor technologies, a large amount of eﬀorts have been made on developing intelligent applications for taxi drivers, which provide beneﬁcial guide and opportunity to improve the proﬁt and work eﬃciency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation aﬀects for better prediction of cab drivers’ future behaviors. To be speciﬁc, we ﬁrst investigate the correlation between drivers’ skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the eﬀects of social inﬂuences. Along this line, by leveraging the classic social inﬂuence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the eﬀectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M47S,1067,2d,hd,FALSE,https://i.ytimg.com/vi/E8J1jolkriY/maxresdefault.jpg,,331,0,0,0,0
209,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ue8FeypMCL4,2016-10-10T16:28:42Z,10/10/16 16:28,Robust Extreme Multi-label Learning,"Author: Chang Xu, Peking University Abstract: Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the signiﬁcance of investigating tail labels and the eﬀectiveness of the proposed algorithm. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M26S,806,2d,hd,FALSE,https://i.ytimg.com/vi/Ue8FeypMCL4/maxresdefault.jpg,,380,0,0,0,1
210,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0mXHZGqkvw8,2016-10-10T16:27:49Z,10/10/16 16:27,DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks,"Author: Shuangfei Zhai, Department of Computer Science, Thomas J. Watson School of Engineering and Applied Sciences, Binghamton University, State University of New York Abstract: In this paper, we investigate the use of recurrent neural networks (RNNs) in the context of search-based online advertising. We use RNNs to map both queries and ads to real valued vectors, with which the relevance of a given (query, ad) pair can be easily computed. On top of the RNN, we propose a novel attention network, which learns to assign attention scores to diﬀerent word locations according to their intent importance (hence the name DeepIntent). The vector output of a sequence is thus computed by a weighted sum of the hidden states of the RNN at each word according their attention scores. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs, which are sampled from a commercial search engine. We show that in most cases the attention network improves the quality of learned vector representations, evaluated by AUC on a manually labeled dataset. Moreover, we highlight the eﬀectiveness of the learned attention scores from two aspects: query rewriting and a modiﬁed BM25 metric. We show that using the learned attention scores, one is able to produce sub-queries that are of better qualities than those of the state-of-the-art methods. Also, by modifying the term frequency with the attention scores in a standard BM25 formula, one is able to improve its performance evaluated by AUC. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT17M31S,1051,2d,hd,FALSE,https://i.ytimg.com/vi/0mXHZGqkvw8/maxresdefault.jpg,,551,1,1,0,0
211,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,LJFydGxFniw,2016-10-10T16:25:43Z,10/10/16 16:25,Annealed Sparsity via Adaptive and Dynamic Shrinking,"Author: Kai Zhang, NEC Laboratories America, Inc. Abstract: Sparse learning has received tremendous amount of interest in high-dimensional data analysis due to its model interpretability and the low-computational cost. Among the various techniques, adaptive l1-regularization is an eﬀective framework to improve the convergence behaviour of the LASSO, by using varying strength of regularization across diﬀerent features. In the meantime, the adaptive structure makes it very powerful in modelling grouped sparsity pat-terns as well, being particularly useful in high-dimensional multi-task problems. However, choosing an appropriate, global regularization weight is still an open problem. In this paper, inspired by the annealing technique in matrial science, we propose to achieve “annealed sparsity” by designing a dynamic shrinking scheme that simultaneously optimizes the regularization weights and model coeﬃcients in sparse (multi-task) learning. The dynamic structures of our algorithm are twofold. Feature-wise (“spatially”), the regularization weights are updated interactively with model coeﬃcients, allowing us to improve the global regularization structure. Iteration-wise (“temporally”), such interaction is coupled with gradually boosted l1-regularization by adjusting an equality norm-constraint, achieving an “annealing” eﬀect to further improve model selection. This renders interesting shrinking behaviour in the whole solution path. Our method competes favorably with state-of-the-art methods in sparse (multi-task) learning. We also apply it in expression quantitative trait loci analysis (eQTL), which gives useful biological insights in human cancer (melanoma) study. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M46S,1426,2d,hd,FALSE,https://i.ytimg.com/vi/LJFydGxFniw/maxresdefault.jpg,,50,0,0,0,0
212,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0eh-5_bVsh4,2016-10-10T16:16:03Z,10/10/16 16:16,Asymmetric Transitivity Preserving Graph Embedding,"Author: Ziwei Zhang, Tsinghua University Abstract: Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embed-ding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More speciﬁcally, we ﬁrst derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities signiﬁcantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT16M22S,982,2d,hd,FALSE,https://i.ytimg.com/vi/0eh-5_bVsh4/maxresdefault.jpg,,424,6,0,0,1
213,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,dMF9tJBEH6U,2016-10-10T16:14:56Z,10/10/16 16:14,NetCycle: Collective Evolution Inference in Heterogeneous Information Networks,"Author: Yizhou Zhang, Department of Computer Science and Engineering, Fudan University Abstract: Collective inference has attracted considerable attention in the last decade, where the response variables within a group of instances are correlated and should be inferred collectively, instead of independently. Previous works on collective inference mainly focus on exploiting the autocorrelation among instances in a static network during the inference process. There are also approaches on time series prediction, which mainly exploit the autocorrelation within an instance at diﬀerent time points during the inference process. However, in many real-world applications, the response variables of related instances can coevolve over time and their evolutions are not following a static correlation across time, but are following an internal life cycle. In this paper, we study the problem of collective evolution inference, where the goal is to predict the values of the response variables for a group of related instances at the end of their life cycles. This problem is extremely important for various applications, e.g., predicting fund-raising results in crowd-funding and predicting gene-expression levels in bioinformatics. This problem is also highly challenging because diﬀerent instances in the network can co-evolve over time and they can be at diﬀerent stages of their life cycles and thus have diﬀerent evolving patterns. Moreover, the instances in collective evolution inference problems are usually connected through heterogeneous information networks, which involve complex relationships among the instances interconnected by multiple types of links. We propose an approach, called NetCycle, by incorporating information from both the correlation among related instances and their life cycles. We compared our approach with existing methods of collective inference and time series analysis on two real-world networks. The results demonstrate that our proposed approach can improve the inference performance by considering the autocorrelation through networks and the life cycles of the instances. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M40S,1180,2d,hd,FALSE,https://i.ytimg.com/vi/dMF9tJBEH6U/maxresdefault.jpg,,93,1,0,0,0
214,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,EIlrXFBSplg,2016-10-10T16:13:50Z,10/10/16 16:13,Come-and-Go Patterns of Group Evolution: A Dynamic Model,"Author: Tianyang Zhang, Tsinghua University Abstract: How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups? In this paper, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model comeNgo, a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) accuracy: it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58% over baseline. (d) usefulness: it can be used in multiple application scenarios such as forecasting and pattern discovery. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M36S,1236,2d,hd,FALSE,https://i.ytimg.com/vi/EIlrXFBSplg/maxresdefault.jpg,,99,1,0,0,0
215,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,A-O2MMbbdNc,2016-10-10T16:12:30Z,10/10/16 16:12,FINAL: Fast Attributed Network Alignment,"Author: Si Zhang, Department of Computer Science and Engineering, Arizona State University Abstract: Multiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., ﬁnding the node correspondence across diﬀerent networks) is often the very ﬁrst step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich at-tribute information on nodes and/or edges. In this paper, we propose a family of algorithms (FINAL) to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from an optimization perspective based on the alignment consistency principle, and develop eﬀective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can signiﬁcantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10× speed-up, while preserving a 95% ac-curacy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and a near real-time response time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M6S,1266,2d,hd,FALSE,https://i.ytimg.com/vi/A-O2MMbbdNc/maxresdefault.jpg,,189,0,0,0,0
216,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xbxHSb3Z38E,2016-10-10T16:11:31Z,10/10/16 16:11,Approximate Personalized PageRank on Dynamic Graphs,"Author: Hongyang Zhang, Department of Computer Science, Stanford University Abstract: We propose and analyze two algorithms for maintaining approximate Personalized PageRank (PPR) vectors on a dynamic graph, where edges are added or deleted. Our algorithms are natural dynamic versions of two known local variations of power iteration. One, Forward Push, propagates probability mass forwards along edges from a source node, while the other, Reverse Push, propagates local changes backwards along edges from a target. In both variations, we maintain an invariant between two vectors, and when an edge is updated, our algorithm ﬁrst modiﬁes the vectors to restore the invariant, then performs any needed local push operations to restore accuracy. For Reverse Push, we prove that for an arbitrary directed graph in a random edge model, or for an arbitrary undirected graph, given a uniformly random target node t, the cost to maintain a PPR vector to t of additive error ε as k edges are updated is O(k + d/ε), where d is the average degree of the graph. This is O(1) work per update, plus the cost of computing a reverse vector once on a static graph. For Forward Push, we show that on an arbitrary undirected graph, given a uniformly random start node s, the cost to maintain a PPR vector from s of degree-normalized error ε as k edges are updated is O(k + 1/ε), which is again O(1) per update plus the cost of computing a PPR vector once on a static graph. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M12S,1092,2d,hd,FALSE,https://i.ytimg.com/vi/xbxHSb3Z38E/maxresdefault.jpg,,536,6,0,0,0
217,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,N66UGAslV14,2016-10-10T16:10:35Z,10/10/16 16:10,GMove: Group-Level Mobility Modeling using Geo-Tagged Social Media,"Author: Chao Zhang, Department of Computer Science, University of Illinois at Urbana-Champaign Abstract: Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and location prediction. While there has been fruitful research on modeling human mobility using tracking data (e.g., GPS traces), the recent growth of geo-tagged social media (GeoSM) brings new opportunities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GeoSM data remains a challenge that cannot be readily addressed by existing techniques. We propose GMOVE, a group-level mobility modeling method using GeoSM data. Our insight is that the GeoSM data usually contains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GMOVE thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Furthermore, to reduce text sparsity of GeoSM data, GMOVE also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correlations as auxiliary knowledge, it performs sampling-based augmentation to alleviate text sparsity and produce high-quality HMMs. Our extensive experiments on two real-life data sets demonstrate that GMOVE can effectively generate meaningful group-level mobility models. Moreover, with context-aware location prediction as an example application, we find that GMOVE significantly outperforms baseline mobility models in terms of prediction accuracy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M35S,1115,2d,hd,FALSE,https://i.ytimg.com/vi/N66UGAslV14/maxresdefault.jpg,,165,1,0,0,0
218,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TWE4NxMWTgM,2016-10-10T16:09:35Z,10/10/16 16:09,Accelerating Online CP Decompositions for Higher Order Tensors,"Author: Shuo Zhou, Department of Computing and Information Systems, The University of Melbourne Abstract: Tensors are a natural representation for multidimensional data. In recent years, CANDECOMP/PARAFAC (CP) decomposition, one of the most popular tools for analyzing multi-way data, has been extensively studied and widely applied. However, today’s datasets are often dynamically changing over time. Tracking the CP decomposition for such dynamic tensors is a crucial but challenging task, due to the large scale of the tensor and the velocity of new data arriving. Traditional techniques, such as Alternating Least Squares (ALS), cannot be directly applied to this problem because of their poor scalability in terms of time and memory. Additionally, existing online approaches have only partially addressed this problem and can only be deployed on third-order tensors. To ﬁll this gap, we propose an efﬁcient online algorithm that can incrementally track the CP decompositions of dynamic tensors with an arbitrary number of dimensions. In terms of eﬀectiveness, our algorithm demonstrates comparable results with the most accurate algorithm, ALS, whilst being computationally much more eﬃcient. Speciﬁcally, on small and moderate datasets, our approach is tens to hundreds of times faster than ALS, while for large-scale datasets, the speedup can be more than 3,000 times. Compared to other state-of-the-art online approaches, our method shows not only signiﬁcantly better decomposition quality, but also better performance in terms of stability, eﬃciency and scalability. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M24S,1344,2d,hd,FALSE,https://i.ytimg.com/vi/TWE4NxMWTgM/maxresdefault.jpg,,387,3,0,0,2
219,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VamnR-_i9JY,2016-10-10T16:08:45Z,10/10/16 16:08,Compact and Scalable Graph Neighborhood Sketching,"Author: Takuya Akiba, National Institute of Informatics Abstract: The all-distances sketch (ADS) has recently emerged as a promising paradigm of graph neighborhood sketching. An ADS is a probabilistic data structure that is deﬁned for each vertex of a graph. ADSs facilitate accurate estimation of many useful indicators for network analysis with the guarantee of accuracy, and the ADSs for all the vertices in a graph can be computed in near-linear time. Because of these useful properties, ADS has attracted considerable attention. However, a critical drawback of ADS is its space requirement, which tends to be much larger than that of the graph itself. In the present study, we address this issue by designing a new graph sketching scheme, namely, sketch retrieval shortcuts (SRS). Although SRSs are more space-eﬃcient than ADSs by an order of magnitude, an ADS of any vertex can be quickly retrieved from the SRSs. The retrieved ADSs can be used to estimate the aforementioned indicators in exactly the same manner as with plain ADSs, inheriting the same accuracy guarantee. Our experiments on real-world networks demonstrate the usefulness of SRSs as a practical back-end of large-scale graph data mining. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M31S,1351,2d,hd,FALSE,https://i.ytimg.com/vi/VamnR-_i9JY/maxresdefault.jpg,,336,1,0,0,0
220,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,D6IhYvvQL6g,2016-10-10T16:08:09Z,10/10/16 16:08,Assessing Human Error Against a Benchmark of Perfection,"Author: Ashton Anderson, Microsoft Research Abstract: An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors. To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging even for the best players in the world. We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difﬁculty of the decision. We identify rich structure in all three of these categories of features, and ﬁnd strong evidence that in our domain, features de-scribing the inherent difﬁculty of an instance are signiﬁcantly more powerful than features based on skill or time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M45S,1605,2d,hd,FALSE,https://i.ytimg.com/vi/D6IhYvvQL6g/maxresdefault.jpg,,40,0,0,0,0
221,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0kEoX2Nq2rU,2016-10-10T16:07:29Z,10/10/16 16:07,Inferring Network Effects from Observational Data,"Author: David Arbour, College of Information and Computer Sciences, University of Massachusetts Amherst Abstract: We present Relational Covariate Adjustment (RCA), a general method for estimating causal effects in relational data. Relational Covariate Adjustment is implemented through two high-level operations: identification of an adjustment set and relational regression adjustment. The former is achieved through an extension of Pearl’s back-door criterion to relational domains. We demonstrate how this extended definition can be used to estimate causal effects in the presence of network interference and confounding. RCA is agnostic to functional form, and it can easily model both discrete and continuous treatments as well as estimate the effects of a wider array of network interventions than existing experimental approaches. We show that RCA can yield robust estimates of causal effects using common regression models without extensive parameter tuning. Through a series of simulation experiments on a variety of synthetic and real- world network structures, we show that causal effects estimated on observational data with RCA are nearly as accurate as those estimated from well-designed network experiments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M30S,1410,2d,hd,FALSE,https://i.ytimg.com/vi/0kEoX2Nq2rU/maxresdefault.jpg,,95,1,0,0,0
222,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,H9nHqQOdbC0,2016-10-10T16:06:42Z,10/10/16 16:06,Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns,"Author: Roel Bertens, Department of Information and Computing Sciences, Utrecht University Abstract: We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover high-quality pattern sets directly from data, we introduce DITTO, a highly efficient algorithm that approximates the ideal result very well. Experiments show that DITTO correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, DITTO discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M11S,1331,2d,hd,FALSE,https://i.ytimg.com/vi/H9nHqQOdbC0/maxresdefault.jpg,,65,1,0,0,0
223,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YWSCMcIQcWk,2016-10-10T16:05:48Z,10/10/16 16:05,Positive Unlabeled Learning in Streaming Networks,"Author: Shiyu Chang, University of Illinois at Urbana-Champaign Abstract: Data of many problems in real-world systems such as link prediction and one-class recommendation share common characteristics. First, data are in the form of positive-unlabeled (PU) measurements ( e.g. Twitter “following”, Facebook “like”, etc.) that do not provide negative information, which can be naturally represented as networks. Second, in the era of big data, such data are generated temporally-ordered, continuously and rapidly, which determines its streaming nature. These common characteristics allow us to unify many problems into a novel framework - PU learning in streaming networks. In this paper, a principled probabilistic approach SPU is proposed to leverage the characteristics of the streaming PU inputs. In particular, SPU captures temporal dynamics and provides real-time adaptations and predictions by identifying the potential negative signals concealed in unlabeled data. Our empirical results on various real-world datasets demonstrate the effectiveness of the proposed framework over other state-of-the-art methods in both link prediction and recommendation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M57S,1497,2d,hd,FALSE,https://i.ytimg.com/vi/YWSCMcIQcWk/maxresdefault.jpg,,742,1,0,0,1
224,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8Y-droPeKu8,2016-10-10T16:04:59Z,10/10/16 16:04,XGBoost: A Scalable Tree Boosting System,"Author: Tianqi Chen, Department of Computer Science and Engineering, University of Washington Abstract: Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M41S,1481,2d,hd,FALSE,https://i.ytimg.com/vi/8Y-droPeKu8/maxresdefault.jpg,,15304,67,26,0,4
225,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,IPOvXd9vZPs,2016-10-10T16:03:39Z,10/10/16 16:03,Robust Influence Maximization,"Author: Wei Chen, Microsoft Research Asia, Microsoft Research Abstract: In this paper, we address the important issue of uncertainty in the edge inﬂuence probability estimates for the well studied inﬂuence maximization problem - the task of ﬁnding k seed nodes in a social network to maximize the inﬂuence spread. We propose the problem of robust inﬂuence maximization, which maximizes the worst-case ratio between the inﬂuence spread of the chosen seed set and the optimal seed set, given the uncertainty of the parameter input. We de-sign an algorithm that solves this problem with a solution-dependent bound. We further study uniform sampling and adaptive sampling methods to eﬀectively reduce the uncertainty on parameters and improve the robustness of the inﬂuence maximization task. Our empirical results show that parameter uncertainty may greatly aﬀect inﬂuence maximization performance and prior studies that learned inﬂuence probabilities could lead to poor performance in robust inﬂuence maximization due to relatively large uncertainty in parameter estimates, and information cascade based adaptive sampling method may be an eﬀective way to improve the robustness of inﬂuence maximization. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M20S,1580,2d,hd,FALSE,https://i.ytimg.com/vi/IPOvXd9vZPs/maxresdefault.jpg,,244,3,0,0,1
226,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_T9U0Dm_FzQ,2016-10-10T16:02:59Z,10/10/16 16:02,FASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks,"Author: Chen Chen, Department of Computer Science and Engineering, Arizona State University Abstract: Multi-layered networks have recently emerged as a new network model, which naturally ﬁnds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative ﬁltering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to ﬁnd the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efﬁciency of the proposed methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M58S,1318,2d,hd,FALSE,https://i.ytimg.com/vi/_T9U0Dm_FzQ/maxresdefault.jpg,,59,1,0,0,0
227,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0DlSDaczRAc,2016-10-10T16:01:45Z,10/10/16 16:01,Predicting Matchups and Preferences in Context,"Author: Shuo Chen, Department of Computer Science, Cornell University Abstract: We present a general probabilistic framework for predicting the outcome of pairwise matchups (e.g. two-player sport matches) and pairwise preferences (e.g. product preferences), both of which have widespread applications ranging from matchmaking in computer games to recommendation in e-commerce. Unlike existing models for these tasks, our model not only learns representations of the items in a more expressive latent vector space, but also models how context modiﬁes matchup and preference outcomes. For example, the context “weather” may alter the winning probability in a tennis match, or the fact that the user is on a mobile device may alter his preferences among restaurants. More generally, the model is capable of handling any symmetric game/comparison problem that can be described by vectorized player/item and game/context features. We provide a comprehensive evaluation of its predictive performance with real datasets from both domains to show its ability to predict preference and game outcomes more accurately than existing models. Furthermore, we demonstrate on synthetic datasets the expressiveness of the model when compared against theoretical limits. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M49S,1129,2d,hd,FALSE,https://i.ytimg.com/vi/0DlSDaczRAc/maxresdefault.jpg,,47,2,0,0,0
228,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VsftFgfN6to,2016-10-10T16:00:57Z,10/10/16 16:00,Ranking Causal Anomalies via Temporal and Dynamical Analysis on Vanishing Correlations,"Author: Wei Cheng, Department of Computer Science, University of North Carolina at Chapel Hill Abstract: Modern world has witnessed a dramatic increase in our ability to collect, transmit and distribute real-time monitoring and surveillance data from large-scale information systems and cyber-physical systems. Detecting system anomalies thus attracts signiﬁcant amount of interest in many ﬁelds such as security, fault management, and industrial optimization. Recently, invariant network has shown to be a powerful way in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, signiﬁcant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: 1) fault propagation in the network is ignored; 2) the root casual anomalies may not always be the nodes with a high-percentage of vanishing correlations; 3) temporal patterns of vanishing correlations are not exploited for robust detection. To address these limitations, in this paper we propose a network diﬀusion based framework to identify signiﬁcant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network, and can perform joint inference on both the structural, and the time-evolving broken invariance patterns. As a result, it can locate high-conﬁdence anomalies that are truly responsible for the vanishing correlations, and can compensate for unstructured measurement noise in the system. Extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets demonstrate the eﬀectiveness of our approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M22S,1642,2d,hd,FALSE,https://i.ytimg.com/vi/VsftFgfN6to/maxresdefault.jpg,,127,1,2,0,0
229,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nLUfAJqXFUI,2016-10-10T15:59:46Z,10/10/16 15:59,Towards Conversational Recommender Systems,"Author: Konstantina Christakopoulou, Department of Computer Science and Engineering, University of Minnesota Abstract: People often ask others for restaurant recommendations as a way to discover new dining experiences. This makes restaurant recommendation an exciting scenario for recommender systems and has led to substantial research in this area. However, most such systems behave very differently from a human when asked for a recommendation. The goal of this paper is to begin to reduce this gap. In particular, humans can quickly establish preferences when asked to make a recommendation for someone they do not know. We address this cold-start recommendation problem in an online learning setting. We develop a preference elicitation framework to identify which questions to ask a new user to quickly learn their preferences. Taking advantage of latent structure in the recommendation space using a probabilistic latent factor model, our experiments with both synthetic and real world data compare different types of feedback and question selection strategies. We ﬁnd that our framework can make very effective use of online user feedback, improving personalized recommendations over a static model by 25% after asking only 2 questions. Our results demonstrate dramatic beneﬁts of starting from ofﬂine embeddings, and highlight the beneﬁt of bandit-based explore-exploit strategies in this setting. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M28S,1408,2d,hd,FALSE,https://i.ytimg.com/vi/nLUfAJqXFUI/maxresdefault.jpg,,895,13,0,0,1
230,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,T5CMeY8E54E,2016-10-10T15:59:02Z,10/10/16 15:59,Graph Wavelets via Sparse Cuts,"Author: Arlei Lopes da Silva, Department of Computer Science, University of California, Santa Barbara Abstract: Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Processing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner. However, how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open problem. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an eﬃcient heuristic. We formulate the basis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regularized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can eﬀectively encode both the graph structure and signal, producing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene net-works) and signiﬁcantly outperforming the best baseline. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M45S,1365,2d,hd,FALSE,https://i.ytimg.com/vi/T5CMeY8E54E/maxresdefault.jpg,,320,2,0,0,0
231,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,X-tP4gkPn58,2016-10-10T15:58:23Z,10/10/16 15:58,TRIEST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size,"Author: Lorenzo De Stefani, Computer Science Department, Brown University Abstract: We present TRIEST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities. Our experimental results on very large graphs demonstrate that TRIEST outperforms state-of-the-art approaches in accuracy and exhibits a small update time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M1S,1441,2d,hd,FALSE,https://i.ytimg.com/vi/X-tP4gkPn58/maxresdefault.jpg,,141,5,0,0,0
232,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tP_VIPsrJoA,2016-10-10T15:57:29Z,10/10/16 15:57,A Subsequence Interleaving Model for Sequential Pattern Mining,"Author: Jaroslav Fowkes, School of Informatics, University of Edinburgh Abstract: Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M55S,1615,2d,hd,FALSE,https://i.ytimg.com/vi/tP_VIPsrJoA/maxresdefault.jpg,,507,2,1,0,3
233,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,1_QH5BEP5BM,2016-10-10T15:56:43Z,10/10/16 15:56,node2vec: Scalable Feature Learning for Networks,"Author: Aditya Grover, Department of Computer Science, Stanford University Abstract: Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node’s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M45S,1545,2d,hd,FALSE,https://i.ytimg.com/vi/1_QH5BEP5BM/maxresdefault.jpg,,5568,27,32,0,7
234,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HVkAwQ4KqL4,2016-10-10T15:55:31Z,10/10/16 15:55,Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification,"Author: Lei Han, Department of Statistics, Rutgers, The State University of New Jersey Abstract: Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates (features) can significantly improve the interpretability for biological data. One major challenge is the exponentially expanded feature space when adding high-order feature interactions to the model. To tackle the huge dimensionality, Hierarchical Sparse Models (HSM) are developed by enforcing sparsity under heredity structures in the interactions among the covariates. However, existing methods only consider pairwise interactions, making the discovery of important high-order interactions a non-trivial open problem. In this paper, we propose a Generalized Hierarchical Sparse Model (GHSM) as a generalization of the HSM models to learn arbitrary-order inter-actions. The GHSM applies the l1 penalty to all the model coefficients under a constraint that given any covariate, if none of its associated kth-order interactions contribute to the regression model, then neither do its associated higher-order interactions. The resulting objective function is non-convex with a challenge lying in the coupled variables appearing in the arbitrary-order hierarchical constraints and we devise an efficient optimization algorithm to directly solve it. Specifically, we decouple the variables in the constraints via both the GIST and ADMM methods into three subproblems, each of which is proved to admit an efficiently analytical solution. We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the flu virus data, where we expand the feature space up to the 5th-order interactions. Empirical results demonstrate the effectiveness and efficiency of the proposed method and the learned high-order interactions have meaningful synergistic covariate patterns in the virus antigenicity. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M56S,1316,2d,hd,FALSE,https://i.ytimg.com/vi/HVkAwQ4KqL4/maxresdefault.jpg,,33,0,0,0,0
235,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hz6kb0tte70,2016-10-10T12:38:52Z,10/10/16 12:38,Mining Subgroups with Exceptional Transition Behavior,"Author: Florian Lemmerich, GESIS - Leibniz Institute for the Social Sciences Abstract: We present a new method for detecting interpretable subgroups with exceptional transition behavior in sequential data. Identifying such patterns has many potential applications, e.g., for studying human mobility or analyzing the behavior of internet users. To tackle this task, we employ exceptional model mining, which is a general approach for identifying interpretable data subsets that exhibit unusual interactions between a set of target attributes with respect to a certain model class. Although exceptional model mining provides a well-suited framework for our problem, previously investigated model classes cannot capture transition behavior. To that end, we introduce first-order Markov chains as a novel model class for exceptional model mining and present a new interestingness measure that quantifies the exceptionality of transition subgroups. The measure compares the distance between the Markov transition matrix of a subgroup and the respective matrix of the entire data with the distance of random dataset samples. In addition, our method can be adapted to find subgroups that match or contradict given transition hypotheses. We demonstrate that our method is consistently able to recover subgroups with exceptional transition models from synthetic data and illustrate its potential in two application examples. Our work is relevant for researchers and practitioners interested in detecting exceptional transition behavior in sequential data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M33S,1413,2d,hd,FALSE,https://i.ytimg.com/vi/hz6kb0tte70/maxresdefault.jpg,,174,1,0,0,1
236,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0PyiHN83IC8,2016-10-10T12:38:35Z,10/10/16 12:38,Smart Reply: Automated Response Suggestion for Email,"Author: Anjuli Kannan, Google Research New York, Google, Inc. Abstract: In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response diversity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M14S,1454,2d,hd,FALSE,https://i.ytimg.com/vi/0PyiHN83IC8/maxresdefault.jpg,,1372,13,1,0,2
237,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tyuynls3cIE,2016-10-10T12:38:25Z,10/10/16 12:38,CatchTartan: Representing and Summarizing Dynamic Multicontextual Behaviors,"Author: Meng Jiang, Department of Computer Science, University of Illinois at Urbana-Champaign Abstract: Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services. Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values (including timestamps) in each dimension. However, the human behaviors are multicontextual and dynamic: (1) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension; (2) many behavior collections, such as tweets or papers, evolve over time. In this paper, we represent the behavioral data as a two-level matrix (temporal-behaviors by dimensional-values) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice. We further develop a propagation method CATCHTAR-TAN to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way: it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. CATCHTARTAN outperforms the baselines on both the accuracy and speed. We apply CATCHTARTAN to four Twitter datasets up to 10 million tweets and the DBLP data, providing comprehensive summaries for the events, human life and scientific development. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M58S,1378,2d,hd,FALSE,https://i.ytimg.com/vi/tyuynls3cIE/maxresdefault.jpg,,297,1,0,0,0
238,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Qrz-tdcnzW8,2016-10-10T12:38:14Z,10/10/16 12:38,Extreme Multi-label Loss Functions,"Author: Himanshu Jain, Indian Institute of Technology Delhi Abstract: The choice of the loss function is critical in extreme multi- label learning where the objective is to annotate each data point with the most relevant subset of labels from an ex- tremely large label set. Unfortunately, existing loss func- tions, such as the Hamming loss, are unsuitable for learning, model selection, hyperparameter tuning and performance evaluation. This paper addresses the issue by developing propensity scored losses which: (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant but instead provide unbiased estimates of the true loss func- tion even when ground truth labels go missing under arbi- trary probabilistic label noise models; and (c) promote the accurate prediction of infrequently occurring, hard to pre- dict, but rewarding tail labels. Another contribution is the development of the PfastreXML algorithm (code available from [1]) which efficiently scales to large datasets with up to 9 million labels, 70 million points and 2 million dimensions and which gives significant improvements over the state-of- the-art. This paper’s results also apply to tagging, recommenda- tion and ranking which are the motivating applications for extreme multi-label learning. They generalize previous at- tempts at deriving unbiased losses under the restrictive as- sumption that labels go missing uniformly at random from the ground truth. Furthermore, they provide a sound the- oretical justification for popular label weighting heuristics used to recommend rare items. Finally, they demonstrate that the proposed contributions align with real world ap- plications by achieving superior clickthrough rates on spon- sored search advertising in Bing. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M13S,1573,2d,hd,FALSE,https://i.ytimg.com/vi/Qrz-tdcnzW8/maxresdefault.jpg,,661,5,0,0,1
239,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,N7U_DrX0l5Q,2016-10-10T12:38:03Z,10/10/16 12:38,Privacy-preserving Class Ratio Estimation,"Author: Arun Iyer, Indian Institute of Technology Bombay Abstract: In this paper we present learning models for the class ratio estimation problem, which takes as input an unlabeled set of instances and predicts the proportions of instances in the set belonging to the diﬀerent classes. This problem has applications in social and commercial data analysis. Existing models for class-ratio estimation however require instance-level supervision. Whereas in domains like politics, and demography, set-level supervision is more common. We present a new method for directly estimating class-ratios using set-level supervision. Another serious limitation in applying these techniques to sensitive domains like health is data privacy. We propose a novel label privacy-preserving mechanism that is well-suited for supervised class ratio estimation and has guarantees for achieving eﬃcient diﬀerential privacy, provided the per-class counts are large enough. We derive learning bounds for the estimation with and with-out privacy constraints, which lead to important insights for the data-publisher. Extensive empirical evaluation shows that our model is more accurate than existing methods and that the proposed privacy mechanism and learning model are well-suited for each other. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M27S,1347,2d,hd,FALSE,https://i.ytimg.com/vi/N7U_DrX0l5Q/maxresdefault.jpg,,139,0,0,0,0
240,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,-Um-397ZdOs,2016-10-10T12:37:52Z,10/10/16 12:37,When Social Influence Meets Item Inference,"Author: Hui-Ju Hung, Pennsylvania State University Abstract: Research issues and data mining techniques for product recommendation and viral marketing have been widely studied. Existing works on seed selection in social networks do not take into account the eﬀect of product recommendations in e-commerce stores. In this paper, we investigate the seed selection problem for viral marketing that considers both eﬀects of social inﬂuence and item inference (for product recommendation). We develop a new model, Social Item Graph (SIG), that captures both eﬀects in the form of hyperedges. Accordingly, we formulate a seed selection problem, called Social Item Maximization Problem (SIMP), and prove the hardness of SIMP. We design an eﬃcient algorithm with performance guarantee, called Hyperedge-Aware Greedy (HAG), for SIMP and develop a new index structure, called SIG-index, to accelerate the computation of diﬀusion process in HAG. Moreover, to construct realistic SIG models for SIMP, we develop a statistical inference based framework to learn the weights of hyperedges from data. Finally, we perform a comprehensive evaluation on our proposals with various baselines. Experimental result validates our ideas and demonstrates the eﬀectiveness and eﬃciency of the pro-posed model and algorithms over baselines. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M31S,1351,2d,hd,FALSE,https://i.ytimg.com/vi/-Um-397ZdOs/maxresdefault.jpg,,92,0,0,0,0
241,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,6bSSTXBsSuA,2016-10-10T12:37:22Z,10/10/16 12:37,Temporal Order-based First-Take-All Hashing,"Author: Hao Hu, Department of Electrical Engineering and Computer Science, University of Central Florida Abstract: Attention Deﬁcit Hyperactive Disorder (ADHD) is one of the most common childhood disorders and can continue through adolescence and adulthood. Although the root cause of the problem still remains unknown, recent advancements in brain imaging technology reveal there exists diﬀerences between neural activities of Typically Developing Children (TDC) and ADHD subjects. Inspired by this, we propose a novel First-Take-All (FTA) hashing framework to investigate the problem of fast ADHD subjects detection through the fMRI time-series of neuron activities. By hashing time courses from regions of interests (ROIs) in the brain into ﬁxed-size hash codes, FTA can compactly encode the temporal order diﬀerences between the neural activity patterns that are key to distinguish TDC and ADHD subjects. Such patterns can be directly learned via minimizing the training loss incurred by the generated FTA codes. By conducting similarity search on the resultant FTA codes, data-driven ADHD detection can be achieved in an eﬃcient fashion. The experiments’ results on real-world ADHD detection bench-marks demonstrate the FTA can outperform the state-of-the-art baselines using only neural activity time series with-out any phenotypic information. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M42S,1362,2d,hd,FALSE,https://i.ytimg.com/vi/6bSSTXBsSuA/maxresdefault.jpg,,37,0,0,0,0
242,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,a7pdiTyU4mU,2016-10-10T12:37:04Z,10/10/16 12:37,FRAUDAR: Bounding Graph Fraud in the Face of Camouflage,"Author: Bryan Hooi, Machine Learning Department, Carnegie Mellon University Abstract: Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look “normal”. Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage-resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M41S,1301,2d,hd,FALSE,https://i.ytimg.com/vi/a7pdiTyU4mU/maxresdefault.jpg,,537,6,0,0,0
243,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,lme-B6QOWZQ,2016-10-10T12:36:53Z,10/10/16 12:36,Joint Community and Structural Hole Spanner Detection via Harmonic Modularity,"Author: Lifang He, Shenzhen University Abstract: Detecting communities (or modular structures) and structural hole spanners, the nodes bridging diﬀerent communities in a network, are two essential tasks in the realm of network analytics. Due to the topological nature of com-munities and structural hole spanners, these two tasks are naturally tangled with each other, while there has been little synergy between them. In this paper, we propose a novel harmonic modularity method to tackle both tasks simultaneously. Speciﬁcally, we apply a harmonic function to mea-sure the smoothness of community structure and to obtain the community indicator. We then investigate the sparsity level of the interactions between communities, with particular emphasis on the nodes connecting to multiple communities, to discriminate the indicator of SH spanners and assist the community guidance. Extensive experiments on real-world networks demonstrate that our proposed method outperforms several state-of-the-art methods in the community detection task and also in the SH spanner identiﬁcation task (even the methods that require the supervised community information). Furthermore, by removing the SH spanners spotted by our method, we show that the quality of other community detection methods can be further improved. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M14S,1094,2d,hd,FALSE,https://i.ytimg.com/vi/lme-B6QOWZQ/maxresdefault.jpg,,197,1,0,0,1
244,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZnrsH3vtXew,2016-10-10T12:36:42Z,10/10/16 12:36,Robust Influence Maximization,"Author: Xinran He, Computer Science Department, University of Southern California Abstract: Uncertainty about models and data is ubiquitous in the computational social sciences, and it creates a need for robust social network algorithms, which can simultaneously provide guarantees across a spectrum of models and parameter set-tings. We begin an investigation into this broad domain by studying robust algorithms for the Inﬂuence Maximization problem, in which the goal is to identify a set of k nodes in a social network whose joint inﬂuence on the network is maximized. We deﬁne a Robust Inﬂuence Maximization framework wherein an algorithm is presented with a set of inﬂuence functions, typically derived from diﬀerent inﬂuence models or diﬀerent parameter settings for the same model. The diﬀerent parameter settings could be derived from observed cascades on diﬀerent topics, under diﬀerent conditions, or at diﬀerent times. The algorithm’s goal is to identify a set of k nodes who are simultaneously inﬂuential for all inﬂuence functions, compared to the (function-speciﬁc) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust inﬂuence to within a factor of 1 − 1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world data sets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world data sets; all algorithms perform fairly well. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M59S,1259,2d,hd,FALSE,https://i.ytimg.com/vi/ZnrsH3vtXew/maxresdefault.jpg,,394,0,0,0,0
245,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5Kq0MrNJi-g,2016-10-10T10:25:46Z,10/10/16 10:25,Evaluating Mobile App Release,"Author: Ya Xu, LinkedIn Corporation Abstract: We have seen an explosive growth of mobile usage, particularly on mobile apps. It is more important than ever to be able to properly evaluate mobile app release. A/B testing is a standard framework to evaluate new ideas. We have seen much of its applications in the online world across the industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different, and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process. Mobile infrastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps, which will be discussed in details in this paper. In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi-experimental framework [21]. Not all features can be A/B tested due to infrastructure changes and wholistic product redesign. We propose and establish quasi-experimental techniques for measuring impact from mobile app release, with results shared from a recent major app launch at LinkedIn. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M6S,1626,2d,hd,FALSE,https://i.ytimg.com/vi/5Kq0MrNJi-g/maxresdefault.jpg,,142,1,0,0,0
246,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,7lmPMiBIw50,2016-10-10T10:25:46Z,10/10/16 10:25,EMBERS at 4 years: Experiences operating an Open Source Indicators Forecasting System,"Author: Sathappan Muthiah, Virginia Polytechnic Institute and State University Abstract: EMBERS is an anticipatory intelligence system forecasting population-level events in multiple countries of Latin America. A deployed system from 2012, EMBERS has been generating alerts 24x7 by ingesting a broad range of data sources including news, blogs, tweets, machine coded events, currency rates, and food prices. In this paper, we describe our experiences operating EMBERS continuously for nearly 4 years, with speciﬁc attention to the discoveries it has enabled, correct as well as missed forecasts, lessons learnt from participating in a forecasting tournament, and our perspectives on the limits of forecasting including ethical considerations. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT30M56S,1856,2d,hd,FALSE,https://i.ytimg.com/vi/7lmPMiBIw50/maxresdefault.jpg,,78,0,0,0,0
247,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BdK13fJ-sA4,2016-10-10T10:25:46Z,10/10/16 10:25,Aircraft Trajectory Prediction made easy with Predictive Analytics,"Author: Samet Ayhan, Department of Computer Science, University of Maryland Abstract: At the heart of Air Traﬃc Management (ATM) lies the Decision Support Systems (DST) that rely upon accurate trajectory prediction to determine how the airspace will look like in the future to make better decisions and advisories. Dealing with airspace that is prone to congestion due to environmental factors still remains the challenge especially when a deterministic approach is used in the trajectory pre-diction process. In this paper, we describe a novel stochastic trajectory prediction approach for ATM that can be used for more eﬃcient and realistic ﬂight planning and to assist airspace ﬂow management, potentially resulting in higher safety, capacity, and eﬃciency commensurate with fuel savings thereby reducing emissions for a better environment. Our approach considers airspace as a 3D grid network, where each grid point is a location of a weather observation. We hypothetically build cubes around these grid points, so the entire airspace can be considered as a set of cubes. Each cube is deﬁned by its centroid, the original grid point, and associated weather parameters that remain homogeneous within the cube during a period of time. Then, we align raw trajectories to a set of cube centroids which are basically ﬁxed 3D positions independent of trajectory data. This creates a new form of trajectories which are 4D joint cubes, where each cube is a segment that is associated with not only spatio-temporal attributes but also with weather parameters. Next, we exploit machine learning techniques to train inference models from historical data and apply a stochastic model, a Hidden Markov Model (HMM), to predict trajectories taking environmental uncertainties into ac-count. During the process, we apply time series clustering to generate input observations from an excessive set of weather parameters to feed into the Viterbi algorithm. Our experiments use a real trajectory dataset with pertaining weather observations and demonstrate the eﬀectiveness of our approach to the trajectory prediction process for ATM. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT29M11S,1751,2d,hd,FALSE,https://i.ytimg.com/vi/BdK13fJ-sA4/maxresdefault.jpg,,497,3,0,0,0
248,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,GizqWmtymT0,2016-10-10T10:25:46Z,10/10/16 10:25,GLMix: Generalized Linear Mixed Models For Large Scale Response Prediction,"Author: Xianxing Zhang, LinkedIn Corporation Abstract: Generalized linear model (GLM) is a widely used class of models for statistical inference and response prediction problems. For instance, in order to recommend relevant content to a user or optimize for revenue, many web companies use logistic regression models to predict the probability of the user’s clicking on an item (e.g., ad, news article, job). In scenarios where the data is abundant, having a more ﬁne-grained model at the user or item level would potentially lead to more accurate prediction, as the user’s personal preferences on items and the item’s speciﬁc attraction for users can be better captured. One common approach is to introduce ID-level regression coeﬃcients in addition to the global regression coeﬃcients in a GLM setting, and such models are called generalized linear mixed models (GLMix) in the statistical literature. However, for big data sets with a large number of ID-level coeﬃcients, ﬁtting a GLMix model can be computationally challenging. In this paper, we re-port how we successfully overcame the scalability bottleneck by applying parallelized block coordinate descent under the Bulk Synchronous Parallel (BSP) paradigm. We deployed the model in the LinkedIn job recommender system, and generated 20% to 40% more job applications for job seekers on LinkedIn. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M18S,1638,2d,hd,FALSE,https://i.ytimg.com/vi/GizqWmtymT0/maxresdefault.jpg,,1234,3,1,0,0
249,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,GmB_PdbOTAQ,2016-10-10T10:25:46Z,10/10/16 10:25,Identifying Decision Makers from Professional Social Networks,"Author: Shipeng Yu, LinkedIn Corporation Abstract: Sales professionals help organizations win clients for products and services. Generating new clients starts with identifying the right decision makers at the target organization. For the past decade, online professional networks have collected tremendous amount of data on people’s identity, their network and behavior data of buyers and sellers building relationships with each other for a variety of use-cases. Sales professionals are increasingly relying on these networks to research, identify and reach out to potential prospects, but it is often hard to find the right people effectively and efficiently. In this paper we present LDMS, the LinkedIn Decision Maker Score, to quantify the ability of making a sales decision for each of the 400M+ LinkedIn members. It is the key data-driven technology underlying Sales Navigator, a proprietary LinkedIn product that is designed for sales professionals. We will specifically discuss the modeling challenges of LDMS, and present two graph-based approaches to tackle this problem by leveraging the professional network data at LinkedIn. Both approaches are able to leverage both the graph information and the contextual information on the vertices, deal with small amount of labels on the graph, and handle heterogeneous graphs among different types of vertices. We will show some offline evaluations of LDMS on historical data, and also discuss its online usage in multiple applications in live production systems as well as future use cases within the LinkedIn ecosystem. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M42S,1422,2d,hd,FALSE,https://i.ytimg.com/vi/GmB_PdbOTAQ/maxresdefault.jpg,,150,0,0,0,0
250,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HT5HWe2PQgE,2016-10-10T10:25:46Z,10/10/16 10:25,Question Independent Grading using Machine Learning: The Case of Computer Program Grading,"Author: Gursimran Singh, Aspiring Minds Assessment Pvt. Ltd. Abstract: Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation speciﬁcally, the focus of this work, this issue is ampliﬁed. The models have to be trained not only for every question but also for every language the question is oﬀered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question speciﬁc models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question speciﬁc models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system’s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M12S,1512,2d,hd,FALSE,https://i.ytimg.com/vi/HT5HWe2PQgE/maxresdefault.jpg,,79,1,0,0,0
251,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HY_0BfYzlQs,2016-10-10T10:25:46Z,10/10/16 10:25,Boosted Decision Tree Regression Adjustment for Variance Reduction of Online Controlled Experiments,"Author: Alexey Drutsa, Yandex Abstract: Nowadays, the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates achieving more than a thousand concurrent experiments per day. Despite the in-creasing need for running more experiments, these services are limited in their user traﬃc. This situation leads to the problem of ﬁnding a new or improving existing key performance metric with a higher sensitivity and lower variance. We focus on the problem of variance reduction for engagement metrics of user loyalty that are widely used in A/B testing of web services. We develop a general framework that is based on evaluation of the mean diﬀerence between the actual and the approximated values of the key performance metric (instead of the mean of this metric). On the one hand, it allows us to incorporate the state-of-the-art techniques widely used in randomized experiments of clinical and social research, but limitedly used in online evaluation. On the other hand, we propose a new class of methods based on advanced machine learning algorithms, including ensembles of decision trees, that, to the best of our knowledge, have not been applied earlier to the problem of variance reduction. We validate the variance reduction approaches on a very large set of real large-scale A/B experiments run at Yandex for diﬀerent engagement metrics of user loyalty. Our best approach demonstrates 63% average variance reduction (which is equivalent to 63% saved user traﬃc) and detects the treatment eﬀect in 2 times more A/B experiments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M49S,1249,2d,hd,FALSE,https://i.ytimg.com/vi/HY_0BfYzlQs/maxresdefault.jpg,,301,0,0,0,0
252,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Hh4AX5VMO6s,2016-10-10T10:25:46Z,10/10/16 10:25,Engagement Capacity and Engaging Team Formation,"Author: Shounak Gore, Department of Computer Science and Engineering, University at Buffalo Abstract: The challenges of assessing the “health” of online social media platforms and strategically growing them are recognized by many practitioners and researchers. For those platforms that primarily rely on user-generated content, the reach -the degree of participation referring to the percentage and involvement of users - is a key indicator of success. This paper lays a theoretical foundation for measuring engagement as a driver of reach that achieves growth via positive externality effects. The paper takes a game theoretic approach to quantifying engagement, viewing a platform’s social capital as a cooperatively created value and finding a fair distribution of this value among the contributors. It introduces engagement capacity, a measure of the ability of users and user groups to engage peers, and formulates the Engaging Team Formation Problem (EngTFP) to identify the sets of users that \make a platform go”. We show how engagement capacity can be useful in characterizing forum user behavior and in the reach maximization efforts. We also stress how engagement analysis differs from influence measurement. Computational investigations with Twitter and Health Forum data reveal the properties of engagement capacity and the utility of EngTFP. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M40S,1360,2d,hd,FALSE,https://i.ytimg.com/vi/Hh4AX5VMO6s/maxresdefault.jpg,,48,1,0,0,0
253,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,JT5YVcKlp5U,2016-10-10T10:25:46Z,10/10/16 10:25,Collaborative Knowledge Base Embedding for Recommender Systems,"Author: Fuzheng Zhang, Microsoft Research Asia, Microsoft Research Abstract: Among different recommendation techniques, collaborative filtering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary information is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstructured data with different semantics, which can be consumed by various applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowledge base, we design three components to extract items’ semantic representations from structural content, textual content and visual content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items’ structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items’ textual representations and visual representations, respectively. Finally, we propose our final integrated framework, which is termed as Collaborative Knowledge Base Embedding (CKE), to jointly learn the latent representations in collaborative filtering as well as items’ semantic representations from the knowledge base. To evaluate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two realworld datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art recommendation methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M55S,1255,2d,hd,FALSE,https://i.ytimg.com/vi/JT5YVcKlp5U/maxresdefault.jpg,,644,3,1,0,0
254,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MDyqvSCP4bU,2016-10-10T10:25:46Z,10/10/16 10:25,Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks,"Author: Jung-Woo Ha, NAVER LABS Abstract: Precise item categorization is a key issue in e-commerce domains. However, it still remains a challenging problem due to data size, category skewness, and noisy metadata. Here, we demonstrate a successful report on a deep learning-based item categorization method, i.e., deep categorization network (DeepCN), in an e-commerce website. DeepCN is an end-to-end model using multiple recurrent neural networks (RNNs) dedicated to metadata attributes for generating features from text metadata and fully connected layers for classifying item categories from the generated features. The categorization errors are propagated back through the fully connected layers to the RNNs for weight update in the learning process. This deep learning-based approach allows diverse attributes to be integrated into a common representation, thus overcoming sparsity and scalability problems. We evaluate DeepCN on large-scale real-world data including more than 94 million items with approximately 4,100 leaf categories from a Korean e-commerce website. Experiment results show our method improves the categorization accuracy compared to the model using single RNN as well as a standard classification model using unigram-based bag-of-words. Furthermore, we investigate how much the model parameters and the used attributes influence categorization performances. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT21M18S,1278,2d,hd,FALSE,https://i.ytimg.com/vi/MDyqvSCP4bU/maxresdefault.jpg,,347,2,0,0,0
255,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Q9XsvzSXXIg,2016-10-10T10:25:46Z,10/10/16 10:25,Anomaly Detection Using Program Control Flow Graph Mining from Execution Logs,"Author: Animesh Nandi, IBM India Research Lab Abstract: We focus on the problem of detecting anomalous run-time behavior of distributed applications from their execution logs. Specifically we mine templates and template sequences from logs to form a control flow graph (cfg) spanning distributed components. This cfg represents the baseline healthy system state and is used to flag deviations from the expected behavior of runtime logs. The novelty in our work stems from the new techniques employed to: (1) overcome the instrumentation requirements or application specific assumptions made in prior log mining approaches, (2) improve the accuracy of mined templates and the cfg in the presence of long parameters and high amount of interleaving respectively, and (3) improve by orders of magnitude the scalability of the cfg mining process in terms of volume of log data that can be processed per day. We evaluate our template and cfg mining approaches using (a) synthetic log traces and (b) multiple real-world log datasets collected at different layers of application stack. Results demonstrate that the template mining, cfg mining, and our anomaly detection algorithms have high accuracy. The distributed implementation of our pipeline is highly scalable and has more than 500 GB/day of log data processing capability even on a 10 low-end VM based (Spark + Hadoop) cluster. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M7S,1627,2d,hd,FALSE,https://i.ytimg.com/vi/Q9XsvzSXXIg/maxresdefault.jpg,,620,4,0,0,0
256,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,RzIxo8-Ofzg,2016-10-10T10:25:46Z,10/10/16 10:25,Dynamic and Robust Wildfire Risk Prediction System: An Unsupervised Approach,"Author: Laura Rusu, IBM Research Abstract: Ability to predict the risk of damaging events (e.g. wildﬁres) is crucial in helping emergency services in their decision-making processes, to mitigate and reduce the impact of such events. Today, wildﬁre rating systems have been in operation extensively in many countries around the world to estimate the danger of wildﬁres. In this paper we propose a data-driven approach to predict wildﬁre risk using weather data. We show how we address the inherent challenge arising due to the temporal dynamicity of weather data. Weather observations naturally change in time, with ﬁner-scale variation (e.g. stationary day or night) or large variations (non-stationary day or night), and this determines a temporal variation of the predicted wildﬁre danger. We show how our dynamic wildﬁre danger prediction model addresses the aforementioned challenge using context-based anomaly detection techniques. We call our predictive model a Context-Based Fire Risk (CBFR) model. The advantage of our model is that it maintains multiple historical models for diﬀerent temporal variations (e.g. day versus night), and uses ensemble learning techniques to predict wildﬁre risk with high accuracy. In addition, it is completely un-supervised and does not rely on expert knowledge, which makes it ﬂexible and easily applied to any region of interest. Our CBFR model is also scalable and can potentially be parallelised to speed up computation. We have considered multiple wildﬁre locations in the Blue Mountains, Australia as a case study, and compared the results of our system with the existing well-established Australian wildﬁre rating system. The experimental results show that our predictive model has a substantially higher accuracy in predicting wild-ﬁre risk, which makes it an eﬀective model to supplement the operational Australian wildﬁre rating system. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M17S,1457,2d,hd,FALSE,https://i.ytimg.com/vi/RzIxo8-Ofzg/maxresdefault.jpg,,141,2,0,0,0
257,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UmP3UePGO7E,2016-10-10T10:25:46Z,10/10/16 10:25,An Engagement-Based Customer Lifetime Value System for E-commerce,"Author: Ali Vanderveld, Groupon, Inc. Abstract: A comprehensive understanding of individual customer value is crucial to any successful customer relationship management strategy. It is also the key to building products for long-term value returns. Modeling customer lifetime value (CLTV) can be fraught with technical difficulties, however, due to both the noisy nature of user-level behavior and the potentially large customer base. Here we describe a new CLTV system that solves these problems. This was built at Groupon, a large global e-commerce company, where confronting the unique challenges of local commerce means quickly iterating on new products and the optimal inventory to appeal to a wide and diverse audience. Given current purchaser frequency we need a faster way to determine the health of individual customers, and given finite resources we need to know where to focus our energy. Our CLTV system predicts future value on an individual user basis with a random forest model which includes features that account for nearly all aspects of each customer’s relationship with our platform. This feature set includes those quantifying engagement via email and our mobile app, which give us the ability to predict changes in value far more quickly than models based solely on purchase behavior. We further model different customer types, such as one-time buyers and power users, separately so as to allow for different feature weights and to enhance the interpretability of our results. Additionally, we developed an economical scoring framework wherein we re-score a user when any trigger events occur and apply a decay function otherwise, to enable frequent scoring of a large customer base with a complex model. This system is deployed, predicting the value of hundreds of millions of users on a daily cadence, and is actively being used across our products and business initiatives. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT27M2S,1622,2d,hd,FALSE,https://i.ytimg.com/vi/UmP3UePGO7E/maxresdefault.jpg,,1829,17,4,0,2
258,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VF6mz6FmtTQ,2016-10-10T10:25:46Z,10/10/16 10:25,Ranking Relevance in Yahoo Search,"Author: Dawei Yin, Yahoo! Inc. Abstract: Search engines play a crucial role in our daily lives. Relevance is the core problem of a commercial search engine. It has attracted thousands of researchers from both academia and industry and has been studied for decades. Relevance in a modern search engine has gone far beyond text matching, and now involves tremendous challenges. The semantic gap between queries and URLs is the main barrier for improving base relevance. Clicks help provide hints to improve relevance, but unfortunately for most tail queries, the click information is too sparse, noisy, or missing entirely. For comprehensive relevance, the recency and location sensitivity of results is also critical. In this paper, we give an overview of the solutions for relevance in the Yahoo search engine. We introduce three key techniques for base relevance – ranking functions, semantic matching features and query rewriting. We also describe solutions for recency sensitive relevance and location sensitive relevance. This work builds upon 20 years of existing efforts on Yahoo search, summarizes the most recent advances and provides a series of practical relevance solutions. The reported performance is based on Yahoo’s commercial search engine, where tens of billions of URLs are indexed and served by the ranking system. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M47S,1427,2d,hd,FALSE,https://i.ytimg.com/vi/VF6mz6FmtTQ/maxresdefault.jpg,,1390,13,1,0,2
259,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VXuY7QqAKs4,2016-10-10T10:25:46Z,10/10/16 10:25,Contextual Intent Tracking for Personal Assistants,"Author: Yu Sun, Department of Computing and Information Systems, The University of Melbourne Abstract: A new paradigm of recommendation is emerging in intelligent personal assistants such as Apple’s Siri, Google Now, and Microsoft Cortana, which recommends “the right information at the right time” and proactively helps you “get things done”. This type of recommendation requires precisely tracking users’ contemporaneous intent, i.e., what type of information (e.g., weather, stock prices) users currently intend to know, and what tasks (e.g., playing music, getting taxis) they intend to do. Users’ intent is closely related to context, which includes both external environments such as time and location, and users’ internal activities that can be sensed by personal assistants. The relationship between context and intent exhibits complicated co-occurring and sequential correlation, and contextual signals are also heterogeneous and sparse, which makes modeling the contextintent relationship a challenging task. To solve the intent tracking problem, we propose the Kalman filter regularized PARAFAC2 (KP2) nowcasting model, which compactly represents the structure and co-movement of context and intent. The KP2 model utilizes collaborative capabilities among users, and learns for each user a personalized dynamic system that enables efficient nowcasting of users’ intent. Extensive experiments using real-world data sets from a commercial personal assistant show that the KP2 model significantly outperforms various methods, and provides inspiring implications for deploying large-scale proactive recommendation systems in personal assistants. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M42S,1422,2d,hd,FALSE,https://i.ytimg.com/vi/VXuY7QqAKs4/maxresdefault.jpg,,190,0,0,0,0
260,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XSPewlyy8Qg,2016-10-10T10:25:46Z,10/10/16 10:25,Predicting Disk Replacement towards Reliable Data Centers,"Author: Mirela Madalina Botezatu, IBM Zurich Research Lab Abstract: Disks are among the most frequently failing components in today’s IT environments. Despite a set of defense mechanisms such as RAID, the availability and reliability of the system are still often impacted severely. In this paper, we present a highly accurate SMART-based analysis pipeline that can correctly predict the necessity of a disk replacement even 10-15 days in advance. Our method has been built and evaluated on more than 30000 disks from two major manufacturers, monitored over 17 months. Our approach employs statistical techniques to automatically detect which SMART parameters correlate with disk replacement and uses them to predict the replacement of a disk with even 98% accuracy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M10S,1090,2d,hd,FALSE,https://i.ytimg.com/vi/XSPewlyy8Qg/maxresdefault.jpg,,205,2,0,0,1
261,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YBMuVe3vbiI,2016-10-10T10:25:46Z,10/10/16 10:25,Predictors without Borders: Behavioral Modeling of Product Adoption in Three Developing Countries,"Author: Muhammad Khan, University of Washington Abstract: Billions of people around the world live without access to banks or other formal ﬁnancial institutions. In the past several years, many mobile operators have launched “Mobile Money” platforms that deliver basic ﬁnancial services over the mobile phone network. While many believe that these services can improve the lives of the poor, in many countries adoption of Mobile Money still remains anemic. In this paper, we develop a predictive model of Mobile Money adoption that uses billions of mobile phone communications records to understand the behavioral determinants of adoption. We describe a novel approach to feature engineering that uses a Deterministic Finite Automaton to construct thousands of behavioral metrics of phone use from a concise set of recursive rules. These features provide the foundation for a predictive model that is tested on mobile phone operators logs from Ghana, Pakistan, and Zambia, three very diﬀerent developing-country contexts. The results highlight the key correlates of Mobile Money use in each country, as well as the potential for such methods to predict and drive adoption. More generally, our analysis provides insight into the extent to which homogenized supervised learning methods can generalize across geographic contexts. We ﬁnd that without careful tuning, a model that performs very well in one country frequently does not generalize to another. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M22S,1342,2d,hd,FALSE,https://i.ytimg.com/vi/YBMuVe3vbiI/maxresdefault.jpg,,44,0,0,0,0
262,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,a-Aiy8XOE00,2016-10-10T10:25:46Z,10/10/16 10:25,Matrix Computations and Optimization in Apache Spark,"Authors: Reza Bosagh Zadeh, Institute for Computational and Mathematical Engineering, Stanford University Abstract: We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for distributed matrices and optimization routines using these matrices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector operations and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computational power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are already merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT22M52S,1372,2d,hd,FALSE,https://i.ytimg.com/vi/a-Aiy8XOE00/maxresdefault.jpg,,1304,9,0,0,1
263,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gvL9gOE921U,2016-10-10T10:25:46Z,10/10/16 10:25,Identifying Police Officers at Risk of Adverse Events,"Author: Joe Walsh, Center for Data Science and Public Policy, University of Chicago Abstract: Adverse events between police and the public, such as deadly shootings or instances of racial proﬁling, can cause serious or deadly harm, damage police legitimacy, and result in costly litigation. Evidence suggests these events can be prevented by targeting interventions based on an Early Intervention System (EIS) that ﬂags police oﬃcers who are at a high risk for involvement in such adverse events. Today’s EIS are not data-driven and typically rely on simple thresholds based entirely on expert intuition. In this paper, we de-scribe our work with the Charlotte-Mecklenburg Police Department (CMPD) to develop a machine learning model to predict which oﬃcers are at risk for an adverse event. Our approach signiﬁcantly outperforms CMPD’s existing EIS, increasing true positives by ∼ 12% and decreasing false positives by ∼ 32%. Our work also sheds light on features related to oﬃcer characteristics, situational factors, and neighborhood factors that are predictive of adverse events. This work provides a starting point for police departments to take a comprehensive, data-driven approach to improve policing and reduce harm to both oﬃcers and members of the public. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT13M35S,815,2d,hd,FALSE,https://i.ytimg.com/vi/gvL9gOE921U/maxresdefault.jpg,,414,2,0,0,0
264,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jD2ebSAezvk,2016-10-10T10:25:46Z,10/10/16 10:25,Catch Me If You Can: Detecting Pickpocket Suspects from Large-Scale Transit Records,"Author: Chuanren Liu, LeBow College of Business, Drexel University Abstract: Massive data collected by automated fare collection (AFC) systems provide opportunities for studying both personal traveling behaviors and collective mobility patterns in the urban area. Existing studies on the AFC data have primarily focused on identifying passengers’ movement patterns. In this paper, however, we creatively leveraged such data for identifying thieves in the public transit systems. In-deed, stopping pickpockets in the public transit systems has been critical for improving passenger satisfaction and public safety. However, it is challenging to tell thieves from regular passengers in practice. To this end, we developed a suspect detection and surveillance system, which can identify pick-pocket suspects based on their daily transit records. Specifically, we ﬁrst extracted a number of features from each passenger’s daily activities in the transit systems. Then, we took a two-step approach that exploits the strengths of unsupervised outlier detection and supervised classiﬁcation models to identify thieves, who exhibit abnormal traveling behaviors. Experimental results demonstrated the eﬀective-ness of our method. We also developed a prototype system with a user-friendly interface for the security personnel. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT23M43S,1423,2d,hd,FALSE,https://i.ytimg.com/vi/jD2ebSAezvk/maxresdefault.jpg,,289,3,0,0,0
265,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,kOqLbibOGus,2016-10-10T10:25:46Z,10/10/16 10:25,Repeat Buyer Prediction for E-Commerce,"Author: Wei Chen, DBS Bank Ltd Abstract: A large number of new buyers are often acquired by merchants during promotions. However, many of the attracted buyers are one-time deal hunters, and the promotions may have little long-lasting impact on sales. It is important for merchants to identify who can be converted to regular loyal buyers and then target them to reduce promotion cost and increase the return on investment (ROI). At International Joint Conferences on Artificial Intelligence (IJCAI) 2015, Alibaba hosted an international competition for repeat buyer prediction based on the sales data of the “Double 11” shop- ping event in 2014 at Tmall.com. We won the first place at stage 1 of the competition out of 753 teams. In this paper, we present our winning solution, which consists of comprehensive feature engineering and model training. We created pro- files for users, merchants, brands, categories, items and their interactions via extensive feature engineering. These profiles are not only useful for this particular prediction task, but can also be used for other important tasks in e-commerce, such as customer segmentation, product recommendation, and customer base augmentation for brands. Feature engineering is often the most important factor for the success of a prediction task, but not much work can be found in the literature on feature engineering for prediction tasks in e-commerce. Our work provides some useful hints and in- sights for data science practitioners in e-commerce. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M7S,1507,2d,hd,FALSE,https://i.ytimg.com/vi/kOqLbibOGus/maxresdefault.jpg,,470,2,0,0,0
266,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,lidcr7ws9P4,2016-10-10T10:25:46Z,10/10/16 10:25,Minimizing Legal Exposure for High-Tech Companies through Collaborative Filtering Methods,"Authors: Bo Jin, Dalian University of Technology Abstract: Patent litigation not only covers legal and technical issues, it is also a key consideration for managers of high-technology (high-tech) companies when making strategic decisions. Paten-t litigation inﬂuences the market value of high-tech companies. However, this raises unique challenges. To this end, in this paper, we develop a novel recommendation framework to solve the problem of litigation risk prediction. We will introduce a speciﬁc type of patent-related litigation, that is, Section 337 investigations, which prohibit all acts of unfair competition, or any unfair trade practices, when exporting products to the United States. To build this recommendation framework, we collect and exploit a large amount of published information related to almost all Section 337 investigation cases. This study has two aims: (1) to predict the litigation risk in a speciﬁc industry category for high-tech companies and (2) to predict the litigation risk from competitors for high-tech companies. These aims can be achieved by mining historical investigation cases and related patents. Speciﬁcally, we propose two methods to meet the needs of both aims: a proximal slope one predictor and a time-aware predictor. Several factors are considered in the proposed methods, including the litigation risk if a company wants to enter a new market and the risk that a potential competitor would ﬁle a lawsuit against the new entrant. Comparative experiments using real-world data demonstrate that the proposed methods outperform several base-lines with a signiﬁcant margin. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT25M33S,1533,2d,hd,FALSE,https://i.ytimg.com/vi/lidcr7ws9P4/maxresdefault.jpg,,33,0,0,0,0
267,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,mGMOzfZ75KM,2016-10-10T10:25:46Z,10/10/16 10:25,Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta,"Authors: Oliver Haimson, Department of Informatics, University of California, Irvine Wenwen Zhang, Georgia Institute of Technology Shang-Tse Chen, Georgia Institute of Technology Michael Madaio, Human-Computer Interaction Institute, Carnegie Mellon University Abstract: The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M47S,1607,2d,hd,FALSE,https://i.ytimg.com/vi/mGMOzfZ75KM/maxresdefault.jpg,,134,2,0,0,0
268,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,oXbkGVkngSE,2016-10-10T10:25:46Z,10/10/16 10:25,An Empirical Study on Recommendation with Multiple Types of Feedback,"Author: Liang Tang, LinkedIn Corporation Abstract: User feedback like clicks and ratings on recommended items provides important information for recommender systems to predict users’ interests in unseen items. Most systems rely on models trained using a single type of feedback, e.g., ratings for movie recommendation and clicks for online news recommendation. How-ever, in addition to the primary feedback, many systems also allow users to provide other types of feedback, e.g., liking or sharing an article, or hiding all articles from a source. These additional feed-back potentially provides extra information for the recommendation models. To optimize user experience and business objectives,it is important for a recommender system to use both the primary feedback and additional feedback. This paper presents an empirical study on various training methods for incorporating multiple user feedback types based on LinkedIn recommendation products. We study three important problems that we face at LinkedIn: (1) Whether to send an email based on clicks and complaints, (2) how to rank updates in LinkedIn feeds based on clicks and hides and (3) how jointly optimize for viral actions and clicks in LinkedIn feeds. Extensive ofﬂine experiments on historical data show the effectiveness of these methods in different situations. Online A/B testing results further demonstrate the impact of these methods on LinkedIn production systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT15M52S,952,2d,hd,FALSE,https://i.ytimg.com/vi/oXbkGVkngSE/maxresdefault.jpg,,32,0,0,0,0
269,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,owxDSNsFLFk,2016-10-10T10:25:46Z,10/10/16 10:25,Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features,"Author: Ying Shan, Microsoft Abstract: Manually crafted combinatorial features have been the ""secret sauce"" behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M19S,1159,2d,hd,FALSE,https://i.ytimg.com/vi/owxDSNsFLFk/maxresdefault.jpg,,232,5,4,0,0
270,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pWgDmiYrKZ8,2016-10-10T10:25:46Z,10/10/16 10:25,Audience Expansion for Online Social Network Advertising,"Author: Haishan Liu, LinkedIn Corporation Abstract: Online social network advertising platforms, such as that provided by LinkedIn, generally allow marketers to specify targeting options so that their ads appear to a desired demographic. Audience Expansion is a technique developed at LinkedIn to simplify targeting and identify new audiences with similar attributes to the original target audience. We developed two methods to achieve Audience Expansion: campaign-agnostic expansion and campaign-aware expansion. In this paper, we describe the details of these methods, present in-depth analysis of their trade-offs, and demonstrate a hybrid strategy that possesses the combined strength of both methods. Through large scale online experiments, we show the effectiveness of the proposed approach, and as a result, the benefits it brings to the whole marketplace including both LinkedIn and advertisers. The achieved benefits can be characterized as: 1) simplified targeting process and increased reach for advertisers, and 2) better utilization of LinkedIn’s ads inventory and higher and more efficient market participation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M41S,1601,2d,hd,FALSE,https://i.ytimg.com/vi/pWgDmiYrKZ8/maxresdefault.jpg,,181,2,0,0,0
271,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ubzEG1kSwxU,2016-10-10T10:25:46Z,10/10/16 10:25,Email Volume Optimization at LinkedIn,"Author: Rupesh Gupta, LinkedIn Corporation Abstract: Online social networking services distribute various types of messages geared towards providing increased value to their members. Common types of messages include news, connection requests, membership notifications, promotions, and event notifications. Such communication, if used judiciously, can provide an enormous value to the members. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member’s mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member’s interests, and potentially a poor brand perception. In this paper, we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system, and experimental results from online A/B tests. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT20M21S,1221,2d,hd,FALSE,https://i.ytimg.com/vi/ubzEG1kSwxU/maxresdefault.jpg,,194,3,0,0,0
272,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yS95Msg7BTE,2016-10-10T10:25:46Z,10/10/16 10:25,Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned,"Author: Xiaolin Shi, Snapchat, Inc. Abstract: Online controlled experiments, also called A/B testing, have been established as the mantra for data-driven decision making in many web-facing companies. In recent years, there are emerging research works focusing on building the platform and scaling it up [34], best practices and lessons learned to obtain trustworthy results [19; 20; 23; 26], and experiment design techniques and various issues related to statistical inference and testing [6; 7; 8]. However, despite playing a central role in online controlled experiments, there is little published work on treating metric development itself as a data-driven process. In this paper, we focus on the topic of how to develop meaningful and useful metrics for online services in their online experiments, and show how data-driven techniques and criteria can be applied in metric development process. In particular, we emphasize two fundamental qualities for the goal metrics (or Overall Evaluation Criteria) of any online service: directionality and sensitivity. We share lessons on why these two qualities are critical, how to measure these two qualities of metrics of interest, how to develop metrics with clear directionality and high sensitivity by using approaches based on user behavior models and data-driven calibration, and how to choose the right goal metrics for the entire online services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT31M50S,1910,2d,hd,FALSE,https://i.ytimg.com/vi/yS95Msg7BTE/maxresdefault.jpg,,226,4,0,0,0
273,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3VP7K301iVA,2016-10-10T10:12:19Z,10/10/16 10:12,Online dual decomposition for performance and delivery-based distributed ad allocation,"Author: Jim C. Huang, Amazon Abstract: Online optimization is central to display advertising, where we must sequentially allocate ad impressions to maximize the total welfare among advertisers, while respecting various advertiser-speciﬁed long-term constraints (e.g., total amount of the ad’s budget that is consumed at the end of the campaign). In this paper, we present the online dual decomposition (ODD) framework for large-scale, online, distributed ad allocation, which combines dual decomposition and on-line convex optimization. ODD allows us to account for the distributed and the online nature of the ad allocation problem and is extensible to a variety of ad allocation problems arising in real-world display advertising systems. Moreover, ODD does not require assumptions about auction dynamics, stochastic or adversarial feedback, or any other characteristics of the ad marketplace. We further provide guarantees for the online solution as measured by bounds on cumulative regret. The regret analysis accounts for the impact of having to estimate constraints in an online setting before they are observed and for the dependence on the smooth-ness with which constraints and constraint violations are generated. We provide an extensive set of results from a large-scale production advertising system at Amazon to validate the framework and compare its behavior to various ad allocation algorithms. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT18M54S,1134,2d,hd,FALSE,https://i.ytimg.com/vi/3VP7K301iVA/maxresdefault.jpg,,192,1,0,0,0
274,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Tl9XPzVROfk,2016-10-10T10:11:10Z,10/10/16 10:11,From Online Behaviors to Offline Retailing,"Author: Qing He, Chinese Academy of Sciences Abstract: To combat the ease of online shopping in pajamas, offline mall owners focus increasingly on driving satisfaction and improving retention by identifying customers’ preferences. However, most of these studies are based on customers’ offline consuming history only. Benefiting from the internet, we can also get customers’ online behaviors, such as the search logs, web browsing logs, online shopping logs, and so on. Might these seemingly irrelevant information from two different modalities (i.e. online and offline) be somehow interrelated? How can we make use of the online behaviors and offline actions jointly to promote recommendation for offline retailing? In this study, we formulate this task as a cross-modality recommendation problem, and present its solution via a proposed probabilistic graphical model, called Online-to-Offline Topic Modeling (O2OTM). Specifically, this method explicitly models the relationships between online and offline topics so that the likelihood of both online and offline behaviors is maximized. Then, the recommendation is made only based on the pairs of online and offline topics, denoted by (t, l), with high values of lift, such that the existence of the online topic t greatly increases the response on the corresponding offline topic l compared with the average response for the population without the online topic t. Furthermore, we evaluate this solution in both live and retrospect experiments. The real-world deployment of this model for the anniversary promotion campaign of a famous shopping mall in Beijing shows that our approach increases the occurred customer purchases per promotion message by 29.75% compared with the baseline. Also, our model finds some interesting interpretable relationships between the online search topics and offline brand topics. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT19M17S,1157,2d,hd,FALSE,https://i.ytimg.com/vi/Tl9XPzVROfk/maxresdefault.jpg,,153,0,0,0,0
275,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-10-10T09:30:12Z,10/10/16 9:30,Introduction to Spark 2.0 (Part 2),"Authors: Reynold Xin, Databricks Inc. Michael Armbrust, Databricks Inc. Doug Bateman, Databricks Inc. Matei Zaharia, Computer Science Department, Stanford University Abstract: Originally started as an academic research project at UC Berkeley, Apache Spark is one of the most popular open source projects for big data analytics. Over 1000 volunteers have contributed code to the project; it is supported by virtually every commercial vendor; many universities are now offering courses on Spark. Spark has evolved significantly since the 2010 research paper: its foundational APIs are becoming more relational and structural with the introduction of the Catalyst relational optimizer, and its execution engine is developing quickly to adopt the latest research advances in database systems such as whole-stage code generation. This tutorial is designed for academic researchers (graduate students, faculty members, and industrial researchers) interested in a brief hands-on overview of Spark. This tutorial covers the core APIs for using Spark 2.0, including DataFrames, Datasets, SQL, streaming and machine learning pipelines. Each topic includes slide and lecture content along with hands-on use of a Spark cluster through a web-based notebook environment. In addition, we will dive into the engine internals to discuss architectural design choices and their implications in practice. We will guide the audience to ""hack"" Spark by extending its query optimizer to speed up distributed join execution. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H8M6S,486,2d,hd,FALSE,https://i.ytimg.com/vi/-PYeKtiNPtQ/maxresdefault.jpg,,81,0,0,0,0
276,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,0dSL_evAXkk,2016-10-10T09:30:12Z,10/10/16 9:30,Big Natural Language Data Processing (Part 1),"Authors: Gabor Melli, OpenGov, Inc. Matt Seal, OpenGov, Inc. Abstract: The automated processing of large volumes of text data has become a mission critical capability in a wide-range of industries. Current tools enabled data scientists to produce every more impactful NLP systems. Getting these systems started however, can still be challenging. This tutorial will review the best-practice data-driven methods, tools, and resources for many common applications that require the processing of high volume, high velocity and/or mixed veracity textual data. We will show PDF content extraction, distributed ETL patterns, and NLP tooling with working code samples (mostly in Python) using the Spark and AWS Lambda distributed computing platforms. When completed participants will understand and be able to prototype components of an end-to-end NLP system that achieve baseline results. They will also be shown advanced strategies that can be expanded and further explored without re-implementing the underlying baseline infrastructure.We assume that audience members have a general understanding of textual data, NLP applications, and data science methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT57M32S,3452,2d,hd,FALSE,https://i.ytimg.com/vi/0dSL_evAXkk/maxresdefault.jpg,,129,0,0,0,0
277,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3K0oJRvTNII,2016-10-10T09:30:12Z,10/10/16 9:30,Introduction to Spark 2.0 (Part 3),"Authors: Reynold Xin, Databricks Inc. Michael Armbrust, Databricks Inc. Doug Bateman, Databricks Inc. Matei Zaharia, Computer Science Department, Stanford University Abstract: Originally started as an academic research project at UC Berkeley, Apache Spark is one of the most popular open source projects for big data analytics. Over 1000 volunteers have contributed code to the project; it is supported by virtually every commercial vendor; many universities are now offering courses on Spark. Spark has evolved significantly since the 2010 research paper: its foundational APIs are becoming more relational and structural with the introduction of the Catalyst relational optimizer, and its execution engine is developing quickly to adopt the latest research advances in database systems such as whole-stage code generation. This tutorial is designed for academic researchers (graduate students, faculty members, and industrial researchers) interested in a brief hands-on overview of Spark. This tutorial covers the core APIs for using Spark 2.0, including DataFrames, Datasets, SQL, streaming and machine learning pipelines. Each topic includes slide and lecture content along with hands-on use of a Spark cluster through a web-based notebook environment. In addition, we will dive into the engine internals to discuss architectural design choices and their implications in practice. We will guide the audience to ""hack"" Spark by extending its query optimizer to speed up distributed join execution. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H22M40S,1360,2d,hd,FALSE,https://i.ytimg.com/vi/3K0oJRvTNII/maxresdefault.jpg,,56,0,0,0,0
278,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Gv39NoRCajU,2016-10-10T09:30:12Z,10/10/16 9:30,MXNet (Part 1),"Authors: Mu Li, Computer Science Department, Carnegie Mellon University Tianqi Chen, Department of Computer Science and Engineering, University of Washington Abstract: This hands-on tutorial will work through the pipeline of developing, training and deploying deep learning applications by using MXNet. Multiple applications including recommendation, word embedding will be covered. The participants will learn how to write a deep learning program in a few lines of codes in their favorite language such as Python, Scala, and R and train it on one or multiple GPUs. They will also learn how to deploy a deep learning application in the cloud or in the mobile phones. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M26S,3026,2d,hd,FALSE,https://i.ytimg.com/vi/Gv39NoRCajU/maxresdefault.jpg,,3682,12,1,0,1
279,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,KTWoXk2UmYc,2016-10-10T09:30:12Z,10/10/16 9:30,MXNet (Part 3),"Authors: Mu Li, Computer Science Department, Carnegie Mellon University Tianqi Chen, Department of Computer Science and Engineering, University of Washington Abstract: This hands-on tutorial will work through the pipeline of developing, training and deploying deep learning applications by using MXNet. Multiple applications including recommendation, word embedding will be covered. The participants will learn how to write a deep learning program in a few lines of codes in their favorite language such as Python, Scala, and R and train it on one or multiple GPUs. They will also learn how to deploy a deep learning application in the cloud or in the mobile phones. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT26M55S,1615,2d,hd,FALSE,https://i.ytimg.com/vi/KTWoXk2UmYc/maxresdefault.jpg,,392,4,0,0,0
280,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,RLSXvVy3UDI,2016-10-10T09:30:12Z,10/10/16 9:30,Building Recommender Systems using Photon ML (Part 1),"Authors: Bee-Chung Chen, LinkedIn Corporation Alex Shelkovnykov, LinkedIn Corporation Josh Fleming, LinkedIn Corporation Xianxing Zhang, LinkedIn Corporation Paul Ogilvie, LinkedIn Corporation Deepak Agarwal, LinkedIn Corporation Abstract: Recommendation systems have become ubiquitous for web applications. Given significant heterogeneity in user preference, providing personalized recommendations is key to the success of such systems. To achieve this goal at scale, using machine learned models to estimate user preference from user feedback data is essential. Providing an easy-to-use and flexible machine learning library for practitioners to build personalization models is the key to productivity, agility, and developer happiness. In this tutorial, we first give an overview of the components required for building an end-to-end web recommender system and then focus on how to use Photon ML (LinkedIn’s open-sourced machine learning library) to train recommendation models and serve the results to users. Participants will get hands-on experience in training models of different levels of granularity to improve model performance and perform the “modeling loop” consisting of training a model, scoring candidate items using the model, seeing recommended items in a web UI, giving feedback to a number of recommended items, and then training a model again using the newly generated feedback. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT59M59S,3599,2d,hd,FALSE,https://i.ytimg.com/vi/RLSXvVy3UDI/maxresdefault.jpg,,661,3,0,0,0
281,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,W4p7gvStwjo,2016-10-10T09:30:12Z,10/10/16 9:30,Building Recommender Systems using Photon ML (Part 2),"Authors: Bee-Chung Chen, LinkedIn Corporation Alex Shelkovnykov, LinkedIn Corporation Josh Fleming, LinkedIn Corporation Xianxing Zhang, LinkedIn Corporation Paul Ogilvie, LinkedIn Corporation Deepak Agarwal, LinkedIn Corporation Abstract: Recommendation systems have become ubiquitous for web applications. Given significant heterogeneity in user preference, providing personalized recommendations is key to the success of such systems. To achieve this goal at scale, using machine learned models to estimate user preference from user feedback data is essential. Providing an easy-to-use and flexible machine learning library for practitioners to build personalization models is the key to productivity, agility, and developer happiness. In this tutorial, we first give an overview of the components required for building an end-to-end web recommender system and then focus on how to use Photon ML (LinkedIn’s open-sourced machine learning library) to train recommendation models and serve the results to users. Participants will get hands-on experience in training models of different levels of granularity to improve model performance and perform the “modeling loop” consisting of training a model, scoring candidate items using the model, seeing recommended items in a web UI, giving feedback to a number of recommended items, and then training a model again using the newly generated feedback. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT51M41S,3101,2d,hd,FALSE,https://i.ytimg.com/vi/W4p7gvStwjo/maxresdefault.jpg,,197,1,0,0,0
282,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,dMAcBJZiXQs,2016-10-10T09:30:12Z,10/10/16 9:30,MXNet (Part 2),"Authors: Mu Li, Computer Science Department, Carnegie Mellon University Tianqi Chen, Department of Computer Science and Engineering, University of Washington Abstract: This hands-on tutorial will work through the pipeline of developing, training and deploying deep learning applications by using MXNet. Multiple applications including recommendation, word embedding will be covered. The participants will learn how to write a deep learning program in a few lines of codes in their favorite language such as Python, Scala, and R and train it on one or multiple GPUs. They will also learn how to deploy a deep learning application in the cloud or in the mobile phones. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT59M45S,3585,2d,hd,FALSE,https://i.ytimg.com/vi/dMAcBJZiXQs/maxresdefault.jpg,,667,3,0,0,0
283,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jO3VN5PZdMo,2016-10-10T09:30:12Z,10/10/16 9:30,Big Natural Language Data Processing (Part 2),"Authors: Gabor Melli, OpenGov, Inc. Matt Seal, OpenGov, Inc. Abstract: The automated processing of large volumes of text data has become a mission critical capability in a wide-range of industries. Current tools enabled data scientists to produce every more impactful NLP systems. Getting these systems started however, can still be challenging. This tutorial will review the best-practice data-driven methods, tools, and resources for many common applications that require the processing of high volume, high velocity and/or mixed veracity textual data. We will show PDF content extraction, distributed ETL patterns, and NLP tooling with working code samples (mostly in Python) using the Spark and AWS Lambda distributed computing platforms. When completed participants will understand and be able to prototype components of an end-to-end NLP system that achieve baseline results. They will also be shown advanced strategies that can be expanded and further explored without re-implementing the underlying baseline infrastructure.We assume that audience members have a general understanding of textual data, NLP applications, and data science methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H21M17S,1277,2d,hd,FALSE,https://i.ytimg.com/vi/jO3VN5PZdMo/maxresdefault.jpg,,33,0,0,0,0
284,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,qAwDZrvKeIc,2016-10-10T09:30:12Z,10/10/16 9:30,Introduction to Spark 2.0 (Part 1),"Authors: Reynold Xin, Databricks Inc. Michael Armbrust, Databricks Inc. Doug Bateman, Databricks Inc. Matei Zaharia, Computer Science Department, Stanford University Abstract: Originally started as an academic research project at UC Berkeley, Apache Spark is one of the most popular open source projects for big data analytics. Over 1000 volunteers have contributed code to the project; it is supported by virtually every commercial vendor; many universities are now offering courses on Spark. Spark has evolved significantly since the 2010 research paper: its foundational APIs are becoming more relational and structural with the introduction of the Catalyst relational optimizer, and its execution engine is developing quickly to adopt the latest research advances in database systems such as whole-stage code generation. This tutorial is designed for academic researchers (graduate students, faculty members, and industrial researchers) interested in a brief hands-on overview of Spark. This tutorial covers the core APIs for using Spark 2.0, including DataFrames, Datasets, SQL, streaming and machine learning pipelines. Each topic includes slide and lecture content along with hands-on use of a Spark cluster through a web-based notebook environment. In addition, we will dive into the engine internals to discuss architectural design choices and their implications in practice. We will guide the audience to ""hack"" Spark by extending its query optimizer to speed up distributed join execution. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M47S,2747,2d,hd,FALSE,https://i.ytimg.com/vi/qAwDZrvKeIc/maxresdefault.jpg,,351,0,0,0,0
285,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,LfE-xkwd-l4,2016-10-10T09:26:32Z,10/10/16 9:26,CNTK - Microsoft’s open-source deep-learning toolkit (Part 3),"Authors: Frank Seide, Microsoft Amit Agarwal, Microsoft Abstract: Train neural networks like Microsoft product groups! This talk will introduce the Computational Network Toolkit, or CNTK, Microsoft’s scalable open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. This tutorial is targeted at current or future deep-learning practitioners looking for a tool that is easy to use yet efficient and scalable across multi-machine GPU clusters for real-world workloads. The tutorial assumes basic knowledge of deep learning. Participants will get to understand CNTK’s core concepts and usage, and practice to run neural-network trainings with CNTK for image recognition and text processing. The tutorial will be a starting point for solving your own real-world deep-learning task with CNTK. To run the examples, a laptop with Windows 7+ or Linux is required, and a CUDA-capable GPU is recommended. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT44M57S,2697,2d,hd,FALSE,https://i.ytimg.com/vi/LfE-xkwd-l4/maxresdefault.jpg,,187,0,0,0,0
286,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,rraTncC5KQ0,2016-10-10T09:26:29Z,10/10/16 9:26,CNTK - Microsoft’s open-source deep-learning toolkit (Part 2),"Authors: Frank Seide, Microsoft Amit Agarwal, Microsoft Abstract: Train neural networks like Microsoft product groups! This talk will introduce the Computational Network Toolkit, or CNTK, Microsoft’s scalable open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. This tutorial is targeted at current or future deep-learning practitioners looking for a tool that is easy to use yet efficient and scalable across multi-machine GPU clusters for real-world workloads. The tutorial assumes basic knowledge of deep learning. Participants will get to understand CNTK’s core concepts and usage, and practice to run neural-network trainings with CNTK for image recognition and text processing. The tutorial will be a starting point for solving your own real-world deep-learning task with CNTK. To run the examples, a laptop with Windows 7+ or Linux is required, and a CUDA-capable GPU is recommended. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H9M13S,553,2d,hd,FALSE,https://i.ytimg.com/vi/rraTncC5KQ0/maxresdefault.jpg,,280,0,0,0,0
287,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,AJwn7doMiU0,2016-10-10T09:26:25Z,10/10/16 9:26,CNTK - Microsoft’s open-source deep-learning toolkit (Part 1),"Authors: Frank Seide, Microsoft Amit Agarwal, Microsoft Abstract: Train neural networks like Microsoft product groups! This talk will introduce the Computational Network Toolkit, or CNTK, Microsoft’s scalable open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. This tutorial is targeted at current or future deep-learning practitioners looking for a tool that is easy to use yet efficient and scalable across multi-machine GPU clusters for real-world workloads. The tutorial assumes basic knowledge of deep learning. Participants will get to understand CNTK’s core concepts and usage, and practice to run neural-network trainings with CNTK for image recognition and text processing. The tutorial will be a starting point for solving your own real-world deep-learning task with CNTK. To run the examples, a laptop with Windows 7+ or Linux is required, and a CUDA-capable GPU is recommended. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT54M25S,3265,2d,hd,FALSE,https://i.ytimg.com/vi/AJwn7doMiU0/maxresdefault.jpg,,1458,3,2,0,0
288,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,zgL-AXnjy-c,2016-10-10T09:24:57Z,10/10/16 9:24,Getting Started with Amazon Web Services Bootcamp (Part 3),"Authors: Androski Spicer, Amazon Web Services, Inc. Ujjwal Ratan, Amazon Web Services, Inc. Jack Hemion, Amazon Web Services, Inc. Abstract: The term ""cloud computing"" refers to the on-demand delivery of IT resources via the Internet with pay-as-you-go pricing. Instead of buying, owning and maintaining your own datacenters and servers, organizations can acquire technology such as compute power, storage, databases, and other services on an as-needed basis. At AWS, we invest in customer success through an accelerating pace of innovation with a focus on operating efficiently at massive scale, and growing an expansive partner ecosystem. We have worked closely with industry leaders from GE to Capital One, Comcast to Kellogg’s, to help them transform their existing businesses at a pace that cannot be duplicated with traditional infrastructure alternatives. This bootcamp is aimed at helping you to build the foundation of up on which all your apps will run. The Getting Started with Amazon Web Services (AWS) Bootcamp is designed to provide you with a close look at using AWS best practices for building Virtual Private Cloud environments using The Amazon Virtual Private Cloud (VPC) service. The Amazon VPC is a logical segmentation of the AWS Cloud in which you can design your network structure and launch your workload with a secured boundary around it. This bootcamp will also introduce you to Amazon EC2 and the best practices for launching and managing these virtual machines. It will also introduce to you concepts like bootstrapping, Amazon Machine Images and building fault tolerant and high availability environments. At the end of both labs, the presenter will demonstrate a simple machine learning application that reacts upon data that is generated by the EC2 Instances that you launched. There are two segments to this bootcamp; presentation and a hands-on lab. Please see the below agenda for the order in which the bootcamp will be executed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT34M10S,2050,2d,hd,FALSE,https://i.ytimg.com/vi/zgL-AXnjy-c/maxresdefault.jpg,,93,1,0,0,0
289,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,DAPKCwifZEo,2016-10-10T09:24:53Z,10/10/16 9:24,Getting Started with Amazon Web Services Bootcamp (Part 2),"Authors: Androski Spicer, Amazon Web Services, Inc. Ujjwal Ratan, Amazon Web Services, Inc. Jack Hemion, Amazon Web Services, Inc. Abstract: The term ""cloud computing"" refers to the on-demand delivery of IT resources via the Internet with pay-as-you-go pricing. Instead of buying, owning and maintaining your own datacenters and servers, organizations can acquire technology such as compute power, storage, databases, and other services on an as-needed basis. At AWS, we invest in customer success through an accelerating pace of innovation with a focus on operating efficiently at massive scale, and growing an expansive partner ecosystem. We have worked closely with industry leaders from GE to Capital One, Comcast to Kellogg’s, to help them transform their existing businesses at a pace that cannot be duplicated with traditional infrastructure alternatives. This bootcamp is aimed at helping you to build the foundation of up on which all your apps will run. The Getting Started with Amazon Web Services (AWS) Bootcamp is designed to provide you with a close look at using AWS best practices for building Virtual Private Cloud environments using The Amazon Virtual Private Cloud (VPC) service. The Amazon VPC is a logical segmentation of the AWS Cloud in which you can design your network structure and launch your workload with a secured boundary around it. This bootcamp will also introduce you to Amazon EC2 and the best practices for launching and managing these virtual machines. It will also introduce to you concepts like bootstrapping, Amazon Machine Images and building fault tolerant and high availability environments. At the end of both labs, the presenter will demonstrate a simple machine learning application that reacts upon data that is generated by the EC2 Instances that you launched. There are two segments to this bootcamp; presentation and a hands-on lab. Please see the below agenda for the order in which the bootcamp will be executed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M53S,2753,2d,hd,FALSE,https://i.ytimg.com/vi/DAPKCwifZEo/maxresdefault.jpg,,103,1,0,0,0
290,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,j7e9veC5qp4,2016-10-10T09:24:50Z,10/10/16 9:24,Getting Started with Amazon Web Services Bootcamp (Part 1),"Authors: Androski Spicer, Amazon Web Services, Inc. Ujjwal Ratan, Amazon Web Services, Inc. Jack Hemion, Amazon Web Services, Inc. Abstract: The term ""cloud computing"" refers to the on-demand delivery of IT resources via the Internet with pay-as-you-go pricing. Instead of buying, owning and maintaining your own datacenters and servers, organizations can acquire technology such as compute power, storage, databases, and other services on an as-needed basis. At AWS, we invest in customer success through an accelerating pace of innovation with a focus on operating efficiently at massive scale, and growing an expansive partner ecosystem. We have worked closely with industry leaders from GE to Capital One, Comcast to Kellogg’s, to help them transform their existing businesses at a pace that cannot be duplicated with traditional infrastructure alternatives. This bootcamp is aimed at helping you to build the foundation of up on which all your apps will run. The Getting Started with Amazon Web Services (AWS) Bootcamp is designed to provide you with a close look at using AWS best practices for building Virtual Private Cloud environments using The Amazon Virtual Private Cloud (VPC) service. The Amazon VPC is a logical segmentation of the AWS Cloud in which you can design your network structure and launch your workload with a secured boundary around it. This bootcamp will also introduce you to Amazon EC2 and the best practices for launching and managing these virtual machines. It will also introduce to you concepts like bootstrapping, Amazon Machine Images and building fault tolerant and high availability environments. At the end of both labs, the presenter will demonstrate a simple machine learning application that reacts upon data that is generated by the EC2 Instances that you launched. There are two segments to this bootcamp; presentation and a hands-on lab. Please see the below agenda for the order in which the bootcamp will be executed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H5S,5,2d,hd,FALSE,https://i.ytimg.com/vi/j7e9veC5qp4/maxresdefault.jpg,,490,4,0,0,0
291,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,c68yqYnpy1A,2016-10-10T09:23:17Z,10/10/16 9:23,Streaming Analytics (Part 2),"Author: Ashish Gupta, LinkedIn Corporation Abstract: Recently we have seen emergence and huge adoption of social media, internet of things for home, industrial internet of things,mobile applications and online transactions. These systems generate streaming data at very large scale. Building technologies and distributed systems that can capture, process and analyze this streaming data in real time is very important for gaining real time insights. Real-time analysis of streaming data can be used for applications as diverse as fraud detection, in-session targeting and recommendations, control systems for transportation systems and smarter cities, earthquake prediction and control of autonomous vehicles. This programming tutorial provides overview of streaming data systems and hands on tutorial on building streaming systems using open source technologies. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT48M29S,2909,2d,hd,FALSE,https://i.ytimg.com/vi/c68yqYnpy1A/maxresdefault.jpg,,34,0,0,0,0
292,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,L-IS5NaUVv4,2016-10-10T09:23:14Z,10/10/16 9:23,Streaming Analytics (Part 1),"Author: Ashish Gupta, LinkedIn Corporation Abstract: Recently we have seen emergence and huge adoption of social media, internet of things for home, industrial internet of things,mobile applications and online transactions. These systems generate streaming data at very large scale. Building technologies and distributed systems that can capture, process and analyze this streaming data in real time is very important for gaining real time insights. Real-time analysis of streaming data can be used for applications as diverse as fraud detection, in-session targeting and recommendations, control systems for transportation systems and smarter cities, earthquake prediction and control of autonomous vehicles. This programming tutorial provides overview of streaming data systems and hands on tutorial on building streaming systems using open source technologies. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT46M59S,2819,2d,hd,FALSE,https://i.ytimg.com/vi/L-IS5NaUVv4/maxresdefault.jpg,,201,0,0,0,0
293,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,R-nVDpCYKNg,2016-10-10T09:21:44Z,10/10/16 9:21,Scalable R on Spark (Part 2),"Authors: John-Mark Agosta, Microsoft Debraj GuhaThakurta, Microsoft Robert Horton, Microsoft Mario Inchiosa, Microsoft Srini Kumar, Microsoft Vanja Paunić, Microsoft Hang Zhang, Microsoft Mengyue Zhao, Microsoft Abstract: R is one of the most popular languages in the data science, statistical and machine learning (ML) community. However, when it comes to scalable data analysis and ML using R, many data scientists are blocked or hindered by (a) its limitations of available functions to handle large data-sets efficiently, and (b) knowledge about the appropriate computing environments to scale R scripts from desktop exploratory analysis to elastic and distributed cloud services. In this tutorial we will discuss solutions that demonstrate the use of distributed compute environments and end to end solutions for R. We will present the topics through presentations and hands-on examples with sample code. In addition, we will provide a public code repository that attendees will be able to access and adapt to their own practice. We believe this tutorial will be of strong interest to a large and growing community of data scientists and developers using R for data analysis and modeling. Prerequisites: A laptop with a web browser and an ssh client that supports port forwarding. Access to cloud-based clusters will be provided. For R scripts, download details, and suggested reading, see the Readme.md file at https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/KDDCup2016. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H28M46S,1726,2d,hd,FALSE,https://i.ytimg.com/vi/R-nVDpCYKNg/maxresdefault.jpg,,73,0,0,0,0
294,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,rcrgCyzVBXo,2016-10-10T09:21:38Z,10/10/16 9:21,Scalable R on Spark (Part 1),"Authors: John-Mark Agosta, Microsoft Debraj GuhaThakurta, Microsoft Robert Horton, Microsoft Mario Inchiosa, Microsoft Srini Kumar, Microsoft Vanja Paunić, Microsoft Hang Zhang, Microsoft Mengyue Zhao, Microsoft Abstract: R is one of the most popular languages in the data science, statistical and machine learning (ML) community. However, when it comes to scalable data analysis and ML using R, many data scientists are blocked or hindered by (a) its limitations of available functions to handle large data-sets efficiently, and (b) knowledge about the appropriate computing environments to scale R scripts from desktop exploratory analysis to elastic and distributed cloud services. In this tutorial we will discuss solutions that demonstrate the use of distributed compute environments and end to end solutions for R. We will present the topics through presentations and hands-on examples with sample code. In addition, we will provide a public code repository that attendees will be able to access and adapt to their own practice. We believe this tutorial will be of strong interest to a large and growing community of data scientists and developers using R for data analysis and modeling. Prerequisites: A laptop with a web browser and an ssh client that supports port forwarding. Access to cloud-based clusters will be provided. For R scripts, download details, and suggested reading, see the Readme.md file at https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/KDDCup2016. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H17M32S,1052,2d,hd,FALSE,https://i.ytimg.com/vi/rcrgCyzVBXo/maxresdefault.jpg,,221,1,0,0,0
295,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MTUYsr5GNWw,2016-10-05T19:11:42Z,5/10/16 19:11,Scalable Learning of Graphical Models (Part 2),"Authors: Geoff Webb, Faculty of Information Technology, Monash University François Petitjean, Faculty of Information Technology, Monash University Abstract: From understanding the structure of data, to classification and topic modeling, graphical models are core tools in machine learning and data mining. They combine probability and graph theories to form a compact representation of probability distributions. In the last decade, as data stores became larger and higher-dimensional, traditional algorithms for learning graphical models from data, with their lack of scalability, became less and less usable, thus directly decreasing the potential benefits of this core technology. To scale graphical modeling techniques to the size and dimensionality of most modern data stores, data science researchers and practitioners now have to meld the most recent advances in numerous specialized fields including graph theory, statistics, pattern mining and graphical modeling. This tutorial covers the core building blocks that are necessary to build and use scalable graphical modeling technologies on large and high-dimensional data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H3M30S,210,2d,hd,FALSE,https://i.ytimg.com/vi/MTUYsr5GNWw/maxresdefault.jpg,,65,0,0,0,0
296,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,V6sN6Z3UAJM,2016-10-05T19:11:40Z,5/10/16 19:11,Scalable Learning of Graphical Models (Part 1),"Authors: Geoff Webb, Faculty of Information Technology, Monash University François Petitjean, Faculty of Information Technology, Monash University Abstract: From understanding the structure of data, to classification and topic modeling, graphical models are core tools in machine learning and data mining. They combine probability and graph theories to form a compact representation of probability distributions. In the last decade, as data stores became larger and higher-dimensional, traditional algorithms for learning graphical models from data, with their lack of scalability, became less and less usable, thus directly decreasing the potential benefits of this core technology. To scale graphical modeling techniques to the size and dimensionality of most modern data stores, data science researchers and practitioners now have to meld the most recent advances in numerous specialized fields including graph theory, statistics, pattern mining and graphical modeling. This tutorial covers the core building blocks that are necessary to build and use scalable graphical modeling technologies on large and high-dimensional data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H22M,1320,2d,hd,FALSE,https://i.ytimg.com/vi/V6sN6Z3UAJM/maxresdefault.jpg,,259,2,0,0,0
297,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_XkSNbuHf08,2016-10-05T19:09:52Z,5/10/16 19:09,Collective Sensemaking via Social Sensors (Part 3),"Authors: Jiebo Luo, Department of Computer Science, University of Rochester Yu-Ru Lin, School of Information Sciences, University of Pittsburgh Yuheng Hu, Department of Information and Decision Sciences, University of Illinois at Chicago Abstract: Social media platforms like Twitter and Facebook have emerged as some of the most important platforms for people to discover, report, share, and communicate with others about various public events, be they of global or local interest (some high profile examples include the U.S Presidential debates, the Boston bombings, the hurricane Sandy, etc). The burst of social media reaction can be seen as a valuable real-time reflection of events as they happen, and can be used for a variety of applications such as computational journalism. Until now, such analysis has been mostly done manually or through primitive tools. Scalable and automated approaches are needed given the massive amounts of both event and reaction information. These approaches must also be able to conduct in-depth analysis of complex interactions between an event and its audience. Supporting such automation and examination however poses several computational challenges. In recent years, research communities have witnessed a growing interest in tackling these challenges. Furthermore, much recent research has begun to focus on solving more complex event analytics tasks such as post-event effect quantification and event progress prediction. This tutorial aims to review and examine current state of the research progress on this emerging topic. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H25M58S,1558,2d,hd,FALSE,https://i.ytimg.com/vi/_XkSNbuHf08/maxresdefault.jpg,,134,0,0,0,0
298,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VSCba3af5N4,2016-10-05T19:09:49Z,5/10/16 19:09,Collective Sensemaking via Social Sensors (Part 2),"Authors: Jiebo Luo, Department of Computer Science, University of Rochester Yu-Ru Lin, School of Information Sciences, University of Pittsburgh Yuheng Hu, Department of Information and Decision Sciences, University of Illinois at Chicago Abstract: Social media platforms like Twitter and Facebook have emerged as some of the most important platforms for people to discover, report, share, and communicate with others about various public events, be they of global or local interest (some high profile examples include the U.S Presidential debates, the Boston bombings, the hurricane Sandy, etc). The burst of social media reaction can be seen as a valuable real-time reflection of events as they happen, and can be used for a variety of applications such as computational journalism. Until now, such analysis has been mostly done manually or through primitive tools. Scalable and automated approaches are needed given the massive amounts of both event and reaction information. These approaches must also be able to conduct in-depth analysis of complex interactions between an event and its audience. Supporting such automation and examination however poses several computational challenges. In recent years, research communities have witnessed a growing interest in tackling these challenges. Furthermore, much recent research has begun to focus on solving more complex event analytics tasks such as post-event effect quantification and event progress prediction. This tutorial aims to review and examine current state of the research progress on this emerging topic. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H9M39S,579,2d,hd,FALSE,https://i.ytimg.com/vi/VSCba3af5N4/maxresdefault.jpg,,53,0,0,0,0
299,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,G5bMeVWZ1fU,2016-10-05T19:09:43Z,5/10/16 19:09,Collective Sensemaking via Social Sensors (Part 1),"Authors: Jiebo Luo, Department of Computer Science, University of Rochester Yu-Ru Lin, School of Information Sciences, University of Pittsburgh Yuheng Hu, Department of Information and Decision Sciences, University of Illinois at Chicago Abstract: Social media platforms like Twitter and Facebook have emerged as some of the most important platforms for people to discover, report, share, and communicate with others about various public events, be they of global or local interest (some high profile examples include the U.S Presidential debates, the Boston bombings, the hurricane Sandy, etc). The burst of social media reaction can be seen as a valuable real-time reflection of events as they happen, and can be used for a variety of applications such as computational journalism. Until now, such analysis has been mostly done manually or through primitive tools. Scalable and automated approaches are needed given the massive amounts of both event and reaction information. These approaches must also be able to conduct in-depth analysis of complex interactions between an event and its audience. Supporting such automation and examination however poses several computational challenges. In recent years, research communities have witnessed a growing interest in tackling these challenges. Furthermore, much recent research has begun to focus on solving more complex event analytics tasks such as post-event effect quantification and event progress prediction. This tutorial aims to review and examine current state of the research progress on this emerging topic. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT46M2S,2762,2d,hd,FALSE,https://i.ytimg.com/vi/G5bMeVWZ1fU/maxresdefault.jpg,,140,1,0,0,0
300,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MTry0cQB1C8,2016-10-05T19:06:38Z,5/10/16 19:06,Mining Reliable Information from Passively and Actively Crowdsourced Data (Part 3),"Authors: Jiawei Han, Department of Computer Science, University of Illinois at Urbana-Champaign Wei Fan, Baidu, Inc. Bo Zhao, LinkedIn Corporation Qi Li, Department of Computer Science and Engineering, University at Buffalo Jing Gao, Department of Computer Science and Engineering, University at Buffalo Abstract: Recent years have witnessed an astonishing growth of crowd-contributed data, which has become a powerful information source that covers almost every aspect of our lives. This big treasure trove of information has fundamentally changed the ways in which we learn about our world. Crowdsourcing has attracted considerable attentions with various approaches developed to utilize these enormous crowdsourced data from different perspectives. From the data collection perspective, crowdsourced data can be divided into two types: ""passively"" crowdsourced data and ""actively"" crowdsourced data; from task perspective, crowdsourcing research includes information aggregation, budget allocation, worker incentive mechanism, etc. To answer the need of a systematic introduction of the field and comparison of the techniques, we will present an organized picture on crowdsourcing methods in this tutorial. The covered topics will be interested for both advanced researchers and beginners in this field. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT1H24M17S,1457,2d,hd,FALSE,https://i.ytimg.com/vi/MTry0cQB1C8/maxresdefault.jpg,,94,0,0,0,0
301,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,FeDYX6Xmv2k,2016-10-05T19:06:35Z,5/10/16 19:06,Mining Reliable Information from Passively and Actively Crowdsourced Data (Part 2),"Authors: Jiawei Han, Department of Computer Science, University of Illinois at Urbana-Champaign Wei Fan, Baidu, Inc. Bo Zhao, LinkedIn Corporation Qi Li, Department of Computer Science and Engineering, University at Buffalo Jing Gao, Department of Computer Science and Engineering, University at Buffalo Abstract: Recent years have witnessed an astonishing growth of crowd-contributed data, which has become a powerful information source that covers almost every aspect of our lives. This big treasure trove of information has fundamentally changed the ways in which we learn about our world. Crowdsourcing has attracted considerable attentions with various approaches developed to utilize these enormous crowdsourced data from different perspectives. From the data collection perspective, crowdsourced data can be divided into two types: ""passively"" crowdsourced data and ""actively"" crowdsourced data; from task perspective, crowdsourcing research includes information aggregation, budget allocation, worker incentive mechanism, etc. To answer the need of a systematic introduction of the field and comparison of the techniques, we will present an organized picture on crowdsourcing methods in this tutorial. The covered topics will be interested for both advanced researchers and beginners in this field. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT42M47S,2567,2d,hd,FALSE,https://i.ytimg.com/vi/FeDYX6Xmv2k/maxresdefault.jpg,,105,0,0,0,0
302,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3o7TbV_dUJs,2016-10-05T19:06:32Z,5/10/16 19:06,Mining Reliable Information from Passively and Actively Crowdsourced Data (Part 1),"Authors: Jiawei Han, Department of Computer Science, University of Illinois at Urbana-Champaign Wei Fan, Baidu, Inc. Bo Zhao, LinkedIn Corporation Qi Li, Department of Computer Science and Engineering, University at Buffalo Jing Gao, Department of Computer Science and Engineering, University at Buffalo Abstract: Recent years have witnessed an astonishing growth of crowd-contributed data, which has become a powerful information source that covers almost every aspect of our lives. This big treasure trove of information has fundamentally changed the ways in which we learn about our world. Crowdsourcing has attracted considerable attentions with various approaches developed to utilize these enormous crowdsourced data from different perspectives. From the data collection perspective, crowdsourced data can be divided into two types: ""passively"" crowdsourced data and ""actively"" crowdsourced data; from task perspective, crowdsourcing research includes information aggregation, budget allocation, worker incentive mechanism, etc. To answer the need of a systematic introduction of the field and comparison of the techniques, we will present an organized picture on crowdsourcing methods in this tutorial. The covered topics will be interested for both advanced researchers and beginners in this field. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT1H3M52S,232,2d,hd,FALSE,https://i.ytimg.com/vi/3o7TbV_dUJs/maxresdefault.jpg,,196,0,0,0,0
303,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5dJDd-gvgJw,2016-10-05T15:25:40Z,5/10/16 15:25,Healthcare Data Mining with Matrix Models (Part 2),"Authors: Joel Dudley, Icahn School of Medicine at Mount Sinai Ping Zhang, IBM Thomas J. Watson Research Center Fei Wang, Department of Healthcare Policy and Research, Cornell University Abstract: In the last decade, advances in high-throughput technologies, growth of clinical data warehouses, and rapid accumulation of biomedical knowledge provided unprecedented opportunities and challenges to researchers in biomedical informatics. One distinct solution, to efficiently conduct big data analytics for biomedical problems, is the application of matrix computation and factorization methods such as non-negative matrix factorization, joint matrix factorization, tensor factorization. Compared to probabilistic and information theoretic approaches, matrix-based methods are fast, easy to understand and implement. In this tutorial, we provide a review of recent advances in algorithms and methods using matrix and their potential applications in biomedical informatics. We survey various related articles from data mining venues as well as from biomedical informatics venues to share with the audience key problems and trends in matrix computation research, with different novel applications such as drug repositioning, personalized medicine, and electronic phenotyping. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H31M13S,1873,2d,hd,FALSE,https://i.ytimg.com/vi/5dJDd-gvgJw/maxresdefault.jpg,,221,2,0,0,0
304,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,vOHC5OSsSww,2016-10-05T15:25:32Z,5/10/16 15:25,Healthcare Data Mining with Matrix Models (Part 1),"Authors: Joel Dudley, Icahn School of Medicine at Mount Sinai Ping Zhang, IBM Thomas J. Watson Research Center Fei Wang, Department of Healthcare Policy and Research, Cornell University Abstract: In the last decade, advances in high-throughput technologies, growth of clinical data warehouses, and rapid accumulation of biomedical knowledge provided unprecedented opportunities and challenges to researchers in biomedical informatics. One distinct solution, to efficiently conduct big data analytics for biomedical problems, is the application of matrix computation and factorization methods such as non-negative matrix factorization, joint matrix factorization, tensor factorization. Compared to probabilistic and information theoretic approaches, matrix-based methods are fast, easy to understand and implement. In this tutorial, we provide a review of recent advances in algorithms and methods using matrix and their potential applications in biomedical informatics. We survey various related articles from data mining venues as well as from biomedical informatics venues to share with the audience key problems and trends in matrix computation research, with different novel applications such as drug repositioning, personalized medicine, and electronic phenotyping. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H44M58S,2698,2d,hd,FALSE,https://i.ytimg.com/vi/vOHC5OSsSww/maxresdefault.jpg,,893,8,0,0,0
305,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,X0F6xRTjPdo,2016-10-05T15:22:48Z,5/10/16 15:22,Lifelong Machine Learning and Computer Reading the Web (Part 3),"Authors: Bing Liu, University of Illinois at Urbana-Champaign Estevam R. Hruschka, Federal University of Săo Carlos Zhiyuan (Brett) Chen, Department of Computer Science, University of Illinois at Chicago Abstract: This tutorial introduces Lifelong Machine Learning (LML) and Machine Reading. The core idea of LML is to learn continuously and accumulate the learned knowledge, and to use the knowledge to help future learning, which is perhaps the hallmark of human learning and human intelligence. By us- ing prior knowledge seamlessly and effortlessly, we humans can learn without a lot of training data, but current machine learning algorithms tend to need a huge amount of training data. LML aims to mimic this human capability. Machine Reading is a research area with the goal of building systems to read natural language text. Among different approaches employed in Machine Reading, this tutorial focuses on projects and approaches that use the idea of LML. Most current machine learning (ML) algorithms learn in isolation. They are designed to address a specific problem using a single dataset. That is, given a dataset, an ML algorithm is executed on the dataset to build a model. Although this type of isolated learning is very useful, it does not have the ability to accumulate past knowledge and to make use of the knowledge for future learning, which we believe are critical for the future of machine learning and data mining. LML aims to design and develop computational systems and algorithms with this capability, i.e., to learn as humans do in a lifelong manner. In this tutorial, we introduce this important problem and the existing LML techniques and discuss opportunities and challenges of big data for lifelong machine learning. We also want to motivate researchers and practitioners to actively explore LML as the big data provides us a golden opportunity to learn a large volume of diverse knowledge, to connect different pieces of it, and to use it to raise data mining and machine learning to a new level. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT40M55S,2455,2d,hd,FALSE,https://i.ytimg.com/vi/X0F6xRTjPdo/maxresdefault.jpg,,170,0,0,0,0
306,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wc2xn4g1-uU,2016-10-05T15:22:38Z,5/10/16 15:22,Lifelong Machine Learning and Computer Reading the Web (Part 2),"Authors: Bing Liu, University of Illinois at Urbana-Champaign Estevam R. Hruschka, Federal University of Săo Carlos Zhiyuan (Brett) Chen, Department of Computer Science, University of Illinois at Chicago Abstract: This tutorial introduces Lifelong Machine Learning (LML) and Machine Reading. The core idea of LML is to learn continuously and accumulate the learned knowledge, and to use the knowledge to help future learning, which is perhaps the hallmark of human learning and human intelligence. By us- ing prior knowledge seamlessly and effortlessly, we humans can learn without a lot of training data, but current machine learning algorithms tend to need a huge amount of training data. LML aims to mimic this human capability. Machine Reading is a research area with the goal of building systems to read natural language text. Among different approaches employed in Machine Reading, this tutorial focuses on projects and approaches that use the idea of LML. Most current machine learning (ML) algorithms learn in isolation. They are designed to address a specific problem using a single dataset. That is, given a dataset, an ML algorithm is executed on the dataset to build a model. Although this type of isolated learning is very useful, it does not have the ability to accumulate past knowledge and to make use of the knowledge for future learning, which we believe are critical for the future of machine learning and data mining. LML aims to design and develop computational systems and algorithms with this capability, i.e., to learn as humans do in a lifelong manner. In this tutorial, we introduce this important problem and the existing LML techniques and discuss opportunities and challenges of big data for lifelong machine learning. We also want to motivate researchers and practitioners to actively explore LML as the big data provides us a golden opportunity to learn a large volume of diverse knowledge, to connect different pieces of it, and to use it to raise data mining and machine learning to a new level. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H1M11S,71,2d,hd,FALSE,https://i.ytimg.com/vi/wc2xn4g1-uU/maxresdefault.jpg,,537,3,0,0,0
307,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,vEM-zuKxtjc,2016-10-05T15:22:30Z,5/10/16 15:22,Lifelong Machine Learning and Computer Reading the Web (Part 1),"Authors: Bing Liu, University of Illinois at Urbana-Champaign Estevam R. Hruschka, Federal University of Săo Carlos Zhiyuan (Brett) Chen, Department of Computer Science, University of Illinois at Chicago Abstract: This tutorial introduces Lifelong Machine Learning (LML) and Machine Reading. The core idea of LML is to learn continuously and accumulate the learned knowledge, and to use the knowledge to help future learning, which is perhaps the hallmark of human learning and human intelligence. By us- ing prior knowledge seamlessly and effortlessly, we humans can learn without a lot of training data, but current machine learning algorithms tend to need a huge amount of training data. LML aims to mimic this human capability. Machine Reading is a research area with the goal of building systems to read natural language text. Among different approaches employed in Machine Reading, this tutorial focuses on projects and approaches that use the idea of LML. Most current machine learning (ML) algorithms learn in isolation. They are designed to address a specific problem using a single dataset. That is, given a dataset, an ML algorithm is executed on the dataset to build a model. Although this type of isolated learning is very useful, it does not have the ability to accumulate past knowledge and to make use of the knowledge for future learning, which we believe are critical for the future of machine learning and data mining. LML aims to design and develop computational systems and algorithms with this capability, i.e., to learn as humans do in a lifelong manner. In this tutorial, we introduce this important problem and the existing LML techniques and discuss opportunities and challenges of big data for lifelong machine learning. We also want to motivate researchers and practitioners to actively explore LML as the big data provides us a golden opportunity to learn a large volume of diverse knowledge, to connect different pieces of it, and to use it to raise data mining and machine learning to a new level. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT29M20S,1760,2d,hd,FALSE,https://i.ytimg.com/vi/vEM-zuKxtjc/maxresdefault.jpg,,777,6,1,0,0
308,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TKkxcBOjY3A,2016-10-05T15:19:36Z,5/10/16 15:19,"Leveraging Propagation for Data Mining: Models, Algorithms and Applications (Part 3)","Authors: Naren Ramakrishnan, Department of Computer Science, Virginia Polytechnic Institute and State University B. Aditya Prakash, Department of Computer Science, Virginia Polytechnic Institute and State University Abstract: Can we infer if a user is sick from her tweet? How do opinions get formed in online forums? Which people should we immunize to prevent an epidemic as fast as possible? How do we quickly zoom out of a graph? Graphs - also known as networks - are powerful tools for modeling processes and situations of interest in real life domains of social systems, cyber-security, epidemiology, and biology. They are ubiquitous, from online social networks, gene-regulatory networks, to router graphs. This tutorial will cover recent and state-of-the-art research on how propagation-like processes can help big-data mining specifically involving large networks and time-series, algorithms behind network problems, and their practical applications in various diverse settings. Topics include diffusion and virus propagation in networks, anomaly and outbreak detection, event prediction and connections with work in public health, the web and online media, social sciences, humanities, and cyber-security. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H12M6S,726,2d,hd,FALSE,https://i.ytimg.com/vi/TKkxcBOjY3A/maxresdefault.jpg,,41,0,0,0,0
309,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,njm-1QD1pZ4,2016-10-05T15:19:30Z,5/10/16 15:19,"Leveraging Propagation for Data Mining: Models, Algorithms and Applications (Part 2)","Authors: Naren Ramakrishnan, Department of Computer Science, Virginia Polytechnic Institute and State University B. Aditya Prakash, Department of Computer Science, Virginia Polytechnic Institute and State University Abstract: Can we infer if a user is sick from her tweet? How do opinions get formed in online forums? Which people should we immunize to prevent an epidemic as fast as possible? How do we quickly zoom out of a graph? Graphs - also known as networks - are powerful tools for modeling processes and situations of interest in real life domains of social systems, cyber-security, epidemiology, and biology. They are ubiquitous, from online social networks, gene-regulatory networks, to router graphs. This tutorial will cover recent and state-of-the-art research on how propagation-like processes can help big-data mining specifically involving large networks and time-series, algorithms behind network problems, and their practical applications in various diverse settings. Topics include diffusion and virus propagation in networks, anomaly and outbreak detection, event prediction and connections with work in public health, the web and online media, social sciences, humanities, and cyber-security. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT50M55S,3055,2d,hd,FALSE,https://i.ytimg.com/vi/njm-1QD1pZ4/maxresdefault.jpg,,55,0,0,0,0
310,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,A-JTgkWWCY0,2016-10-05T15:19:27Z,5/10/16 15:19,"Leveraging Propagation for Data Mining: Models, Algorithms and Applications (Part 1)","Authors: Naren Ramakrishnan, Department of Computer Science, Virginia Polytechnic Institute and State University B. Aditya Prakash, Department of Computer Science, Virginia Polytechnic Institute and State University Abstract: Can we infer if a user is sick from her tweet? How do opinions get formed in online forums? Which people should we immunize to prevent an epidemic as fast as possible? How do we quickly zoom out of a graph? Graphs - also known as networks - are powerful tools for modeling processes and situations of interest in real life domains of social systems, cyber-security, epidemiology, and biology. They are ubiquitous, from online social networks, gene-regulatory networks, to router graphs. This tutorial will cover recent and state-of-the-art research on how propagation-like processes can help big-data mining specifically involving large networks and time-series, algorithms behind network problems, and their practical applications in various diverse settings. Topics include diffusion and virus propagation in networks, anomaly and outbreak detection, event prediction and connections with work in public health, the web and online media, social sciences, humanities, and cyber-security. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT59M49S,3589,2d,hd,FALSE,https://i.ytimg.com/vi/A-JTgkWWCY0/maxresdefault.jpg,,128,1,0,0,0
311,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-10-05T15:19:19Z,5/10/16 15:19,Business Applications of Predicitive Modeling at Scale (Part 3),"Authors: Yan Liu, LinkedIn Corporation Paul Ogilvie, LinkedIn Corporation Songtao Guo, LinkedIn Corporation Qiang Zhu, LinkedIn Corporation Abstract: Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H2M48S,168,2d,hd,FALSE,https://i.ytimg.com/vi/-soZ69_wPRI/maxresdefault.jpg,,51,0,0,0,0
312,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,fMHxvDl2o6A,2016-10-05T15:19:15Z,5/10/16 15:19,Business Applications of Predicitive Modeling at Scale (Part 2),"Authors: Yan Liu, LinkedIn Corporation Paul Ogilvie, LinkedIn Corporation Songtao Guo, LinkedIn Corporation Qiang Zhu, LinkedIn Corporation Abstract: Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT38M56S,2336,2d,hd,FALSE,https://i.ytimg.com/vi/fMHxvDl2o6A/maxresdefault.jpg,,79,1,0,0,0
313,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VaTh6SpHgHI,2016-10-05T15:19:11Z,5/10/16 15:19,Business Applications of Predicitive Modeling at Scale (Part 1),"Authors: Yan Liu, LinkedIn Corporation Paul Ogilvie, LinkedIn Corporation Songtao Guo, LinkedIn Corporation Qiang Zhu, LinkedIn Corporation Abstract: Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H4M54S,294,2d,hd,FALSE,https://i.ytimg.com/vi/VaTh6SpHgHI/maxresdefault.jpg,,192,0,0,0,0
314,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pkkd-oJ-kc0,2016-10-05T15:19:07Z,5/10/16 15:19,IoT Big Data Stream Mining (Part 3),"Authors: Latifur Khan, Department of Computer Science, Erik Jonsson School of Engineering & Computer Science, The University of Texas at Dallas João Gama, Laboratory of Artificial Intelligence and Decision Support, University of Porto Albert Bifet, Telecom ParisTech Abstract: The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT45M27S,2727,2d,hd,FALSE,https://i.ytimg.com/vi/pkkd-oJ-kc0/maxresdefault.jpg,,299,1,0,0,0
315,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BTZEukgwOwE,2016-10-05T15:19:01Z,5/10/16 15:19,IoT Big Data Stream Mining (Part 2),"Authors: Latifur Khan, Department of Computer Science, Erik Jonsson School of Engineering & Computer Science, The University of Texas at Dallas João Gama, Laboratory of Artificial Intelligence and Decision Support, University of Porto Albert Bifet, Telecom ParisTech Abstract: The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT52M2S,3122,2d,hd,FALSE,https://i.ytimg.com/vi/BTZEukgwOwE/maxresdefault.jpg,,381,2,0,0,0
316,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,abw5Lb76nfw,2016-10-05T15:18:56Z,5/10/16 15:18,IoT Big Data Stream Mining (Part 1),"Authors: Latifur Khan, Department of Computer Science, Erik Jonsson School of Engineering & Computer Science, The University of Texas at Dallas João Gama, Laboratory of Artificial Intelligence and Decision Support, University of Porto Albert Bifet, Telecom ParisTech Abstract: The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H14M59S,899,2d,hd,FALSE,https://i.ytimg.com/vi/abw5Lb76nfw/maxresdefault.jpg,,2053,9,0,0,0
317,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ErgHjxJsEKA,2016-10-05T15:18:50Z,5/10/16 15:18,Algorithmic Bias: From Discrimination Discovery to Fairness-Aware Data Mining (Part 3),"Authors: Carlos Castillo, EURECAT, Technology Centre of Catalonia Francesco Bonchi, ISI Foundation Abstract: Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT1H21M53S,1313,2d,hd,FALSE,https://i.ytimg.com/vi/ErgHjxJsEKA/maxresdefault.jpg,,897,7,0,0,1
318,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nKemhMbaYcU,2016-10-05T15:18:48Z,5/10/16 15:18,Algorithmic Bias: From Discrimination Discovery to Fairness-Aware Data Mining (Part 2),"Authors: Carlos Castillo, EURECAT, Technology Centre of Catalonia Francesco Bonchi, ISI Foundation Abstract: Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT1H14M46S,886,2d,hd,FALSE,https://i.ytimg.com/vi/nKemhMbaYcU/maxresdefault.jpg,,1149,9,0,0,0
319,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,mJcWrfoGup8,2016-10-05T15:17:36Z,5/10/16 15:17,Algorithmic Bias: From Discrimination Discovery to Fairness-Aware Data Mining (Part 1),"Authors: Carlos Castillo, EURECAT, Technology Centre of Catalonia Francesco Bonchi, ISI Foundation Abstract: Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research. More on http://www.kdd.org/kdd2016/ KDD2016 conference is published on http://videolectures.net/",22,People & Blogs,PT35M12S,2112,2d,hd,FALSE,https://i.ytimg.com/vi/mJcWrfoGup8/maxresdefault.jpg,,2189,16,0,0,0
320,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,sXT3FVnZzb8,2016-10-05T13:51:13Z,5/10/16 13:51,KDD 2016 Award Ceremony,"Presenters: Bing Liu, Department of Computer Science, University of Illinois at Chicago Charu Aggarwal, IBM Thomas J. Watson Research Center Alexander J. Smola, Machine Learning Department, Carnegie Mellon University Rastogi Rajeev, Amazon Dou Shen, Baidu, Inc. Usama Fayyad, Barclays Bank PLC. Rajesh Parekh, Facebook Evangelos Simoudis, Synapse Partners Faisal Farooq, IBM Thomas J. Watson Research Center Ron Bekkerman, University of Haifa Markus Weimer, Microsoft Vijay Narayanan, Microsoft Jiawei Han, Department of Computer Science, University of Illinois at Urbana-Champaign Wei Wang, Computer Science Department, University of California, Los Angeles, UCLA Jian Pei, School of Computing Science, Simon Fraser University More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H7M6S,426,2d,hd,FALSE,https://i.ytimg.com/vi/sXT3FVnZzb8/maxresdefault.jpg,,628,2,0,0,0
321,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,uVCkT8nh-YI,2016-10-05T13:48:34Z,5/10/16 13:48,A Big Data Perspective (ACM SIGKDD 2016 Innovation Award),"Author: Philip S. Yu, Department of Computer Science, College of Engineering, University of Illinois at Chicago More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",28,Science & Technology,PT24M34S,1474,2d,hd,FALSE,https://i.ytimg.com/vi/uVCkT8nh-YI/maxresdefault.jpg,,539,4,0,0,1
322,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,EkluJMh8wu4,2016-10-05T13:46:34Z,5/10/16 13:46,KDD 2016 Opening Ceremony,"Organizers: Shipeng Yu, LinkedIn Corporation Mohak Shah, Bosch Research and Technology Center North America Balaji Krishnapuram, IBM More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",28,Science & Technology,PT16M14S,974,2d,hd,FALSE,https://i.ytimg.com/vi/EkluJMh8wu4/maxresdefault.jpg,,2659,1,0,0,0
323,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,KKNoyWmbK1k,2016-09-05T14:06:35Z,5/9/16 14:06,Large Scale Machine Learning at Verizon: Theory and Applications,"Author: Jeff Stribling, Verizon Communications Abstract: This talk will cover recent innovations in large-scale machine learning and their applications on massive, real-world data sets at Verizon. These applications power new revenue generating products and services for the company and are hosted on a massive computing and storage platform known as Orion. We will discuss the architecture of Orion and the underlying algorithmic framework. We will also cover some of the real-world aspects of building a new organization dedicated to creating new product lines based on data science. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT34M56S,2096,2d,hd,FALSE,https://i.ytimg.com/vi/KKNoyWmbK1k/maxresdefault.jpg,,1188,8,0,0,2
324,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9tLBodPBciM,2016-09-05T14:05:43Z,5/9/16 14:05,Computational Social Science: Exciting Progress and Future Challenges,"Author: Duncan Watts, Microsoft Research Abstract: The past 15 years have witnessed a remarkable increase in both the scale and scope of social and behavioral data available to researchers, leading some to herald the emergence of a new field: “computational social science.” Against these exciting developments stands a stubborn fact: that in spite of many thousands of published papers, there has been surprisingly little progress on the “big” questions that motivated the field in the first place—questions concerning systemic risk in financial systems, problem solving in complex organizations, and the dynamics of epidemics or social movements, among others. In this talk I highlight some examples of research that would not have been possible just a handful of years ago and that illustrate the promise of CSS. At the same time, they illustrate its limitations. I then conclude with some thoughts on how CSS can bridge the gap between its current state and its potential. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT39M37S,2377,2d,hd,FALSE,https://i.ytimg.com/vi/9tLBodPBciM/maxresdefault.jpg,,1746,19,0,0,2
325,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ya_8uLta15s,2016-09-05T14:03:28Z,5/9/16 14:03,Can You Teach The Elephant To Dance? AKA: Culture Eats Data Science for Breakfast,"Author: Jonathan Becher, SAP SE Abstract: In the past 20 years, the practical examples of KDD/data mining have become so ubiquitous that it’s almost impossible to imagine a new venture that isn’t based on data science. Uber, Facebook, 23andMe, Tesla – they aren’t just technology companies; they are data companies. And yet the reality is that these companies are still anomalies. Large, successful companies usually still treat KDD as either an afterthought or as an experiment. It’s not core to how they run the business. As practitioners we compound this problem by concentrating our efforts on valuable business problems; but ones which are usually on the periphery of the business. We do this because changing the heart of how a company operates requires more than just process or technology changes. It requires cultural changes. And these cultural changes usually trigger corporate antibodies adverse to anything new. This talk will review some practical realities of instituting data-driven decisions in a very large multi-national company. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT38M13S,2293,2d,hd,FALSE,https://i.ytimg.com/vi/Ya_8uLta15s/maxresdefault.jpg,,415,8,0,0,0
326,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,MBPH7dNb2AA,2016-09-05T13:54:07Z,5/9/16 13:54,Democratizing Consumer Identity: Data Science’s Answer to Facebook,"Authors: Devin Guan, Drawbridge, Inc. Randell Cotta, Drawbridge, Inc. Abstract: With today’s massive global adoption of personal computers, smartphones, tablets, and the emerging class of devices, we live in a time of unprecedented device proliferation. This gives rise to very fundamental challenges around the fragmentation of consumer identity. The big Internet giants of Google, Facebook, etc. have an inherent advantage in solving for this through their first-party data of made up of a stateful, logged-in base of user information. This creates an inequity across the digital and Internet economy in the ability to provide seamless experiences and solutions from understanding consumer identity. This talk will address how machine learning and data science best-practices can solve for this identity capability, and how it can be done in a privacy-safe and data-safe manner, with high precision and at massive scale. The talk focuses on the algorithms developed at Drawbridge that process a massive scales of data in near-real-time spans to solve for a single user’s identity across different domains, while still protecting consumer anonymity. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M54S,1494,2d,hd,FALSE,https://i.ytimg.com/vi/MBPH7dNb2AA/maxresdefault.jpg,,128,0,0,0,0
327,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,-7TUaJPhRuU,2016-09-05T13:52:05Z,5/9/16 13:52,It’s About Time,"Author: Caitlin Smallwood, Netflix, Inc. Abstract: This talk will explore how time - and timing - impact Netflix’s data science applications. Netflix enables stories from around the world to be discovered, enjoyed, and shared globally. Success in this space means finding and creating the right movies and TV shows, connecting them with the cultures and people who will enjoy them, and delivering a high-quality video streaming experience. As we lean on data science to help in these areas, the specific manner in which time is handled can impact results dramatically. I will share examples of how Netflix incorporates time in algorithm training, feature engineering, A/B test execution, and measurement of member behaviors. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT39M58S,2398,2d,hd,FALSE,https://i.ytimg.com/vi/-7TUaJPhRuU/maxresdefault.jpg,,364,3,0,0,0
328,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,OYJNCE7-2uc,2016-09-05T13:46:05Z,5/9/16 13:46,Accelerating the Race to Autonomous Cars,"Author: Danny Shapiro, NVIDIA Corporation Abstract: Every automaker is working on driver assistance systems and self-driving cars. Conventional computer vision used for ADAS is reaching its threshold because it is impossible to write code for every possible scenario as a vehicle navigates. In order to develop a truly autonomous car, deep learning and artificial intelligence are required. With deep learning, the vehicle can be trained to have super human levels of perception, driving safer than anyone on the road. An end-to-end artificial intelligence platform based on supercomputers in the cloud and in the vehicle enables cars to get smarter and smarter. Coupled with an extensive software development kit with vision and AI libraries and software modules, automakers, tier 1s, and startups can build scalable systems from ADAS to full autonomy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT38M56S,2336,2d,hd,FALSE,https://i.ytimg.com/vi/OYJNCE7-2uc/maxresdefault.jpg,,550,5,0,0,0
329,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,neQL9rWD2bw,2016-09-05T13:44:50Z,5/9/16 13:44,Bayesian Optimization and Embedded Learning Systems,"Author: Jeff Schneider, School of Computer Science, Carnegie Mellon University Abstract: An important property of embedded learning systems is the ever-changing environment they create for all algorithms operating in the system. Optimizing the performance of those algorithms becomes a perpetual on-line activity rather than a one-off task. I will review some of these challenges in autonomous vehicles. I will discuss Bayesian optimization methods and their application in robotics and scientific applications, focusing on scaling up the dimensionality and managing multi-fidelity evaluations. I will finish with lessons learned and thoughts on future directions as these methods move into embedded systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT32M53S,1973,2d,hd,FALSE,https://i.ytimg.com/vi/neQL9rWD2bw/maxresdefault.jpg,,1167,9,1,0,1
330,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UaU3j-lrTeE,2016-09-05T13:43:53Z,5/9/16 13:43,Big Data Tools and Solutions: The Myths and the Reality,"Moderator: Usama Fayyad, Oasis500 Panelists: Richard Rovner, MathWorks, Inc. Ingo Mierswa, Rapid-I GmbH Dan Steinberg, Salford Systems Udo Sglavo, SAS Institute Inc. Abstract: The panel is intended to address a fundamental issue - What roles are the commercial data mining tools having in enabling Data Science? In a world, where most new Data Science is utilizing open source R as the primary tools library, and where in large enterprises analysis efforts are heavily reliant on established classical tools like SAS and MathWorks/MatLab, a question poses itself: Have there been recent changes or successes enabled by new tools? Are tools essential? Do we have the right tools for the modern #BigData world? More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT58M33S,3513,2d,hd,FALSE,https://i.ytimg.com/vi/UaU3j-lrTeE/maxresdefault.jpg,,250,2,0,0,0
331,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,J646Z4nFBVE,2016-09-05T13:43:37Z,5/9/16 13:43,Big Data Needs Big Dreamers: Lessons from successful Big Data investors,"Moderator: Evangelos Simoudis, Synapse Partners Panelists: Greg Sands, Costanoa Venture Capital Matt Ocko, DCVC Management Co, LLC. Tim Guleri, Sierra Ventures Mark Gorenberg, Zetta Venture Partners Abstract: The potential of big data and its exploitation using machine intelligence has captured investor imagination in the same way it continues to capture the imagination of consumers and business executives. As a result, hundreds of startups have already been funded by institutional investors and many more have received funding from other sources, including corporations. New opportunities for startups that provide solutions to important consumer and enterprise problems continue to emerge, and institutional investors remain eager to support great entrepreneurs ready to tackle them. Like in every field that attracts strong investor attention, while we are seeing many ideas that may not warrant institutional investment, we are also seeing some really great venture investment opportunities that have VCs trampling over themselves to invest in them. BigData Needs Big Dreamers: Lessons from BigData Investors panel brings together seasoned investors from the Venture Capital (VC) community who have invested in BigData companies and will share with us their impression of this space from their investor perspective. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H2M35S,155,2d,hd,FALSE,https://i.ytimg.com/vi/J646Z4nFBVE/maxresdefault.jpg,,220,0,0,0,0
332,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,g4j7laPzprk,2016-09-05T13:41:08Z,5/9/16 13:41,The Dirty Little Secret of Enterprise Data,"Author: Andy Palmer, Tamr, Inc. Abstract: The time has come to manage information across the enterprise as a strategic asset and deliver to enterprise data consumers, experiences that rival their experiences managing information on the public web. People who use information technology at work are the same people that go home and use the Internet. Why should their information systems at work be harder to use—or less satisfying—than the systems they use at home? In this presentation, serial entrepreneur and Tamr Co-Founder Andy Palmer exposes the dirty secret of messy, siloed, enterprise data. Pulling from first hand experience, Palmer shares how Fortune 500 companies are saving $100s of millions by automatically unifying and preparing enterprise data at scale—and empowering enterprise data and IT professionals to become the “Googlers” of their enterprise. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT37M12S,2232,2d,hd,FALSE,https://i.ytimg.com/vi/g4j7laPzprk/maxresdefault.jpg,,194,2,0,0,0
333,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Kb3d1O1WITs,2016-09-05T13:41:01Z,5/9/16 13:41,The Wisdom of Crowds: Best Practices for Data Prep & Machine Learning,"Author: Ingo Mierswa, Rapid-I GmbH Abstract: With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level? More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT38M21S,2301,2d,hd,FALSE,https://i.ytimg.com/vi/Kb3d1O1WITs/maxresdefault.jpg,,843,8,0,0,1
334,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,WkHBm13J_0g,2016-09-05T13:40:39Z,5/9/16 13:40,Learning Sparse Models at Scale,"Author: Ralf Herbrich, Amazon Abstract: Recently, learning deep models from dense data has received a lot of attention in tasks such as object recognition and signal processing. However, when dealing with non-sensory data about real-world entities, data is often sparse; for example people interaction with products in e-Commerce, people interacting with each other in social networks or word sequences in natural language. In this talk, I will share lessons learned over the past 10 years when learning predictive models based on sparse data: 1) how to scale the inference algorithms to distributed data setting, 2) how to automate the learning process by reducing the amount of hyper-parameters to zero, 3) how to deal with Zipf distributions when learning resource-constrained models, and 4) how to combine dense and sparse-learning algorithms. The talk will be drawing from many real-world experiences I gathered over the past decade in applications of the techniques in gaming, search, advertising and recommendations of systems developed at Microsoft, Facebook and Amazon. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT37M5S,2225,2d,hd,FALSE,https://i.ytimg.com/vi/WkHBm13J_0g/maxresdefault.jpg,,372,1,0,0,0
335,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,R-Vf1ud1D9M,2016-09-05T13:36:15Z,5/9/16 13:36,How Machine Learning has Finally Solved Wanamaker’s Dilemma,"Author: Oliver Downs, Analytical Insights, Inc. Abstract: We are all familiar with John Wanamaker’s famous quote: “Half the money I spend on advertising is wasted; the trouble is I don’t know which half.” What’s ironic is that Wanamaker’s dilemma is as much the marketer’s dilemma today as it was in the 1900’s. In an increasingly sophisticated marketing environment, where new marketing tools have enabled more precise targeting and marketing measurement than ever before, the challenge of solving Wanamaker’s dilemma has still loomed – that is, until now. In this talk, I will show you how at Amplero we have solved what’s not humanly possible to solve, making available to marketers an AI-powered marketing technology that makes use of dynamic machine learning at massive scale to enable smarter, more effective marketing. In particular, I will focus on the revolution of multi-armed bandit experimentation coupled with machine learning, highlighting what it means for today’s marketer, and specifically the marketer’s ability to directly impact business KPIs and customer lifetime value. As part of my presentation I will share experiences and examples from our work with BtoC enterprises with 10M’s of customers. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT24M40S,1480,2d,hd,FALSE,https://i.ytimg.com/vi/R-Vf1ud1D9M/maxresdefault.jpg,,571,4,1,0,0
336,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,SOGbe2f-Fj8,2016-09-01T08:53:45Z,1/9/16 8:53,A VC View of Investing in ML,"Author: Greg Papadopoulos, NEA Abstract: We are seeing a remarkable watershed in the application of data science across markets and industries. A trifecta of advances in algorithms, cheap cycles, and the capture of networked data from everywhere are no doubt the catalysts. The results for many are continuous improvements in efficiencies, and for some are a fundamental re-imagination and disruption of just about every industry. This talk will give examples we are seeing (and funding!) for the latter, and then focus on our views of the ecosystem of value-from-data infrastructure and end-application companies. A big question is whether the enormous collective advances in tools, techniques and education are in-fact converting would-be differentiated products into democratized features used everywhere. We’ll follow the value and make our own predictions on future as ML as a business. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H19M27S,1167,2d,hd,FALSE,https://i.ytimg.com/vi/SOGbe2f-Fj8/maxresdefault.jpg,,1355,13,0,0,0
337,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,furfdqtdAvc,2016-09-01T07:43:37Z,1/9/16 7:43,Plenary Panel: Is Deep Learning the New 42?,"Authors: moderator: Andrei Broder, Yahoo! Research panelist: Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington panelist: Nando de Freitas, Department of Computer Science, University of Oxford panelist: Isabelle Guyon, Clopinet panelist: Jitendra Malik, UC Berkeley panelist: Jennifer Neville, Computer Science Department, Purdue University Abstract: The history of deep learning goes back more than five decades but in the marketplace of ideas its perceived value went through booms and busts. We are no doubt at an all time high: in the last couple of years we witnessed extraordinary advances in vision, speech recognition, game playing, translation, and so on, all powered by deep networks. At the same time companies such as Amazon, Apple, Facebook, Google, and Microsoft are making huge bets on deep learning research and infrastructure, ML competitions are dominated by deep learning approaches, open source deep learning software is proliferating, and the popular press both cheerleads the progress and raises the dark specter of unintended consequences. So is deep learning the answer to everything? According to Douglas Adams’s famous “Hitchhiker’s Guide to the Galaxy” after 7.5 millions years of work the “Deep Thought” computer categorically found out that 42 is the “Answer to the Ultimate Question of Life, the Universe, and Everything” (although unfortunately, no one knows exactly what that question was). Rather than wait another 7.5 million years for “Deep Thought” to answer our quest we have assembled a distinguished panel of experts to give us their opinion on deep learning and its present and future impact. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H46M20S,2780,2d,hd,FALSE,https://i.ytimg.com/vi/furfdqtdAvc/maxresdefault.jpg,,18443,250,4,0,21
338,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,30hj43gNnVU,2016-09-01T07:41:33Z,1/9/16 7:41,"People, Computers, and The Hot Mess of Real Data","Author: Joseph M. Hellerstein, Department of Electrical Engineering and Computer Sciences, UC Berkeley Abstract: In practice, end-to-end data analysis is rarely a cleanly engineered process. Acquiring data can be tricky. Data assessment, wrangling and feature extraction are time-consuming and subjective. Models and algorithms used to derive data products are highly contextualized by time-varying properties of data sources, code and application needs. All of these issues would ideally benefit from an organizational view, but are often driven by individual users. Viewed holistically, both agile analytics and the establishment of analytic pipelines involve interactions between people, computation and infrastructure. In this talk I’ll share some anecdotes from our research, user studies, and field experience with companies (Trifacta, Captricity), as well as an emerging open-source project (Ground). More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H3M57S,237,2d,hd,FALSE,https://i.ytimg.com/vi/30hj43gNnVU/maxresdefault.jpg,,960,8,0,0,0
339,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,CIZh0CHXGC4,2016-09-01T07:39:26Z,1/9/16 7:39,The Evolving Meaning of Information Security,"Author: Whitfield Diffie, Center for International Security and Cooperation, Stanford University Abstract: When you are developing security systems, new penetration techniques seem to appear as responses to new security measures but in general the flow is the other way around: security exists and evolves because of the evolution of threats. Beginning with the rise of radio in the 20th Century attacks on communication networks have shown two forms: those that go for the big kill—- such as the breaking of Enigma—- and those that assemble small seemingly innocuous leaks of information into a comprehensive understanding of the target’s behavior. We will analyze the way in which these trends interact with others to create a situation in which what is possible in security and even the meaning of security in communication networks needs reexamination. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H29M27S,1767,2d,hd,FALSE,https://i.ytimg.com/vi/CIZh0CHXGC4/maxresdefault.jpg,,866,6,1,0,0
340,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,x1kf4Zojtb0,2016-09-01T07:36:47Z,1/9/16 7:36,Learning to learn and compositionality with deep recurrent neural networks,"Author: Nando de Freitas, Department of Computer Science, University of Oxford Abstract: Deep neural network representations play an important role in computer vision, speech, computational linguistics, robotics, reinforcement learning and many other data-rich domains. In this talk I will show that learning-to-learn and compositionality are key ingredients for dealing with knowledge transfer so as to solve a wide range of tasks, for dealing with small-data regimes, and for continual learning. I will demonstrate this with three examples: learning learning algorithms, neural programmers and interpreters, and learning communication. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H23M45S,1425,2d,hd,FALSE,https://i.ytimg.com/vi/x1kf4Zojtb0/maxresdefault.jpg,,15609,244,4,0,8
341,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9tny8lbt25A,2016-09-01T07:33:23Z,1/9/16 7:33,Graphons and Machine Learning: Modeling and Estimation of Sparse Massive Networks,"Author: Jennifer Chayes, Microsoft Research Abstract: There are numerous examples of sparse massive networks, in particular the Internet, WWW and online social networks. How do we model and learn these networks? In contrast to conventional learning problems, where we have many independent samples, it is often the case for these networks that we can get only one independent sample. How do we use a single snapshot today to learn a model for the network, and therefore be able to predict a similar, but larger network in the future? In the case of relatively small or moderately sized networks, it’s appropriate to model the network parametrically, and attempt to learn these parameters. For massive networks, a non-parametric representation is more appropriate. In this talk, we first review the theory of graphons, developed over the last decade to describe limits of dense graphs, and the more the recent theory describing sparse graphs of unbounded average degree, including power-law graphs. We then show how to use these graphons as non-parametric models for sparse networks. Finally, we show how to get consistent estimators of these non-parametric models, and moreover how to do this in a way that protects the privacy of individuals on the network. More on http://www.kdd.org/kdd2016/ KDD2016 Conference is published on http://videolectures.net/",22,People & Blogs,PT1H12M17S,737,2d,hd,FALSE,https://i.ytimg.com/vi/9tny8lbt25A/maxresdefault.jpg,,3226,33,1,0,2
342,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UngpFp2T9hE,2016-08-16T10:02:58Z,16/8/16 10:02,KDD2016 paper 839,"Title: Parallel Lasso Screening for Big Data Optimization Authors: Qingyang Li*, Arizona State University Shuang Qiu, University of Michigan at Ann Arbor Shuiwang Ji, Washington State University Paul M. Thompson, University of Southern California Jieping Ye, University of Michigan at Ann Arbor Jie Wang, University of Michigan at Ann Arbor Abstract: Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M22S,142,2d,hd,FALSE,https://i.ytimg.com/vi/UngpFp2T9hE/maxresdefault.jpg,,277,2,0,0,0
343,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ApSausNaC7s,2016-08-16T10:02:54Z,16/8/16 10:02,KDD2016 paper 913,"Title: Designing Policy Recommendations to Reduce Home Abandonment in Mexico Authors: Klaus Ackermann, Monash University Eduardo Blancas Reyes*, The University of Chicago Sue He, University of Virginia Thomas Anderson Keller, UC San Diego Paul van der Boor, Data Science for Social Good Romana Khan, Data Science for Social Good Rayid Ghani, University of Chicago Abstract: Infonavit, the largest provider of mortgages in Mexico, assists working families to obtain low-interest rate housing solutions. An increasingly prevalent problem is home abandonment: when a homeowner decides to leave their property and forego their investment. A major causal factor of this outcome is a mismatch between the homeowner’s needs, in terms of access to services and employment, and the location characteristics of the home. This paper describes our collaboration with Infonavit to reduce home abandonment at two levels: develop policy recommendations for targeted improvements in location characteristics, and develop a decision-support tool to assist the homeowner in the home location decision. Using 20 years of mortgage history data combined with surveys, census, and location information, we develop a model to predict the probability of home abandonment based on both individual and location characteristics. The model is used to develop a tool that provides Infonavit the ability to give ad-vice to Mexican workers when they apply for a loan, evaluate and improve the locations of new housing developments, and provide data-driven recommendations to the federal government to inﬂuence local development initiatives and infrastructure investments. The result is improving economic out-comes for the citizens of Mexico by pre-emptively identifying at-risk home mortgages, thereby allowing them to be altered or remedied before they result in abandonment. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M27S,207,2d,hd,FALSE,https://i.ytimg.com/vi/ApSausNaC7s/maxresdefault.jpg,,421,0,0,0,0
344,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pzgqd9OhvDA,2016-07-20T10:35:44Z,20/7/16 10:35,KDD2016 paper 1036,"Title: Gemello: Creating a Detailed Energy Breakdown from just the Monthly Electricity Bill Authors: Nipun Batra*, Indraprastha Institute of Information Technology, Delhi Amarjeet Singh, Indraprastha Institute of Information Technology, Delhi Kamin Whitehouse, University of Virginia Abstract: The ﬁrst step to saving energy in the home is often to create an energy breakdown: the amount of energy used by each individual appliance in the home. Unfortunately, current techniques that produce an energy breakdown are not scalable: they require hardware to be installed in each and every home. In this paper, we propose a more scalable solution called Gemello that estimates the energy breakdown for one home by matching it with similar homes for which the breakdown is already known. This matching requires only the monthly energy bill and household characteristics such as square footage of the home and the size of the household. We evaluate this approach using 57 homes and results indicate that the accuracy of Gemello is comparable to or better than existing techniques that use sensing infrastructure in each home. The information required by Gemello is often publicly available and, as such, it can be immediately applied to many homes around the world. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M10S,130,2d,hd,FALSE,https://i.ytimg.com/vi/pzgqd9OhvDA/maxresdefault.jpg,,8556,333,15,0,19
345,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ndny26wZmO0,2016-07-20T10:33:06Z,20/7/16 10:33,KDD2016 paper 585,"Title: Overcoming key weaknesses of Distance-based Neighbourhood Methods using a Data Dependent Dissimilarity Authors: Kai Ming Ting*, Federation University Ye Zhu, Monash University Mark Carman, Monash University Yue Zhu, Nanjing University Zhi-Hua Zhou, Nanjing University Abstract: This paper introduces the ﬁrst generic version of data dependent dissimilarity and shows that it provides a better closest match than distance measures for three existing algorithms in clustering, anomaly detection and multi-label classiﬁcation. For each algorithm, we show that by simply replacing the distance measure with the data dependent dissimilarity measure, it overcomes a key weakness of the otherwise unchanged algorithm. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M55S,175,2d,hd,FALSE,https://i.ytimg.com/vi/ndny26wZmO0/maxresdefault.jpg,,613,4,0,0,0
346,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tNcV2i4YWGs,2016-07-15T06:24:08Z,15/7/16 6:24,KDD2016 paper 890,"Title: Just One More: Modeling Binge Watching Behavior Authors: William Trouleau*, EPFL Azin Ashkan, Technicolor Research Weicong Ding, Technicolor Research Brian Eriksson, Technicolor Research Abstract: Easy accessibility can often lead to over-consumption, as seen in food and alcohol habits. On video on-demand (VOD) services, this has recently been referred to as “binge watching”, where potentially entire seasons of TV shows are consumed in a single viewing session. While a user viewership model may reveal this binging behavior, creating an accurate model has several challenges, including censored data, deviations in the population, and the need to consider external influences on consumption habits. In this paper, we introduce a novel statistical mixture model that incorporates these factors and presents a “first of its kind” characterization of viewer consumption behavior using a real-world dataset that includes playback data from a VOD service. From our modeling, we tackle various predictive tasks to infer the consumption decisions of a user in a viewing session, including estimating the number of episodes they watch and classifying if they continue watching another episode. Using these insights, we then identify binge watching sessions based on deviation from normal viewing behavior. We observe different types of binging behavior, that binge watchers often view certain content out-of-order, and that binge watching is not a consistent behavior among our users. These insights and our findings have application in VOD revenue generation, consumer health applications, and customer retention analysis. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M46S,166,2d,hd,FALSE,https://i.ytimg.com/vi/tNcV2i4YWGs/maxresdefault.jpg,,577,7,0,0,0
347,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,CPClQEYDU34,2016-07-13T06:22:40Z,13/7/16 6:22,KDD2016 paper 224,"Title: Convex Optimization for Linear Query Processing under Approximate Differential Privacy Authors: Ganzhao Yuan*, South China University of Technology Yin Yang, South China University of Technology Zhenjie Zhang, South China University of Technology Zhifeng Hao, South China University of Technology Abstract: Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We further propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). The usefulness of explanations is shown via novel experiments, both simulated and with human subjects. Our explanations empower users in various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M31S,151,2d,sd,FALSE,,,283,1,0,0,1
348,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hUnRCxnydCc,2016-07-07T12:46:49Z,7/7/16 12:46,KDD2016 paper 573,"Title: ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier Authors: Marco Túlio Ribeiro*, University of Washington Sameer Singh, University of Washington Carlos Guestrin, University of Washington Abstract: Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We further propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). The usefulness of explanations is shown via novel experiments, both simulated and with human subjects. Our explanations empower users in various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M55S,175,2d,hd,FALSE,https://i.ytimg.com/vi/hUnRCxnydCc/maxresdefault.jpg,,75824,679,15,0,19
349,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,DrAc82oGSJ8,2016-07-06T11:45:08Z,6/7/16 11:45,KDD2016 paper 291,"Title: Compute Job Memory Recommender System Using Machine Learning Authors: Taraneh Taghavi*, Qualcomm Inc. Maria Lupetini, Qualcomm Inc. Yaron Kretchmer, Qualcomm Inc. Abstract: This paper presents a machine learning approach to predict the amount of compute memory needed by jobs which are submitted to Load Sharing Facility (LSF®) with a high level of accuracy. LSF® is the compute resource manager and job scheduler for Qualcomm chip design process. It schedules the jobs based on available resources: CPU, memory, storage, and software licenses. Memory is one of the key resources and its proper utilization leads to a substantial improvement in saving machine resources which in turn results in a significant reduction in overall job pending time. In addition, efficient memory utilization helps to reduce the operations cost by decreasing the number of servers needed for the end-to-end design process. In this paper, we explored a suite of statistical and machine learning techniques to develop a Compute Memory Recommender System for the Qualcomm chip design process with over 90% accuracy in predicting the amount of memory a job needs. Moreover, it demonstrates the potential to significantly reduce job pending time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M12S,132,2d,hd,FALSE,https://i.ytimg.com/vi/DrAc82oGSJ8/maxresdefault.jpg,,577,11,1,0,0
350,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TSzc3pi7AsU,2016-07-06T11:45:04Z,6/7/16 11:45,KDD2016 paper 645,"Title: Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams Authors: Hesam Amoualian*, University Grenoble Alps Marianne Clausel, University of Grenoble Alps Eric Gaussier, University of Grenoble Alps Massih-Reza Amini, University of Grenoble Alps Abstract: We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal LDA), both in terms of perplexity and for tracking similar topics in a document stream. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M19S,139,2d,hd,FALSE,https://i.ytimg.com/vi/TSzc3pi7AsU/maxresdefault.jpg,,655,0,1,0,0
351,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gk2LnrDu_wE,2016-07-04T15:28:06Z,4/7/16 15:28,KDD2016 paper 780,"Title: Aircraft Trajectory Prediction Made Easy with Predictive Analytics Authors: Samet Ayhan*, University of Maryland Hanan Samet, University of Maryland Abstract: At the heart of Air Traffic Management (ATM) lies the Decision Support Systems (DST) that rely upon accurate trajectory prediction to determine how the airspace will look like in the future to make better decisions and advisories. Dealing with airspace that is prone to congestion due to environmental factors still remains the challenge especially when a deterministic approach is used in the trajectory prediction process. In this paper, we describe a novel stochastic trajectory prediction approach for ATM that can be used for more efficient and realistic flight planning and to assist airspace flow management, potentially resulting in higher safety, capacity, and efficiency commensurate with fuel savings thereby reducing emissions for a better environment. Our approach considers airspace as a 3D grid network, where each grid point is a location of a weather observation. We hypothetically build cubes around these grid points, so the entire airspace can be considered as a set of cubes. Each cube is defined by its centroid, the original grid point, and associated weather parameters that remain homogeneous within the cube during a period of time. Then, we align raw trajectories to a set of cube centroids which are basically fixed 3D positions independent of trajectory data. This creates a new form of trajectories which are 4D joint cubes, where each cube is a segment that is associated with not only spatio-temporal attributes but also with weather parameters. Next, we exploit machine learning techniques to train inference models from historical data and apply a stochastic model, a Hidden Markov Model (HMM), to predict trajectories taking environmental uncertainties into account. Our experiments use a real trajectory dataset with pertaining weather observations and demonstrate the effectiveness of our approach to the trajectory prediction process for ATM. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M4S,184,2d,hd,FALSE,https://i.ytimg.com/vi/gk2LnrDu_wE/maxresdefault.jpg,,394,0,0,0,0
352,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UN3cn9WU-hc,2016-07-04T10:06:27Z,4/7/16 10:06,KDD2016 paper 351,"Title: Transferring Knowledge between Cities: A Perspective of Multimodal Data and A Case Study in Air Qual Authors: Ying Wei*, Hong Kong University of Science and Technology Yu Zheng, Microsoft Research Qiang Yang, Hong Kong University of Science and Technology Abstract: The rapid urbanization has motivated extensive research on urban computing. It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by different sources in urban spaces, such as vehicles and humans. However, we are more likely to encounter the label scarcity problem and the data insufficiency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built. In this paper, we propose a FLexible multimOdal tRAnsfer Learning (FLORAL) method to transfer knowledge from a city where there exist sufficient multimodal data and labels, to this kind of cities to fully alleviate the two problems. FLORAL learns semantically related dictionaries for multiple modalities from a source domain, and simultaneously transfers the dictionaries and labelled instances from the source into a target domain. We evaluate the proposed method with a case study of air quality prediction. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M49S,169,2d,sd,FALSE,,,599,22,0,0,0
353,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,swSZe_uTRUM,2016-07-04T10:06:24Z,4/7/16 10:06,KDD2016 paper 16,"Title: Continuous Experience-aware Language Model Authors: Subhabrata Mukherjee*, Max Planck Institute for Informatics Stephan Günnemann, Technical University of Munich Gerhard Weikum, Max Planck Institute for Informatics Abstract: Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also outperforms state-of-the-art methods for predicting item ratings. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M41S,161,2d,hd,FALSE,https://i.ytimg.com/vi/swSZe_uTRUM/maxresdefault.jpg,,455,3,0,0,0
354,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,bezkUnBBwIw,2016-07-04T02:19:41Z,4/7/16 2:19,KDD2016 paper 454,"Title: QUINT: On Query-Specific Optimal Networks Authors: Liangyue Li*, Arizona State University Yuan Yao, Nanjing University Jie Tang, Tsinghua University Wei Fan, Baidu Hanghang Tong, Arizona State University Abstract: Measuring node proximity on large scale networks is a fundamental building block in many application domains, ranging from computer vision, e-commerce, social networks, software engineering, disaster management to biology and epidemiology. The state of the art (e.g., random walk based methods) typically assumes the input network is given a prior, with the known network topology and the associated edge weights. A few recent works aim to further infer the optimal edge weights based on the side information. This paper generalizes the challenge in multiple dimensions, aiming to learn optimal networks for node proximity measures. First (optimization scope), our proposed formulation explores a much larger parameter space, so that it is able to simultaneously infer the optimal network topology and the associated edge weights. This is important as a noisy or missing edge could greatly mislead the network node proximity measures. Second (optimization granularity), while all the existing works assume one common optimal network, be it given as the input or learned by the algorithms, exists for all queries, our method performs optimization at a much finer granularity, essentially being able to infer an optimal network that is specific to a given query. Third (optimization efficiency), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user preference set. We perform extensive empirical evaluations on a diverse set of 10+ real networks, which show that the proposed algorithms (1) consistently outperform the existing methods on all six commonly used metrics; (2) empirically scale sub-linearly to billion-scale networks and (3) respond in a fraction of a second. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/bezkUnBBwIw/maxresdefault.jpg,,420,0,0,0,0
355,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,diNo9-c0B38,2016-07-01T15:44:29Z,1/7/16 15:44,KDD2016 paper 247,"Title: Lightweight Monitoring of Distributed Streams Authors: Daniel Keren*, University of Haifa Assaf Schuster, Technion Arnon Lazerson, Israeli Institute of technology Abstract: As data becomes dynamic, large, and distributed, there is increasing demand for what have become known as distributed stream algorithms. Since continuously collecting the data to a central server and processing it there incurs very high communication and computation complexities, it is advantageous to define local conditions at the nodes, such that—as long as they are maintained—some desirable global condition holds. A generic algorithm which proved very useful for reducing communication in distributed streaming environments is geometric monitoring (GM). Alas, applying GM to many important tasks is computationally very demanding, as it requires solving a notoriously difficult problem—computing the distance between a point and a surface, which is often very time-consuming even in low dimensions. Thus, while useful for reducing communication, GM often suffers from exceedingly heavy computational burden at the nodes, which renders it very problematic to apply, especially for “thin”, battery-operated sensors, which are prevalent in numerous applications, including the “Internet of Things” paradigm. Here we propose a very different approach, designated CB (for Convex/Concave Bounds). CB is based on directly bounding the monitored function by suitably chosen convex and concave functions, that naturally enable monitoring distributed streams. These functions can be checked on the fly, yielding far simpler local conditions than those applied by GM. CB’s superiority over GM is demonstrated in reducing computational complexity, by several orders of magnitude in some cases. As an added bonus, CB also reduced communication overhead in all application scenarios we tested. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M26S,146,2d,hd,FALSE,https://i.ytimg.com/vi/diNo9-c0B38/maxresdefault.jpg,,160,0,0,0,0
356,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ptvv_ibyy8E,2016-07-01T15:43:24Z,1/7/16 15:43,KDD2016 paper 920,"Title: Reconstructing an Epidemic over Time Authors: Polina Rozenshtein, Aalto University Aristides Gionis*, Aalto University B. Aditya Prakash, Virginia Tech Jilles Vreeken, Max-Planck Institute for Informatics and Saarland University Abstract: We consider the problem of reconstructing an epidemic over time, or, more general, reconstructing the propagation of an activity in a network. Our input consists of a \emph{temporal network}, which contains information about when two nodes interacted, and a small sample of nodes that have been reported as infected. The goal is to recover the flow of the spread, including discovering the starting nodes, and identifying other likely-infected nodes that were not reported. This has multiple applications, from public health to social media and viral marketing purposes. Previous work explicitly factor-in many unrealistic assumptions: (a) the underlying network does not change or that we see all interactions; (b) that we have access to perfect noise-free data; or (c) that we know the exact propagation model. In contrast, we avoid these simplifications, and take into account the temporal network, require only a small sample of reported infections, and do not make any restrictive assumptions on the propagation model. We develop CulT, a scalable and effective algorithm to reconstruct epidemics that is also suited for an online setting. It works by formulating the problem as that of a temporal Steiner-tree computation, for which we design a fast algorithm leveraging the specific structure of our problem. We demonstrate the efficacy of CulT through extensive experiments on diverse datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M36S,156,2d,hd,FALSE,https://i.ytimg.com/vi/ptvv_ibyy8E/maxresdefault.jpg,,796,7,1,0,0
357,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xExD6KrnDwQ,2016-07-01T15:42:34Z,1/7/16 15:42,KDD2016 paper 12,"Title: Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data Authors: Payam Siyari*, Georgia Institute of Technology Bistra Dilkina, Georgia Institute of Technology\ Constantine Dovrolis, Georgia Institute of Technology Abstract: Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as “Lexis”, that produces an optimized hierarchical representation of a given set of “target” strings. The resulting hierarchy, “Lexis-DAG”, shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the “core” of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M36S,156,2d,hd,FALSE,https://i.ytimg.com/vi/xExD6KrnDwQ/maxresdefault.jpg,,2102,67,0,0,1
358,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ew8rYCyCD10,2016-06-30T17:20:38Z,30/6/16 17:20,KDD2016 paper 478,"Title: Text Mining in Clinical Domain: Dealing with Noise Authors: Hoang Nguyen*, National ICT Australia Jon Patrick, University of Sydney Abstract: Text mining in clinical domain is usually more difficult than general domains (e.g. newswire reports and scientific literature) because of the high level of noise in both the corpus and training data for machine learning (ML). A large number of unknown word, non-word and poor grammatical sentences made up the noise in the clinical corpus. Unknown words are usually complex medical vocabularies, misspellings, acronyms and abbreviations where unknown non-words are generally the clinical patterns including scores and measures. This noise produces obstacles in the initial lexical processing step as well as subsequent semantic analysis. Furthermore, the labelled data used to build ML models is very costly to obtain because it requires intensive clinical knowledge from the annotators. And even created by experts, the training examples usually contain errors and inconsistencies due to the variations in human annotators’ attentiveness. Clinical domain also suffers from the nature of the imbalanced data distribution problem. These kinds of noise are very popular and potentially affect the overall information extraction performance but they were not carefully investigated in most presented health informatics systems. This paper introduces a general clinical data mining architecture which is potential of addressing all of these challenges using: automatic proof-reading process, trainable finite state pattern recogniser, iterative model development and active learning. The reportability classifier based on this architecture achieved 98.25% sensitivity and 96.14% specificity on an Australian cancer registry’s held-out test set and up to 92% of training data provided for supervised ML was saved by active learning. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M3S,183,2d,hd,FALSE,https://i.ytimg.com/vi/Ew8rYCyCD10/maxresdefault.jpg,,306,2,0,0,0
359,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wixIKLGG2WA,2016-06-30T17:20:38Z,30/6/16 17:20,KDD2016 paper 482,"Title: Unified Point-of-Interest Recommendation with Temporal Interval Assessment Authors: Yanchi Liu*, Rutgers University Chuanren Liu, Drexel University Bin Liu, Rutgers University Meng Qu, Rutgers University Hui Xiong, Rutgers University Abstract: Point-of-interest (POI) recommendation, which helps mobile users explore new places, has become an important location-based service. Existing approaches for POI recommendation have been mainly focused on exploiting the information about user preferences, social influence, and geographical influence. However, these approaches cannot handle the scenario where users are expecting to have POI recommendation for a specific time period. To this end, in this paper, we propose a unified recommender system, named the ‘Where and When to gO’ (WWO) recommender system, to integrate the user interests and their evolving sequential preferences with temporal interval assessment. As a result, the WWO system can make recommendations dynamically for a specific time period and the traditional POI recommender system can be treated as the special case of the WWO system by setting this time period long enough. Specifically, to quantify users’ sequential preferences, we consider the distributions of the temporal intervals between dependent POIs in the historical check-in sequences. Then, to estimate the distributions with only sparse observations, we develop the low-rank graph construction model, which identifies a set of bi-weighted graph bases so as to learn the static user preferences and the dynamic sequential preferences in a coherent way. Moreover, we exploit mobile regularizations to incorporate heterogeneous human mobility data to boost the performances of the WWO recommendation. Finally, we evaluate the proposed approach using real-world data sets from several location-based social networks (LBSNs). The experimental results show that our method outperforms the state-of-the-art approaches for POI recommendation in terms of various metrics, such as F-measure and NDCG, with a significant margin. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M5S,125,2d,hd,FALSE,https://i.ytimg.com/vi/wixIKLGG2WA/maxresdefault.jpg,,379,0,0,0,0
360,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,6KU-vAWWdPs,2016-06-30T17:20:37Z,30/6/16 17:20,KDD2016 paper 469,"Title: From Truth Discovery to Trustworthy Opinion Discovery: An Uncertainty-Aware Quantitative Modeling Approach Authors: Mengting Wan*, UC San Diego Xiangyu Chen, University of Illinois, Urbana-Champaign Lance Kaplan, U.S. Army Research Laboratory Jiawei Han, University of Illinois at Urbana-Champaign Jing Gao, SUNY Buffalo Bo Zhao, LinkedIn Abstract: In this era of information explosion, conflicts are often encountered when information is provided by multiple sources. Traditional truth discovery task aims to identify the truth—the most trustworthy information, from conflicting sources in different scenarios. In this kind of tasks, truth is regarded as a fixed value or a set of fixed values. However, in a number of real-world cases, objective truth existence cannot be ensured and we can only identify single or multiple reliable facts from opinions. Different from traditional truth discovery task, we address this uncertainty and introduce the concept of trustworthy opinion of an entity, treat it as a random variable, and use its distribution to describe consistency or controversy, which is particularly difficult for data which can be numerically measured, i.e. quantitative information. In this study, we focus on the quantitative opinion, propose an uncertainty-aware approach called Kernel Density Estimation from Multiple Sources (KDEm) to estimate its probability distribution, and summarize trustworthy information based on this distribution. Experiments indicate that KDEm not only has outstanding performance on the classical numeric truth discovery task, but also shows good performance on multi-modality detection and anomaly detection in the uncertain-opinion setting. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M18S,138,2d,hd,FALSE,https://i.ytimg.com/vi/6KU-vAWWdPs/maxresdefault.jpg,,242,1,0,0,0
361,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,DVe7YCzmj2U,2016-06-30T17:20:37Z,30/6/16 17:20,KDD2016 paper 465,"Title: TRIÈST: Counting Local and Global Triangles in Fully-dynamicStreams with Fixed Memory Size Authors: Lorenzo De Stefani*, Brown University Alessandro Epasto, Brown University Matteo Riondato, Two Sigma Investments Eli Upfal, Brown University Abstract: We present Trièst, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches which use hard-to choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they will use. We show a full analysis of the variance of the estimations and novel concentration bounds for these quantities. Our experimental results on very large graphs show that Trièst outperforms state-of-the-art approaches in accuracy and exhibits a small update time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M11S,131,2d,hd,FALSE,https://i.ytimg.com/vi/DVe7YCzmj2U/maxresdefault.jpg,,446,2,0,0,0
362,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,31Ty1tYh1tw,2016-06-30T17:20:36Z,30/6/16 17:20,KDD2016 paper 461,"Title: Kam1n0: MapReduce-based Assembly Clone Search for Reverse Engineering Authors: Steven H. H. Ding, McGill University Benjamin C. M. Fung*, McGill University Philippe Charland, Defence Research and Development Canada Abstract: Assembly code analysis is one of the critical processes for detecting and justifying software plagiarism and software patent infringements when the source code is unavailable. It is also a common practice to discover exploits and vulnerabilities in existing software. However, it is a manually intensive and time-consuming process even for experienced reverse engineers. An effective and efficient assembly code clone search engine can greatly reduce the effort of this process, since it can identify the cloned parts that have been previously analyzed. The assembly code clone search problem belongs to the field of software engineering. However, it strongly depends on practical nearest neighbor search techniques in data mining and database. By closely collaborating with reverse engineers and Defence Research and Development Canada (DRDC), we study the concerns and challenges that make existing assembly code clone approaches not practically applicable from the perspective of data mining. We propose a new variant of LSH scheme and incorporate it with graph matching to address these challenges. We implement an integrated assembly clone search engine called Kam1n0. It is the first clone search engine that can efficiently identify the given query assembly function’s subgraph clones from a large assembly code repository. Kam1n0 is built upon the Apache Spark computation framework and Cassandra-like key-value distributed storage. The deployed system is publicly available and readers can try out its beta version on Google Cloud. Extensive experimental results suggest that Kam1n0 is accurate, efficient, and scalable for handling large volume of assembly code. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/31Ty1tYh1tw/maxresdefault.jpg,,3538,44,1,0,4
363,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4zh3VjsNwdI,2016-06-30T17:20:35Z,30/6/16 17:20,KDD2016 paper 450,"Title: CaSMoS: A Framework for Learning Candidate Selection Models over Structured Queries and Documents Authors: Fedor Borisyuk*, LinkedIn Corporation Krishnaram Kenthapadi, LinkedIn Corporation David Stein, LinkedIn Corporation Bo Zhao, LinkedIn Corporation Abstract: User experience at social media and web platforms such as LinkedIn is heavily dependent on the performance and scalability of its products. Applications such as personalized search and recommendations require real-time scoring of millions of structured candidate documents associated with each query, with strict latency constraints. In such applications, the query incorporates the context of the user (in addition to search keywords if present), and hence can become very large, comprising of thousands of Boolean clauses over hundreds of document attributes. Consequently, candidate selection techniques need to be applied since it is infeasible to retrieve and score all matching documents from the underlying inverted index. We propose CaSMoS, a machine learned candidate selection framework that makes use of Weighted AND (WAND) query. Our framework is designed to prune irrelevant documents and retrieve documents that are likely to be part of the top-k results for the query. We apply a constrained feature selection algorithm to learn positive weights for feature combinations that are used as part of the weighted candidate selection query. We have implemented and deployed this system to be executed in real time using LinkedIn’s Galene search platform. We perform extensive evaluation with different training data approaches and parameter settings, and investigate the scalability of the proposed candidate selection model. Our deployment of this system as part of LinkedIn’s job recommendation engine has resulted in significant reduction in latency (up to 30%) without sacrificing the quality of the retrieved results, thereby paving the way for more sophisticated scoring models. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M16S,136,2d,hd,FALSE,https://i.ytimg.com/vi/4zh3VjsNwdI/maxresdefault.jpg,,431,5,0,0,0
364,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,A8QzAIfn034,2016-06-30T17:20:35Z,30/6/16 17:20,KDD2016 paper 460,"Title: Annealed Sparsity for Adaptive and Dynamic Shrinking Authors: Kai Zhang*, NEC labs America Shandian Shan, Purdue University Zhengzhang Chen, NEC Lab America Chaoran Cheng, New Jersey Institute of Technology Zhi Wei, New Jersey Institute of Technology Guofei Jiang, NEC labs America Jieping Ye, University of Michigan, Ann Arbor Abstract: Sparse learning has received tremendous amount of interest in high-dimensional data analysis due to its model interpretability and the low-computational cost. Among the various techniques, adaptive l1-regularization is an effective framework to improve the convergence behaviour of the LASSO, by using varying strength of regularization across different features. In the meantime, the adaptive structure makes it very powerful in modelling grouped sparsity patterns as well, being particularly useful in high-dimensional multi-task problems. However, choosing an appropriate, global regularization weight is still an open problem. In this paper, inspired by the annealing technique in material science, we propose to achieve ``annealed sparsity’’ by designing a dynamic shrinking scheme that simultaneously optimizes the regularization weights and model coefficients in sparse (multi-task) learning. The dynamic structures of our algorithm are twofold. Feature-wise (spatially), the regularization weights are updated interactively with model coefficients, allowing us to improve the global regularization structure. Iteration-wise (temporally), such interaction is coupled with gradually boosted $\ell_1$-regularization by adjusting an equality norm-constraint, achieving an ``annealing’’ effect to further improve model selection. This renders interesting shrinking behaviour in the whole solution path. Our method competes favorably with state-of-the-art methods in sparse (multi-task) learning. We also apply it in identifying target genes in expression quantitative trait loci analysis (eQTL), which gives useful biological insights in human cancer study. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M10S,190,2d,hd,FALSE,https://i.ytimg.com/vi/A8QzAIfn034/maxresdefault.jpg,,150,0,0,0,0
365,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:34Z,30/6/16 17:20,KDD2016 paper 242,"Title: Accelerated Stochastic Block Coordinate Descent with Optimal Sampling Authors: Aston Zhang*, University of Illinois at Urbana-Champaign Quanquan Gu, University of Virginia Abstract: We study the composite minimization problem where the objective function is the sum of two convex functions: one is a sum of a finite number of strongly convex and smooth functions, and the other is a general convex function that is non-differentiable. Specifically, we consider the case where the non-differentiable function is block separable and admits a simple proximal mapping for each block. This type of composite optimization is common in many data mining and machine learning problems, and can be solved by block coordinate descent algorithms. We propose an accelerated stochastic block coordinate descent (ASBCD) algorithm, which exploits optimal sampling. We show that ASBCD attains a linear rate of convergence, and achieves a lower iteration complexity than block coordinate descent algorithm. In contrast to uniform sampling, we reveal that an optimal non-uniform sampling should be employed to achieve a faster convergence rate. Experimental results on different large-scale real data sets support our theory. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M13S,133,2d,hd,FALSE,https://i.ytimg.com/vi/-H4dn0GwlCU/maxresdefault.jpg,,254,1,1,0,0
366,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Tb3qW-cMtXQ,2016-06-30T17:20:34Z,30/6/16 17:20,KDD2016 paper 185,"Title: Mining Subgroups with Exceptional Transition Behavior Authors: Florian Lemmerich*, Gesis Martin Becker, University of Würzburg Philipp Singer, Gesis Denis Helic, TU Graz Andreas Hotho, University of Wuerzburg Markus Strohmaier, Gesis Abstract: We present a new method for detecting interpretable subgroups with exceptional transition behavior in sequential data. Identifying such patterns has many potential applications, e.g., for studying human mobility or analyzing the behavior of internet users. To tackle this task, we employ exceptional model mining, which is a general approach for identifying interpretable data subsets that exhibit unusual interactions between a set of target attributes with respect to a certain model class. Although exceptional model mining provides a well-suited framework for our problem, previously investigated model classes cannot capture transition behavior. To that end, we introduce first-order Markov chains as a novel model class for exceptional model mining and present a new interestingness measure that quantifies the exceptionality of transition subgroups. The measure compares the distance between the Markov transition matrix of a subgroup and the respective matrix of the entire data with the distance of random dataset samples. In addition, our method can be adapted to find subgroups that match or contradict given transition hypotheses. We demonstrate that our method is consistently able to recover subgroups with exceptional transition models from synthetic data and illustrate its potential in two application examples. Our work is relevant for researchers and practitioners interested in detecting exceptional transition behavior in sequential data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M32S,152,2d,hd,FALSE,https://i.ytimg.com/vi/Tb3qW-cMtXQ/maxresdefault.jpg,,295,3,0,0,0
367,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,CQRnNaIkI8Y,2016-06-30T17:20:33Z,30/6/16 17:20,KDD2016 paper 446,"Title: FINAL: Fast Attributed Network Alignment Authors: Si Zhang*, Arizona State University Hanghang Tong, Arizona State University Abstract: Multiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., finding the node correspondence across different networks) is often the very first step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich attribute information on nodes and/or edges. In this paper, we propose a family of algorithms FINAL to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from optimization perspective based on the alignment consistency principle, and develop effective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can significantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10 times speed-up, while preserving a 95% accuracy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and near real-time response time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M2S,182,2d,hd,FALSE,https://i.ytimg.com/vi/CQRnNaIkI8Y/maxresdefault.jpg,,328,1,1,0,0
368,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,cabFKkQM1cc,2016-06-30T17:20:33Z,30/6/16 17:20,KDD2016 paper 183,"Title: Scalable Time-Decaying Adaptive Prediction Algorithm Authors: Yinyan Tan*, Huawei Software Technologies CO. LTD Zhe Fan, Huawei Software Technologies CO. LTD Guilin Li, Huawei Software Technologies CO. LTD Fangshan Wang, Huawei Software Technologies CO. LTD Zhengbing Li, Huawei Software Technologies CO. LTD Shikai Liu, Huawei Software Technologies CO. LTD Qiuling Pan, Huawei Software Technologies CO. LTD Eric P. Xing, Carnegie Mellon University Qirong Ho, Carnegie Mellon University Abstract: Online learning is used in a wide range of real applications, e.g., predicting ad click-through rates (CTR) and personalized recommendations. Based on the analysis of users’ behaviors in Video-On-Demand (VoD) recommender systems, we discover that the most recent users’ actions can better reflect users’ current intentions and preferences. Under this observation, we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm, called Time-Decaying Adaptive Prediction (TDAP) algorithm. To scale Big Data, we further parallelize our algorithm following the data parallel scheme on both BSP and SSP consistency model. We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms, i.e., Spark and Petuum. TDAP achieves good accuracy: it improves at least 5.6% in terms of prediction precision, compared to FTRL-proximal algorithm; and TDAP scales well: it runs 4 times faster when the number of machines increases from 2 to 10. In our real running business cases, TDAP significantly increases the degree of user activity, which brings more revenue than existing ones for our customers. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M51S,111,2d,hd,FALSE,https://i.ytimg.com/vi/cabFKkQM1cc/maxresdefault.jpg,,641,1,0,0,0
369,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,10WKZ4UY_OU,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 936,"Title: Smart Broadcasting: Do you want to be seen? Authors: Mohammad Reza Karimi*, Sharif University Erfan Tavakoli, Sharif University Mehrdad Farajtabar, Georgia Tech Le Song, Georgia Tech Manuel Gomez-Rodriguez, Max Planck Institute for Software Systems Abstract: Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers’ most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the “visibility” of the user in her followers’ feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the “when-to-post” problem. Our method can find broadcasting strategies that reach a desired “visibility” level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters’ post more visible than alternatives. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M50S,170,2d,hd,FALSE,https://i.ytimg.com/vi/10WKZ4UY_OU/maxresdefault.jpg,,222,0,0,0,0
370,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5yVC2LX3iCE,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 163,"Title: Matrix Computations and Optimization in Apache Spark Authors: Reza Bosagh Zadeh*, Matroid and Stanford Xiangrui Meng, Databricks Alexander Ulanov, Matroid and HP Labs Burak Yavuz, Databricks Li Pu, Twitter Shivaram Venkataraman, UC Berkeley Evan Sparks, UC Berkeley Aaron Staple, Databricks Matei Zaharia, MIT and Databricks Abstract: We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for distributed matrices and optimization routines using these matrices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector operations and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computational power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are already merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M11S,131,2d,sd,FALSE,,,472,2,0,0,0
371,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,F0CVwgNhexo,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 702,"Title: Diversified Temporal Subgraph Pattern Mining Authors: Yi Yang, Fudan University Da Yan, The Chinese University of Hong Kong Huanhuan Wu, The Chinese University of Hong Kong James Cheng*, The Chinese University of Hong Kong Shuigeng Zhou, Fudan University John C.S. Lui, The Chinese University of Hong Kong Abstract: Many graphs in real-world applications, such as telecommunications networks, social-interaction graphs and co-authorship graphs, contain temporal information. However, existing graph mining algorithms fail to exploit these temporal information and the resulting subgraph patterns do not contain any temporal attribute. In this paper, we study the problem of mining a set of diversified temporal subgraph patterns from a temporal graph, where each subgraph is associated with the time interval that the pattern spans. This problem motivates important applications such as finding social trends in social networks, or detecting temporal hotspots in telecommunications networks. We propose a divide-and-conquer algorithm along with effective pruning techniques, and our approach runs 2 to 3 orders of magnitude faster than a baseline algorithm and obtains high-quality temporal subgraph patterns in real temporal graphs. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M46S,166,2d,hd,FALSE,https://i.ytimg.com/vi/F0CVwgNhexo/maxresdefault.jpg,,290,0,1,0,0
372,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,mBZDrp4MwZ0,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 176,"Title: A Truth Discovery Approach with Theoretical Guarantee Authors: Houping Xiao*, SUNY Buffalo Jing Gao, SUNY Buffalo Zhaoran Wang, Princeton University Shiyu Wang, 3University of Illinois, Urbana-Champaign Lu Su, SUNY Buffalo Han Liu, Princeton University Abstract: In the information age, people can easily collect information about the same set of entities from multiple sources, among which conflicts are inevitable. This leads to an important task, \textit{truth discovery}, i.e., to identify true facts (truths) via iteratively updating truths and source reliability. However, the convergence to the truths is never discussed in existing work, and thus there is no theoretical guarantee in the results of these truth discovery approaches. In contrast, in this paper we propose a truth discovery approach with theoretical guarantee. We propose a randomized gaussian mixture model (RGMM) to represent multi-source data, where truths are model parameters. We incorporate source bias which captures its reliability degree into RGMM formulation. The truth discovery task is then modeled as seeking the maximum likelihood estimate (MLE) of the truths. Based on expectation-maximization (EM) techniques, we propose population-based (i.e., on the limit of infinite data) and sample-based (i.e., on a finite set of samples) solutions for the MLE. Theoretically, we prove that both solutions are contractive to an $\epsilon$-ball around the MLE, under certain conditions. Experimentally, we evaluate our method on both simulated and real-world datasets. Experimental results show that our method achieves high accuracy in identifying truths with convergence guarantee. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M34S,214,2d,hd,FALSE,https://i.ytimg.com/vi/mBZDrp4MwZ0/maxresdefault.jpg,,328,0,0,0,0
373,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nAqJHmuG4Ok,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 445,"Title: Ranking Causal Anomalies via Temporal and Dynamical Analysis on Vanishing Correlations Authors: Wei Cheng*, NEC Labs America Kai Zhang, NEC labs America Haifeng Chen, NEC Research Lab Guofei Jiang, NEC labs America Wei Wang, UC Los Angeles Abstract: Modern world has witnessed a dramatic increase in our ability to collect, transmit and distribute real-time monitoring and surveillance data from large-scale information systems and cyber-physical systems. Detecting system anomalies thus attracts significant amount of interest in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be a powerful way in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: 1) fault propagation in the network is ignored; 2) the root casual anomalies may not always be the nodes with a high-percentage of vanishing correlations; 3) temporal patterns of vanishing correlations are not exploited for robust detection. To address these limitations, in this paper we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network, and can perform joint inference on both the structural, and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations, and can compensate for unstructured measurement noise in the system. Extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets demonstrate the effectiveness of our approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M57S,177,2d,hd,FALSE,https://i.ytimg.com/vi/nAqJHmuG4Ok/maxresdefault.jpg,,415,0,0,0,0
374,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tDBH75sXecc,2016-06-30T17:20:32Z,30/6/16 17:20,KDD2016 paper 429,"Title: Predicting Matchups and Preferences in Context Authors: Shuo Chen*, Cornell University Thorsten Joachims, Cornell University Abstract: We present a general probabilistic framework for predicting the outcome of pairwise matchups (e.g. two-player sport matches) and pairwise preferences (e.g. product preferences), both of which have widespread applications ranging from matchmaking in computer games to recommendation in e-commerce. Unlike existing models for these tasks, our model not only learns representations of the items in a more expressive latent vector space, but also models how context modifies matchup and preference outcomes. For example, the context ``weather’’ may alter the winning probability in a tennis match, or the fact that the user is on a mobile device may alter his preferences among restaurants. More generally, the model is capable of handling any symmetric game/comparison problem that can be described by vectorized player/item and game/context features. We provide a comprehensive evaluation of its predictive performance with real datasets from both domains to show its ability to predict preference and game outcomes more accurately than existing models. Furthermore, we demonstrate on synthetic datasets the expressiveness of the model when compared against theoretical limits. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M16S,136,2d,hd,FALSE,https://i.ytimg.com/vi/tDBH75sXecc/maxresdefault.jpg,,299,2,0,0,0
375,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,10tBgmVig5c,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 926,"Title: Data-driven Automatic Treatment Regimen Development and Recommendation Authors: Leilei Sun*, Dalian University of Technology Chuanren Liu, Drexel University Chonghui Guo, Dalian University of Technology Hui Xiong, Rutgers University Yanming Xie, China Academy of Chinese Medical Sciences Abstract: The analysis of large-scale Electrical Medical Records (EMRs) has the potential to optimize treatment regimen development and make proper treatment recommendation, and thus improve cure and effective rate. A treatment record usually includes a series of doctor orders containing temporal and heterogeneous information, such as event sequences and the descriptions, which can raise significant challenges for data analytics. However, for many existing studies, a doctor order is simplified as an event code and a treatment is simplified as a code sequence. Thus, the rich information inherent in doctor orders is not fully used for studying treatments. To this end, we aim on exploiting the rich temporal and heterogenous information in doctor orders and develop new approaches for automatic treatment regimen design and recommendation. Along this line, we first propose a general similarity measurement method which has the ability in capturing the similarities between treatments by measuring the temporal as well as heterogeneous information in doctor orders. Then, an efficient density-peaks-based clustering algorithm is proposed to cluster treatments, and extract a semantic representation of each treatment cluster. Finally, we develop a unified framework for evaluating the extracted treatment regimens, which can combine different categories of information recorded by EMRs together and determine the most effective treatment regimen for a patient cohort. In the empirical study, we exploit the EMRs of 27,678 patients from 14 hospitals for validation. The results show that: 1) Our method can successfully extract typical treatment regimens from large-scale EMRs. The treatment regimens are intuitive and provide managerial implications for treatment regimen design and optimization. 2) By matching patient cohorts with the most effective treatment regimens, the total cure rate in our data improves from 19.89% to 21.28%, and the effective rate increases up to 98.29%. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M12S,132,2d,hd,FALSE,https://i.ytimg.com/vi/10tBgmVig5c/maxresdefault.jpg,,145,0,0,0,0
376,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9Gcr7jLuEGI,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 934,"Title: SQUISH: Near-Optimal Compression for Archival of Relational Datasets Authors: Yihan Gao*, University of Illinois at Urbana-Champaign Aditya Parameswaran, , University of Illinois at Urbana-Champaign Abstract: In every sphere of existence, relational datasets are being generated at an alarmingly rapid rate. Compressing these datasets could significantly reduce storage and archival costs. Traditional compression algorithms, e.g., gzip, are suboptimal for compressing relational datasets since they ignore the table structure and relationships between attributes. In this paper, we study compression algorithms that leverage the relational structure to compress datasets to a much greater extent. We develop SQUISH, a system that uses a combination of Bayesian Networks and Arithmetic Coding to capture multiple kinds of dependencies among attributes and achieve near-entropy compression rate. SQUISH also supports user-defined attributes: users can instantiate new data types by simply implementing five functions for a new class interface. We prove the asymptotic optimality of our compression algorithm and conduct experiments to show the effective of our system: SQUISH achieves a reduction of over 50% in compression relative to systems developed in prior work on a variety of real datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M26S,146,2d,sd,FALSE,,,59,0,0,0,0
377,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,IeLbW-PppKU,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 150,"Title: Meta Structure: Computing Relevance in Large Heterogeneous Information Networks Authors: Zhipeng Huang*, The University of Hong Kong Yudian Zheng, The University of Hong Kong Reynold Cheng, The University of Hong Kong Yizhou Sun, Northeastern University Nikos Mamoulis, The University of Hong Kong Xiang Li, The University of Hong Kong Abstract: A heterogeneous information network (HIN) is a graph model in which objects and edges are annotated with types. Large and complex databases, such as YAGO and DBLP, can be modeled as HINs. A fundamental problem in HINs is the computation of closeness, or relevance, between two HIN objects. Relevance measures can be used in various applications, including entity resolution, recommendation, and information retrieval. However, few works have investigated the use of HIN information for relevance computation. In this paper, we propose the meta structure, which is a directed acyclic graph of object types with edge types connecting in between. The meta structure can describe complex relationship between two HIN objects (e.g., two papers in DBLP share the same authors and topics). We develop three relevance measures based on meta structure. Due to the computational complexity of these measures, we further design an algorithm with data structures proposed to support their evaluation. Our extensive experiments on YAGO and DBLP show that meta structure relevance is more effective than state-of-the-art approaches, and can be efficiently computed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M25S,85,2d,hd,FALSE,https://i.ytimg.com/vi/IeLbW-PppKU/maxresdefault.jpg,,312,1,0,0,0
378,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,LRHay5byNeE,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 427,"Title: Fast Unsupervised Online Drift Detection Using Incremental Kolmogorov-Smirnov Test Authors: Denis Dos Reis*, Universidade de São Paulo Gustavo Batista, Universidade de Sao Paulo at Sao Carlos Peter Flach, University of Bristol Stan Matwin, Dalhousie University Abstract: Data stream research has grown rapidly over the last decade. Two major features distinguish data stream from batch learning: stream data are generated on the fly, possibly in a fast and variable rate; and the underlying data distribution can be non-stationary, leading to a phenomenon known as concept drift. Therefore, most of the research on data stream classification research focuses on proposing efficient models that can adapt to concept drift and maintain a stable performance over time. However, specifically for the classification task, the majority of such methods rely on the instantaneous availability of true labels for all already classified instances. This is a strong assumption that is rarely fulfilled in practical applications. Hence there is a clear need for efficient methods that can detect concept drifts in an unsupervised way. One possibility is the well-known Kolmogorov-Smirnov test, a statistical hypothesis test that tests whether two samples differ. This work has two main contributions. The first one is an Incremental Kolmogorov-Smirnov algorithm that allows performing the Kolmogorov-Smirnov hypothesis test instantly using two samples that change over time, where a change is an insertion and/or removal of an observation. Our algorithm employs a randomized tree and is able to perform the insertion and removal operations in O(log N) with high probability and calculate the Kolmogorov-Smirnov test in O(1), where N is the number of sample observations. This is a significant speed-up compared to the O(N log N) cost of the non-incremental implementation. The second contribution is the use of the Incremental Kolmogorov-Smirnov test to detect concept drifts without true labels. Classification algorithms adapted to use the test rely on a limited portion of those labels just to update the classification model after a concept drift is detected. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M12S,132,2d,sd,FALSE,,,249,2,0,0,0
379,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nXKO25siPNE,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 144,"Title: AnyDBC: An Efficient Anytime Density-based Clustering Algorithm for Very Large Complex Datasets Authors: Son Mai*, Aarhus University Ira Assent, Aarhus University Martin Storgaard, Aarhus University Abstract: The density-based clustering algorithm DBSCAN is a state-of-the-art data clustering technique with numerous applications in many fields. However, its $O(n^2)$ time complexity remains a severe weakness despite many research efforts. In this paper, we propose a novel \emph{anytime} approach to cope with this problem on massive complex datasets by reducing both the range query and the label propagation time of DBSCAN. Our algorithm, called AnyDBC, compresses the data into smaller density-connected subsets called primitive clusters and labels objects based on connected components of these primitive clusters for reducing the label propagation time. Moreover, instead of \emph{passively} performing the range query for all objects, AnyDBC \emph{iteratively} and \emph{actively} learns the current cluster structure of the data and selects some most promising objects for performing the range query and producing clusters at each iteration. Thus, in the end, it performs substantially fewer range queries compared to DBSCAN while still guaranteeing the \emph{exact} final result of DBSCAN. Experiments show speedup factors of several orders of magnitude compared to DBSCAN and its fastest variants on very large real and synthetic complex datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net",22,People & Blogs,PT2M59S,179,2d,hd,FALSE,https://i.ytimg.com/vi/nXKO25siPNE/maxresdefault.jpg,,596,0,0,0,0
380,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,sVrzJ-zL_h0,2016-06-30T17:20:31Z,30/6/16 17:20,KDD2016 paper 697,"Title: XGBoost: A Scalable Tree Boosting System Authors: Tianqi Chen, University of Washington Carlos Guestrin, University of Washington Abstract: Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M9S,129,2d,hd,FALSE,https://i.ytimg.com/vi/sVrzJ-zL_h0/maxresdefault.jpg,,1164,3,0,0,0
381,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,2KVlwCzifbg,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 123,"Title: Dynamics of Large Multi-View Social Networks: Synergy, Cannibalization and Cross-View Interplay Authors: Yu Shi*, University of Illinois at Urbana-Champaign Myunghwan Kim, LinkedIn Corporation Shaunak Chatterjee, LinkedIn Corporation Mitul Tiwari, LinkedIn Corporation Souvik Ghosh, LinkedIn Corporation Romer Rosales, LinkedIn Corporation Abstract: Most social networking services support multiple types of relationships between users, such as getting connected, sending messages, and consuming feed updates. These users and relationships can be naturally represented as a dynamic multi-view network, which is a set of weighted graphs with shared common nodes but having their own respective edges. Different network views, representing structural relationship and interaction types, could have very distinctive properties individually and these properties may change due to interplay across views. Therefore, it is of interest to study how multiple views interact and affect network dynamics and, in addition, explore possible applications to social networking. In this paper, we propose approaches to capture and analyze multi-view network dynamics from various aspects. Through our proposed descriptors, we observe the synergy/cannibalization between different user groups and network views from LinkedIn dataset. We then develop models that consider the synergy/cannibalization per each new relationship, and show the outperforming predictive capability of our models compared to baseline models. Finally, the proposed models allow us to understand the interplay among different views where they dynamically change over time. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M3S,183,2d,hd,FALSE,https://i.ytimg.com/vi/2KVlwCzifbg/maxresdefault.jpg,,928,6,0,0,1
382,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,KkjTqN06PnQ,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 916,"Title: Talent Circle Detection in Job Transition Networks Authors: Huang Xu*, Northwestern Polytechnical University Jingyuan Yang, Rutgers University Zhiwen Yu, Northwestern Polytechnical University Hui Xiong, Rutgers University Hengshu Zhu, Baidu Inc. Abstract: With the high mobility of talent, it becomes critical for the recruitment team to find the right talent from the right source in an efficient manner. The prevalence of Online Professional Networks (OPNs), such as LinekdIn, enables the new paradigm for talent recruitment and job search. However, the dynamic and complex nature of these talent information imposes significant challenges to identify prospective talent sources from large-scale professional networks. Therefore, in this paper, we propose to create a job transition network where vertices stands for organizations and a directed edge represents the talent flow between two organizations for a time period. By analyzing this job transition network, it is able to extract talent circles in a way such that every circle includes the organizations with similar talent exchange patterns. Then, the characteristics of these talent circles can be used for talent recruitment and job search. To this end, we develop a talent circle detection model and design the corresponding learning method by maximizing the Normalized Discounted Cumulative Gain (NDCG) of inferred probability for the edge existence based on edge weights. Then, the identified circles will be labelled by the representative organizations as well as keywords in job descriptions. Moreover, based on these identified circles, we develop a talent exchange prediction method for talent recommendation. Finally, we have performed extensive experiments on real-world data. The results show that, our method can achieve much higher modularity compared to the benchmark approaches as well as high precision and recall for talent exchange prediction. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M43S,163,2d,hd,FALSE,https://i.ytimg.com/vi/KkjTqN06PnQ/maxresdefault.jpg,,128,0,0,0,0
383,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,KwO3P0gUB2o,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 423,"Title: Multi-Task Feature Interaction Learning Authors: Kaixiang Lin*, Michigan State University Jianpeng Xu, Michigan State University Shuiwang Ji, Washington State University Jiayu Zhou, Michigan State University Abstract: Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM’s. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M59S,119,2d,hd,FALSE,https://i.ytimg.com/vi/KwO3P0gUB2o/maxresdefault.jpg,,269,0,0,0,0
384,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,OoCj3QyyLKM,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 426,"Title: Learning Cumulatively to Become More Knowledgeable Authors: Geli Fei*, University of Illinois at Chicago Shuai Wang, University of Illinois at Chicago Bing Liu, University of Illinois at Chicago Abstract: In traditional supervised learning, a learning algorithm takes a fixed training data of several classes to build a classifier. In this paper, we propose to solve a new problem, i.e., building a learning system that learns cumulatively. As time goes by, the system sees more and more classes of data and becomes more and more knowledgeable. We believe that this is similar to human learning. We humans learn continuously, retaining the learned knowledge, learning new things, and updating the existing knowledge with new experiences. Over time, we cumulate more and more knowledge, and become more and more knowledgeable. An intelligent learning or mining system should be capable of doing the same. As algorithmic learning matures, it is time to tackle this cumulative learning problem, which presents two unique challenges. First, the system must be able to detect data from unseen classes in the test set. Classic supervised learning assumes all testing classes are known at training time. Second, the system needs to be able to selectively update its models whenever a new class of data arrives without re-training using all the past training data. This paper proposes a novel approach and system to tackle these challenges. Experimental results on two datasets with learning from 2 classes to up to 100 classes show that the proposed learning system is highly promising in terms of both classification results and computational efficiency. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M24S,144,2d,hd,FALSE,https://i.ytimg.com/vi/OoCj3QyyLKM/maxresdefault.jpg,,143,1,0,0,0
385,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,c_2Py9KzOnY,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 693,"Title: Fast Memory-efficient Anomaly Detection in Streaming Heterogenous Graphs Authors: Emaad Ahmed Manzoor, Stony Brook University Leman Akoglu, Stony Brook University Abstract: Given a stream of heterogeneous graphs containing different types of nodes and edges, how can we spot anomalous ones in real-time while consuming bounded memory? This problem is motivated by and generalizes from its application in security to host-level advanced persistent threat (APT) detection. We propose StreamSpot, a clustering based anomaly detection approach that addresses challenges in two key fronts: (1) heterogeneity, and (2) streaming nature. We introduce a new similarity function for heterogeneous graphs that compares two graphs based on their relative frequency of local substructures, represented as short strings. This function lends itself to a vector representation of a graph, which is (a) fast to compute, and (b) amenable to a sketched version with bounded size that preserves similarity. StreamSpot exhibits desirable properties that a streaming application requires - it is (i) fully-streaming; processing the stream one edge at a time as it arrives, (ii) memory-efficient; requiring constant space for the sketches and the clustering, (iii) fast; taking constant time to update the graph sketches and the cluster summaries that can process over 100K edges per second, and (iv) online; scoring and flagging anomalies in real time. Experiments on datasets containing simulated system-call flow graphs from normal browser activity and various attack scenarios (ground truth) show that our proposed StreamSpot is high-performance; achieving above 95% detection accuracy with small delay, as well as competitive time and memory usage. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT56S,56,2d,hd,FALSE,https://i.ytimg.com/vi/c_2Py9KzOnY/maxresdefault.jpg,,695,44,0,0,2
386,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ftt1L9Kc8PY,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 696,"Title: FASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks Authors: Chen Chen*, Arizona State Unversity Hanghang Tong, Arizona State University Lei Xie, City University of New York Lei YIng, Arizona State University Qing He, Arizona State University Abstract: Multi-layered networks have recently emerged as a new network model, which naturally finds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative filtering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to find the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efficiency of the proposed methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M44S,164,2d,hd,FALSE,https://i.ytimg.com/vi/ftt1L9Kc8PY/maxresdefault.jpg,,308,2,0,0,0
387,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yRajXsfAXrQ,2016-06-30T17:20:30Z,30/6/16 17:20,KDD2016 paper 914,"Title: The Million Domain Challenge: Broadcast Email Prioritization by Cross-domain Recommendation Authors: Bediou Wang*, Simon Fraser University Martin Ester, Simon Fraser University Yikang Liao, Zhejiang University Jiajun Bu, Zhejiang University Yu Zhu, Zhejiang University Deng Cai, Zhejiang University Ziyu Guan, Northwest University of China Abstract: With email overload becoming a billion-level drag on the economy, personalized email prioritization is of urgent need to help predict the importance level of an email. Despite lots of previous effort on the topic, broadcast email, an important type of emails with its unique challenges and intriguing opportunities, has been overlooked. The most salient opportunity lies in that effective collaborative filtering features can be exploited due to thousands of receivers of a typical broadcast email. However, every broadcast email is completely ``cold’’ and it is very costly to obtain users’ preference feedback. Fortunately, there exist up to million-level broadcast mailing lists in a real life email system. Similar mailing lists can provide useful extra information for broadcast email prioritization in a target mailing list. How to mine such useful extra information is a challenging problem that has never been touched. In this work, we propose the first broadcast email prioritization framework considering large numbers of mailing lists by formulating this problem as a cross domain recommendation problem. An optimization framework is proposed to select the optimal set of source domains considering multiple criteria including overlap of users, feedback pattern similarity and coverage of users. Our method is thoroughly evaluated on a real world industrial dataset from Samsung Electronics and is proved highly effective and outperforms all the baselines. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT4M28S,268,2d,hd,FALSE,https://i.ytimg.com/vi/yRajXsfAXrQ/maxresdefault.jpg,,150,1,0,0,0
388,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,1jGZXTey-7c,2016-06-30T17:20:29Z,30/6/16 17:20,KDD2016 paper 403,"Title: Accelerating Online CP Decompositions for Higher Order Tensors Authors: Shuo Zhou*, University of Melbourne Nguyen Vinh, University of Melbourne James Bailey, University of Melbourne Yunzhe Jia, University of Melbourne Ian Davidson, University of California-Davis Abstract: Tensors are a natural representation for multidimensional data. In recent years, CANDECOMP/PARAFAC (CP) decomposition, one of the most popular tools for analyzing multi-way data, has been extensively studied and widely applied. However, today’s datasets are often dynamically changing over time. Tracking the CP decomposition for such dynamic tensors is a crucial but challenging task, due to the large scale of the tensor and the velocity of new data arriving. Traditional techniques, such as Alternating Least Squares (ALS), cannot be directly applied to this problem because of their poor scalability in terms of time and memory. Additionally, existing online approaches have only partially addressed this problem and can only be deployed on third-order tensors. To fill this gap, we propose an efficient online algorithm that can incrementally track the CP decompositions of dynamic tensors with an arbitrary number of dimensions. In terms of effectiveness, our algorithm demonstrates comparable results with the most accurate algorithm, ALS, whilst being computationally much more efficient. Specifically, on small and moderate datasets, our approach is tens to hundreds of times faster than ALS, while for large-scale datasets, the speedup can be more than 3,000 times. Compared to other state-of-the-art online approaches, our method shows not only significantly better decomposition quality, but also better performance in terms of stability, efficiency and scalability. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M39S,159,2d,sd,FALSE,https://i.ytimg.com/vi/1jGZXTey-7c/maxresdefault.jpg,,447,4,0,0,0
389,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Sh7FGw3hnMw,2016-06-30T17:20:29Z,30/6/16 17:20,KDD2016 paper 118,"Title: Dynamic Clustering of Streaming Short Documents Authors: Shangsong Liang*, University College London Emine Yilmaz, University College London Evangelos Kanoulas, University of Amsterdam Abstract: Clustering technology has found numerous applications in mining textual data. It was shown to enhance the performance of retrieval systems in various different ways, such as identifying different query aspects in search result diversification, improving smoothing in the context of language modeling, matching queries with documents in a latent topic space in ad-hoc retrieval, summarizing documents etc. The vast majority of clustering methods have been developed under the assumption of a static corpus of long (and hence textually rich) documents. Little attention has been given to streaming corpora of short text, which is the predominant type of data in Web 2.0 applications, such as social media, forums, and blogs. In this paper, we consider the problem of dynamically clustering a streaming corpus of short documents. The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time. To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster-based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M30S,90,2d,hd,FALSE,https://i.ytimg.com/vi/Sh7FGw3hnMw/maxresdefault.jpg,,405,0,1,0,0
390,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ZlnwndFPj-Q,2016-06-30T17:20:29Z,30/6/16 17:20,KDD2016 paper 908,"Title: GMove: Group-Level Mobility Modeling using Geo-Tagged Social Media Authors: Chao Zhang*, University of Illinois at Urbana-Champaign Keyang Zhang, University of Illinois at Urbana-Champaign Quan Yuan, University of Illinois at Urbana-Champaign Luming Zhang, University of Illinois at Urbana-Champaign Tim Hanratty, University of Illinois at Urbana-Champaign Jiawei Han, University of Illinois at Urbana-Champaign Abstract: Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and location prediction. While there has been fruitful research on modeling human mobility using tracking data (e.g., GPS traces), the explosively growing geo-tagged social media (GSM) brings new opportunities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GSM data remains a challenge that cannot be readily addressed by existing techniques. We propose GMove, a group-level mobility modeling method for GSM data. Our key insight is that, the GSM data usually contains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GMove thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Furthermore, to reduce text sparsity of GSM data, GMove also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correlations as auxiliary knowledge, it performs sampling-based augmentation to alleviate text sparsity and produce high-quality HMMs. Our extensive experiments on two real-life data sets demonstrate that GMove can effectively generate meaningful group-level mobility models. Moreover, with context-aware location prediction as an example application, we observe that GMove significantly outperforms baseline mobility models in terms of prediction accuracy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M30S,150,2d,hd,FALSE,https://i.ytimg.com/vi/ZlnwndFPj-Q/maxresdefault.jpg,,260,0,0,0,0
391,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tVCdltRlE3Q,2016-06-30T17:20:29Z,30/6/16 17:20,KDD2016 paper 122,"Title: CatchTartan: Representing and Summarizing Dynamic Multicontextual Behaviors Authors: Meng Jiang*, University of Illinois at Urbana-Champaign Christos Faloutsos, Carnegie Mellon University Jiawei Han, University of Illinois at Urbana-Champaign Abstract: Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services. Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values (including timestamps) in each dimension. However, the human behaviors are multicontextual and dynamic: (1) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension; (2) many behavior collections, such as tweets or papers, evolve over time. In this paper, we represent the behavioral data as a two-level matrix (temporal-behaviors by dimensional-values) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice. We further develop a propagation method CatchTartan to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way: it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. CatchTartan outperforms the baselines on both the accuracy and speed. We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data, providing comprehensive summaries for the events, human life and scientific development. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M16S,136,2d,hd,FALSE,https://i.ytimg.com/vi/tVCdltRlE3Q/maxresdefault.jpg,,233,1,0,0,0
392,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,x0m51D2K6wQ,2016-06-30T17:20:29Z,30/6/16 17:20,KDD2016 paper 690,"Title: Hierarchical Incomplete Multi-source Feature Learning for Spatiotemporal Event Forecasting Authors: Liang Zhao*, Virginia Tech Jieping Ye, University of Michigan at Ann Arbor Feng Chen, SUNY Albany Chang-Tien Lu, Virginia Tech Naren Ramakrishnan, Virginia Tech Abstract: Significant societal event forecasting is an important and complex process as it involves the consideration of many aspects of that society, including its economics, politics, and culture. Traditional forecasting methods based on a single data source find it hard to cover all these aspects comprehensively, thus limiting the model performance. Multi-source event forecasting requires more sophisticated models but still suffers from several challenges, including 1) geographical hierarchies in the multi-source data features, 2) missing values in the interactive features, and 3) the characterization of structured feature sparsity. This paper proposes a novel feature learning model that concurrently addresses all the above challenges. Specifically, given multi-source data from different geographical levels, we design a new forecasting model by characterizing the lower-level features’ dependence on higher-level features. To handle the structured sparsity and deal with missing values among the coupled features, we propose a novel feature learning model based on Nth-order strong hierarchy and fused-overlapping group Lasso. An efficient algorithm is developed to optimize the model parameters and ensure global optima. Extensive experiments on 10 datasets in different domains demonstrate the effectiveness and efficiency of the proposed model. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M11S,191,2d,hd,FALSE,https://i.ytimg.com/vi/x0m51D2K6wQ/maxresdefault.jpg,,174,1,0,0,0
393,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,6BB-xVKBOkw,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 392,"Title: Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks Authors: Jung-Woo Ha*, NAVER LABS Hyuna Pyo, NAVER LABS Jeonghee Kim, NAVER LABS Abstract: Precise item categorization is a key issue in e-commerce domains. However, it still remains a challenging problem due to data size, category skewness, and noisy metadata. Here, we demonstrate a successful report on a deep learning-based item categorization method, i.e., deep categorization network (DeepCN), in an e-commerce website. DeepCN is an end-to-end model using multiple recurrent neural networks (RNNs) dedicated to metadata attributes for generating features from text metadata and fully connected layers for classifying item categories from the generated features. The categorization errors are propagated back through the fully connected layers to the RNNs for weight update in the learning process. This deep learning-based approach allows diverse attributes to be integrated into a common representation, thus overcoming sparsity and scalability problems. We evaluate DeepCN on large-scale real-world data including more than 94 million items with approximately 4,100 leaf categories from a Korean e-commerce website. Experiment results show our method improves the categorization accuracy compared to the model using single RNN as well as a standard classification model using unigram-based bag-of-words. Furthermore, we investigate how much the model parameters and the used attributes influence categorization performances. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M48S,168,2d,hd,FALSE,https://i.ytimg.com/vi/6BB-xVKBOkw/maxresdefault.jpg,,1626,58,0,0,0
394,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,L4kx_neNW0Y,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 397,"Title: Compact and Scalable Graph Neighborhood Sketching Authors: Takuya Akiba*, National Institute of Informatics Yosuke Yano, National Institute of Informatics Abstract: The all-distances sketch (ADS) has recently emerged as a promising paradigm of graph neighborhood sketching. An ADS is a probabilistic data structure that is defined for each vertex of a graph. ADSs facilitate accurate estimation of many useful indicators for network analysis with the guarantee of accuracy, and the ADSs for all the vertices in a graph can be computed in near-linear time. Because of these useful properties, ADS has attracted considerable attention. However, a critical drawback of ADS is its space requirement, which tends to be much larger than that of the graph itself. In the present study, we address this issue by designing a new graph sketching scheme, namely, sketch retrieval shortcuts (SRS). Although SRSs are more space-efficient than ADSs by an order of magnitude, an ADS of any vertex can be quickly retrieved from the SRSs. The retrieved ADSs can be used to estimate the aforementioned indicators in exactly the same manner as with plain ADSs, inheriting the same accuracy guarantee. Our experiments on real-world networks demonstrate the usefulness of SRSs as a practical back-end of large-scale graph data mining. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M47S,167,2d,hd,FALSE,https://i.ytimg.com/vi/L4kx_neNW0Y/maxresdefault.jpg,,291,0,0,0,0
395,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,P5WbemlahEI,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 683,"Title: MANTRA: A Scalable Approach to Mining Temporally Anomalous Sub-trajectories Authors: Prithu Banerjee*, UBC Pranali Yawalkar, IIT Madras Sayan Ranu, IIT Madras Abstract: In this paper, we study the problem of mining temporally anomalous sub-trajectory patterns from trajectory streams. Given the prevailing road conditions, a sub-trajectory is temporally anomalous if its travel time deviates significantly from the expected time. Mining these patterns requires us to delve into the sub-trajectory space, which is not scalable for real-time analytics. To overcome this scalability challenge, we design a technique called MANTRA. We study the properties unique to anomalous sub-trajectories and utilize them in MANTRA to iteratively refine the search space into a disjoint set of sub-trajectory islands. The expensive enumeration of all possible sub-trajectories is performed only on the islands to compute the answer set of maximal anomalous sub-trajectories. Extensive experiments on both real and synthetic datasets establish MANTRA as more than 3 orders of magnitude faster than baseline techniques. Moreover, through trajectory classification and segmentation, we demonstrate that the proposed model conforms to human intuition. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M29S,209,2d,hd,FALSE,https://i.ytimg.com/vi/P5WbemlahEI/maxresdefault.jpg,,3872,60,1,0,11
396,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UFJJr1NJlLY,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 880,"Title: Portfolio Selections in P2P Lending: A Multi-Objective Perspective Authors: Hongke Zhao*, University of Science and Technology of China Guifeng Wang, University of Science and Technology of China Yong Ge, University of North Carolina at Charlotte Qi Liu, University of Science and Technology of China Enhong Chen, University of Science and Technology of China Abstract: P2P lending is an emerging wealth-management service for individuals, which allows lenders to directly bid and invest on the loans created by borrowers without traditional financial intermediaries. In these platforms, lenders often consider multiple objectives (e.g., no-default probability, fullyfunded probability and winning-bid probability) when they select loans to invest. However, how to automatically assess loans from multiple objectives and help lenders select loan portfolios is a very challenging problem. To that end, in this paper, we present a holistic study on portfolio selection in P2P lending. Specifically, we first propose to adapt gradient boosting decision tree, which combines both static features and dynamic features, to assess loans from multiple objectives. Then we propose two strategies, i.e., weighted objective optimization strategy and multi-objective optimization strategy, to solve the portfolio selection problem for lenders. While weighted objective optimization strategy provides one optimal portfolio for each lender, multi-objective optimization strategy gets the Pareto-optimal (skyline) portfolio set to each lender. To resolve the two optimization problems, we propose two algorithms, namely DPA and EVA. These two algorithms can efficiently solve the optimization problems of the proposed two strategies respectively. Finally, we evaluate our methods via conducting extensive experiments with a large-scale real-world data set. The experimental results demonstrate the effectiveness of our solutions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M36S,156,2d,hd,FALSE,https://i.ytimg.com/vi/UFJJr1NJlLY/maxresdefault.jpg,,186,0,0,0,0
397,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Y3dvxWnVhJ4,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 93,"Title: Smart Broadcasting: Do you want to be seen? Authors: Mohammad Reza Karimi*, Sharif University Erfan Tavakoli, Sharif University Mehrdad Farajtabar, Georgia Tech Le Song, Georgia Tech Manuel Gomez-Rodriguez, Max Planck Institute for Software Systems Abstract: Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers’ most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the “visibility” of the user in her followers’ feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the “when-to-post” problem. Our method can find broadcasting strategies that reach a desired “visibility” level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters’ post more visible than alternatives. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M,180,2d,hd,FALSE,https://i.ytimg.com/vi/Y3dvxWnVhJ4/maxresdefault.jpg,,183,0,0,0,0
398,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,e-L48-0M638,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 883,"Title: Compressing Graphs and Indexes with Recursive Graph Bisection Authors: Laxman Dhulipala*, Carnegie Mellon University Igor Kabiljo, Facebook Brian Karrer, Facebook Abstract: Graph reordering is a powerful technique to increase the locality of the representations of graphs, which can be helpful in several applications. We study how the technique can be used to improve compression of graphs and inverted indexes. We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for graph compression, and show how it can be employed for compression-friendly reordering of social networks and web graphs and for assigning document identifiers in inverted indexes. We design and implement a novel theoretically sound reordering algorithm that is based on recursive graph bisection. Our experiments show a significant improvement of the compression rate of graph and indexes over existing heuristics. The new method is relatively simple and allows efficient parallel and distributed implementations, which is demonstrated on graphs with billions of vertices and hundreds of billions of edges. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/e-L48-0M638/maxresdefault.jpg,,560,5,0,0,0
399,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,tHy1AG9jv9Q,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 679,"Title: Structural Neighborhood Based Classification of Nodes in a Network Authors: Sharad Nandanwar*, Indian Institute of Science Musti Narasimha Murty, Indian Institute of Science Abstract: Classification of entities based on the underlying network structure is an important problem. Networks encountered in practice are sparse and have many missing and noisy links. Even though statistical learning techniques have been used for intra-network classification based on local neighborhood, they perform poorly as they exploit only local information. In this paper, we propose a novel structural neighborhood based learning using a random walk. For classifying a node we take a random walk from the corresponding node, and make a decision based on how nodes in the respective k th - level neighborhood are getting classified. We observe that random walks of short length are helpful in classification. Emphasizing role of longer random walk may cause under- lying markov-chain to converge towards stationary distribu- tion. Considering this, we take a lazy random walk based ap- proach with variable termination probability for each node, based on its structural properties including degree. Our ex- perimental study on real world datasets demonstrates the superiority of the proposed approach over the existing state- of-the-art approaches. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT4M29S,269,2d,hd,FALSE,https://i.ytimg.com/vi/tHy1AG9jv9Q/maxresdefault.jpg,,2576,71,1,0,7
400,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,zLUmjpQhzWQ,2016-06-30T17:20:28Z,30/6/16 17:20,KDD2016 paper 110,"Title: FRAUDAR: Bounding Graph Fraud in the Face of Camouflage Authors: Bryan Hooi*, Carnegie Mellon University Hyun Ah Song, Carnegie Mellon University Alex Beutel, Carnegie Mellon University Neil Shah, Carnegie Mellon University Kijung Shin, Carnegie Mellon University Christos Faloutsos, Carnegie Mellon University Abstract: Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look “normal”. Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage-resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M59S,119,2d,hd,FALSE,https://i.ytimg.com/vi/zLUmjpQhzWQ/maxresdefault.jpg,,897,1,1,0,0
401,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 671,"Title: FUSE: Full Spectral Clustering Authors: Wei Ye*, University of Munich Sebastian Goebl, University of Munich Claudia Plant, University of Munich Christian Boehm, University of Munich Abstract: Multi-scale data which contains structures at different scales of size and density is a big challenge for spectral clustering. Even given a suitable locally scaled affinity matrix, the first k eigenvectors of such a matrix still cannot separate clusters well. Thus, in this paper, we exploit the fusion of the cluster-separation information from all eigenvectors to achieve a better clustering result. Our method FUll Spectral ClustEring FUSE is based on Power Iteration (PI) and Independent Component Analysis ICA. PI is used to fuse all eigenvectors to one pseudo-eigenvector which inherits all the cluster-separation information. To conquer the cluster-collision problem, we utilize PI to generate a number of p pseudo-eigenvectors. Since these pseudo-eigenvectors are redundant and the cluster-separation information is contaminated with noise, ICA is adopted to rotate the pseudo-eigenvectors to make them pairwise statistically independent. We find such a rotation is beneficial to clustering. To make ICA overcome local optima and speed up the search process, we develop a self-adaptive and self-learning greedy search method. Finally, we select k rotated pseudo-eigenvectors (independent components) which have more cluster-separation information measured by kurtosis for clustering. Various synthetic and real-world data verifies the effectiveness and efficiency of our FUSE method. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M45S,165,2d,hd,FALSE,https://i.ytimg.com/vi/-JRH92Cu87c/maxresdefault.jpg,,230,0,0,0,0
402,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5fzNw5CMg9w,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 86,"Title: Deep Visual-Semantic Hashing for Cross-Modal Retrieval Authors: Yue Cao*, Tsinghua University Mingsheng Long, Tsinghua University Jianmin Wang, Tsinghua University Qiang Yang, Hong Kong University of Science and Technology Philip S. Yu, Tsinghua University Abstract: Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep VisualSemantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visualsemantic fusion network for learning joint embedding space with images and text sentences, and two modality-specific hashing networks for learning hash functions with compact binary codes. Our architecture effectively unifies joint multimodal embedding with cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and highquality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in crossmodal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/5fzNw5CMg9w/maxresdefault.jpg,,281,1,0,0,0
403,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,6_S1F9pOv-Y,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 66,"Title: Collaborative Knowledge Base Embedding for Recommender Systems Authors: Fuzheng Zhang*, Microsoft Research Nicholas Jing Yuan, Microsoft Research Defu Lian, University of Electronic Science and Technology of China Xing Xie, Microsoft Research Wei-Ying Ma, University of Electronic Science and Technology of China Abstract: Among different recommendation techniques, collaborative filtering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary information is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstructured data with different semantics, which can be consumed by various applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowledge base, we design three components to extract items’ semantic representations from structural content, textual content and visual content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items’ structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items’ textual representations and visual representations, respectively. Finally, we propose our final integrated framework, which is termed as Collaborative Knowledge Base Embedding (CKE), to jointly learn the latent representations in collaborative filtering as well as items’ semantic representations from the knowledge base. To evaluate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two real-world datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art recommendation methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M26S,146,2d,hd,FALSE,https://i.ytimg.com/vi/6_S1F9pOv-Y/maxresdefault.jpg,,992,0,2,0,0
404,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HOm6xIsUU-Q,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 677,"Title: An Empirical Study on Recommendation with Multiple Types of Feedback Authors: Liang Tang*, LinkedIn Corp. Bo Long, LinkedIn Corp. Bee-Chung Chen, LinkedIn Corp. Deepak Agarwal, LinkedIn Corp. Abstract: User feedback like clicks and ratings on recommended items provides important information for recommender systems to predict users’ interests in unseen items. Most systems rely on models trained using a single type of feedback, e.g., ratings for movie recommendation and clicks for online news recommendation. However, in addition to the primary feedback, many systems also allow users to provide other types of feedback, e.g., liking or sharing an article, or hiding all articles from a source. These additional feedback potentially provides extra information for the recommendation models. To optimize user experience and business objectives, it is important for a recommender system to use both the primary feedback and additional feedback. This paper presents an empirical study on various training methods for incorporating multiple user feedback types based on LinkedIn recommendation products. We study three important problems that we face at LinkedIn: (1) Wether to send an email based on clicks and complaints, (2) how to rank updates in LinkedIn feeds based on clicks and hides and (3) how jointly optimize for viral actions and clicks in LinkedIn feeds. Extensive offline experiments on historical data show the effectiveness of these methods in different situations. Online A/B testing results further demonstrate the impact of these methods on LinkedIn production systems. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M55S,175,2d,hd,FALSE,https://i.ytimg.com/vi/HOm6xIsUU-Q/maxresdefault.jpg,,446,2,0,0,0
405,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Q0PyLfwPX6M,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 878,"Title: Inferring Network Effects from Observational Data Authors: David Arbour*, University of Massachusetts Amherst Dan Garant, University of Massachusetts Amherst David Jensen, University of Massachusetts Amherst Abstract: We present Relational Causal Inference (RCA), a general method for estimating causal effects in relational data. Relational Causal Inference is implemented through two high-level operations: identification of an adjustment set and relational regression adjustment. The former is achieved through an extension of Pearl’s back-door criterion to relational domains. We demonstrate how this extended definition can be used to estimate causal effects in the presence of network interference and confounding. RCA is agnostic to functional form, and it can easily model both discrete and continuous treatments as well as estimate the effects of a wider array of network interventions than existing experimental approaches. We show that RCA can yield robust estimates of causal effects using common regression models without extensive parameter tuning. Through a series of simulation experiments on a variety of synthetic and real-world network structures, we show that causal effects estimated on observational data with RCA are nearly as accurate as those estimated from well-designed network experiments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M38S,158,2d,hd,FALSE,https://i.ytimg.com/vi/Q0PyLfwPX6M/maxresdefault.jpg,,370,0,0,0,0
406,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,fFFWTt6Ai7I,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 347,"Title: Efficient Processing of Network Proximity Queries via Chebyshev Acceleration Authors: Mustafa Coskun*, Case Western Reserve University Ananth Grama, Purdue University Mehmet Koyuturk, Case Western Reserve University Abstract: Network proximity is at the heart of a large class of network analytics and information retrieval techniques, including node/ edge rankings, network alignment, and random walk based proximity queries, among many others. Owing to its importance, signi?cant e?ort has been devoted to accelerating iterative processes underlying network proximity computations. These techniques rely on numerical properties of power iterations, as well as structural properties of the networks to reduce the runtime of iterative algorithms. In this paper, we present an alternate approach to acceleration of network proximity queries using Chebyshev polynomials. We show that our approach, called Chopper, yields asymptotically faster convergence in theory, and signi?cantly reduced convergence times in practice. We also show that other existing acceleration techniques can be used in conjunction with Chopper to further reduce runtime. Using a number of large real-world networks, and top-k proximity queries as the benchmark problem, we show that Chopper outperforms existing methods for wide ranges of parameter values. When integrated with existing methods, Chopper yields two-fold improvement in performance over the state of the art for networks with millions of nodes and edges. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M9S,189,2d,hd,FALSE,https://i.ytimg.com/vi/fFFWTt6Ai7I/maxresdefault.jpg,,224,0,0,0,0
407,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,joh3oMJUa8Y,2016-06-30T17:20:27Z,30/6/16 17:20,KDD2016 paper 350,"Title: FLASH: Fast Bayesian Optimization for Data Analytic Pipelines Authors: Yuyu Zhang*, Georgia Institute of Technology Mohammad Bahadori, Georgia Institute of Technology Hang Su, Georgia Institute of Technology Jimeng Sun, Georgia Institute of Technology Abstract: Modern data science relies on data analytic pipelines to organize interdependent computational steps. Such analytic pipelines often involve different algorithms across multiple steps, each with its own hyperparameters. To achieve the best performance, it is often critical to select optimal algorithms and to set appropriate hyperparameters, which requires large computational efforts. Bayesian optimization provides a principled way for searching optimal hyperparameters for a single algorithm. However, many challenges remain in solving pipeline optimization problems with high-dimensional and highly conditional search space. In this work, we propose Fast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines. FLASH is a two-layer Bayesian optimization framework, which firstly uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms. FLASH also includes an effective caching algorithm which can further accelerate the search process. Extensive experiments on a number of benchmark datasets have demonstrated that FLASH significantly outperforms previous state-of-the-art methods in both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20% improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art performance on a real-world application for healthcare predictive modeling. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M56S,176,2d,hd,FALSE,https://i.ytimg.com/vi/joh3oMJUa8Y/maxresdefault.jpg,,185,0,0,0,0
408,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9O1apLAWSjs,2016-06-30T17:20:26Z,30/6/16 17:20,KDD2016 paper 853,"Title: Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned Authors: Xiaolin Shi*, Yahoo! Labs Alex Deng, Microsoft Abstract: Online controlled experiments, also called A/B testing, have been established as the mantra for data-driven decision making in many web-facing companies. In recent years, there are emerging research works focusing on building the platform and scaling it up, best practices and lessons learned to obtain trustworthy results, and experiment design techniques and various issues related to statistical inference and testing. However, despite playing a central role in online controlled experiments, there is little published work on treating metric development itself as a data-driven process. In this paper, we focus on the topic of how to develop meaningful and useful metrics for online services in their online experiments, and show how data-driven techniques and criteria can be applied in metric development process. In particular, we emphasize two fundamental qualities for the goal metrics (or Overall Evaluation Criteria) of any online service: directionality and sensitivity. We share lessons on why these two qualities are critical, how to measure these two qualities of metrics of interest, how to develop metrics with clear directionality and high sensitivity by using approaches based on user behavior models and data-driven calibration, and how to choose the right goal metrics for the entire online services.",22,People & Blogs,PT1M47S,107,2d,hd,FALSE,https://i.ytimg.com/vi/9O1apLAWSjs/maxresdefault.jpg,,201,0,0,0,0
409,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,K5EXAVlEF5U,2016-06-30T17:20:26Z,30/6/16 17:20,KDD2016 paper 63,"Title: Towards Conversational Recommender Systems Authors: Konstantina Christakopoulou*, University of Minnesota Katja Hofmann, Microsoft Filip Radlinski, Microsoft Abstract: People often ask others for restaurant recommendations as a way to discover new dining experiences. This makes restaurant recommendation an exciting scenario for recommender systems and has led to substantial research in this area. However, most such systems behave very differently from a human when asked for a recommendation. The goal of this paper is to begin to reduce this gap. In particular, humans can quickly establish preferences when asked to make a recommendation for someone they do not know. We address this cold-start recommendation problem in an online learning setting. We develop a preference elicitation framework to identify which questions to ask a new user to quickly learn their preferences. Taking advantage of latent structure in the recommendation space using a probabilistic latent factor model, our experiments with both synthetic and user study data compare different types of feedback and question selection strategies. We find that our framework can make very effective use of online user feedback, improving personalized recommendations over a static model by 25% after asking only 2 questions. Our results demonstrate dramatic benefits of starting from offline embeddings, and highlight the benefit of bandit-based explore-exploit strategies in this setting. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M13S,133,2d,sd,FALSE,,,976,1,0,0,0
410,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YmYkBtKAePE,2016-06-30T17:20:26Z,30/6/16 17:20,KDD2016 paper 343,"Title: Absolute Fused Lasso and Its Application to Genome-Wide Association Studies Authors: Tao Yang*, Arizona State University Jun Liu, SAS Institute Inc. Pinghua Gong, University of Michigan Ruiwen Zhang, SAS Institute Inc. Xiaotong Shen, University of Minnesota Jieping Ye, University of Michigan at Ann Arbor Abstract: In many real-world applications, the samples/features acquired are in spatial or temporal order. In such cases, the magnitudes of adjacent samples/features are typically close to each other. Meanwhile, in the high-dimensional scenario, identifying the most relevant samples/features is also desired. In this paper, we consider a regularized model which can simultaneously identify important features and group similar features together. The model is based on a penalty called Absolute Fused Lasso (AFL). The AFL penalty encourages sparsity in the coefficients as well as their successive differences of absolute values—-i.e., local constancy of the coefficient components in absolute values. Due to the non-convexity of AFL, it is challenging to develop efficient algorithms to solve the optimization problem. To this end, we employ the Difference of Convex functions (DC) programming to optimize the proposed non-convex problem. At each DC iteration, we adopt the proximal algorithm to solve a convex regularized sub-problem. One of the major contributions of this paper is to develop a highly efficient algorithm to compute the proximal operator. Empirical studies on both synthetic and real-world data sets from Genome-Wide Association Studies demonstrate the efficiency and effectiveness of the proposed approach in simultaneous identifying important features and grouping similar features. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M11S,131,2d,hd,FALSE,https://i.ytimg.com/vi/YmYkBtKAePE/maxresdefault.jpg,,64,0,0,0,0
411,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,mdDteL3LWKE,2016-06-30T17:20:26Z,30/6/16 17:20,KDD2016 paper 864,"Title: Scalable Fast Rank-1 Dictionary Learning for fMRI Big Data Analysis Authors: Xiang Li*, The University of Georgia Milad Makkie, The University of Georgia Binbin Lin, University of Michigan, Ann Arbor Mojtaba Sedigh Fazli, The University of Georgia Ian Davidson, University of California-Davis Jieping Ye, University of Michigan, Ann Arbor Tianming Liu, The University of Georgia Shannon Quinn, The University of Georgia Abstract It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M47S,167,2d,sd,FALSE,,,102,0,0,0,0
412,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,uwdV49YWHTc,2016-06-30T17:20:26Z,30/6/16 17:20,KDD2016 paper 653,"Title: Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments Authors: Alexey Drutsa*, Yandex Alexey Poyarkov, Yandex Andrey Khalyavin, Yandex Gleb Gusev, Yandex Pavel Serdyukov, Yandex Abstract: Nowadays, the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates achieving more than a thousand concurrent experiments per day. Despite the increasing need for running more experiments, these services are limited in their user traffic. This situation leads to the problem of finding a new or improving existing key performance metric with a higher sensitivity and lower variance. We focus on the problem of variance reduction for engagement metrics of user loyalty that are widely used in A/B testing of web services. We develop a general framework that is based on evaluation of the mean difference between the actual and the approximated values of the key performance metric (instead of the mean of this metric). On the one hand, it allows us to incorporate the state-of-the-art techniques widely used in randomized experiments of clinical and social research, but limitedly used in online evaluation. On the other hand, we propose a new class of methods based on advanced machine learning algorithms, including ensembles of decision trees, that, to the best of our knowledge, have not been applied earlier to the problem of variance reduction. We validate the variance reduction approaches on a very large set of real large-scale A/B experiments run at Yandex for different engagement metrics of user loyalty. Our best approach demonstrates 63% average variance reduction (which is equivalent to 63% saved user traffic) and detects the treatment effect in 2 times more A/B experiments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M59S,179,2d,hd,FALSE,https://i.ytimg.com/vi/uwdV49YWHTc/maxresdefault.jpg,,2228,6,2,0,0
413,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,95sv5HDmUZ8,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 49,"Title: Positive-Unlabeled Learning in Streaming Networks Authors: Shiyu Chang*, UIUC Yang Zhang, UIUC Jiliang Tang, Yahoo! Labs Dawei Yin, Yahoo! Labs Yi Chang, Yahoo! Labs Mark Hasegawa-Johnson, UIUC Thomas Huang, UIUC Abstract: Data of many problems in real-world systems such as link prediction and one-class recommendation share common characteristics. First, data are in the form of positive-unlabeled (PU) measurements (e.g. Twitter “following”, Facebook “like”, etc.) that do not provide negative information, which can be naturally represented as networks. Second, in the era of big data, such data are generated temporally-ordered, continuously and rapidly, which determines its streaming nature. These common characteristics allow us to unify many problems into a novel framework – PU learning in streaming networks. In this paper, principled probabilistic approach SPU is proposed to leverage the characteristics of the streaming PU inputs. In particular, SPU captures temporal dynamics and provides real-time adaptations and predictions by identifying the potential negative signals concealed in unlabeled data. Our empirical results on various real-world datasets demonstrate the effectiveness of the pro- posed framework over other state-of-the-arts in both link prediction and recommendation. In addition, the implementation of the proposed method will become publicly available upon the acceptance of this paper. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M12S,132,2d,hd,FALSE,https://i.ytimg.com/vi/95sv5HDmUZ8/maxresdefault.jpg,,454,0,0,0,0
414,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,ABxx7BkL85I,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 849,"Title: Predicting Disk Replacement towards Reliable Data Centers Authors: Mirela Botezatu*, IBM Research Ioana Giurgiu, IBM Research Jasmina Bogojeska, IBM Research Dorothea Wiesmann, IBM Research Abstract: Disks are among the most frequently failing components in today’s IT environments. Despite a set of defense mechanisms such as RAID, the availability and reliability of the system are still often impacted severely. In this paper, we present a highly accurate SMART-based analysis pipeline that can correctly predict the necessity of a disk replacement even 10-15 days in advance. Our method has been built and evaluated on more than 30000 disks from two major manufacturers, monitored over 17 months. Our approach employs statistical techniques to automatically detect which SMART parameters correlate with disk replacement and uses them to predict the replacement of a disk with even 98% accuracy.",22,People & Blogs,PT2M26S,146,2d,hd,FALSE,https://i.ytimg.com/vi/ABxx7BkL85I/maxresdefault.jpg,,1090,3,0,0,0
415,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Cj96rlReskE,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 635,"Title: Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining Authors: Kazuya Nakagawa*, Nagoya Institute of Technology Shinya Suzumura, Nagoya Institute of Technology Masayuki Karasuyama, Nagoya Institute of Technology Koji Tsuda, University of Tokyo Ichiro Takeuchi, Nagoya Institute of Technology Japan Abstract: In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pattern pruning (SPP) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe feature screening. In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demonstrate its computational advantage. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M37S,157,2d,hd,FALSE,https://i.ytimg.com/vi/Cj96rlReskE/maxresdefault.jpg,,232,0,0,0,0
416,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Hs865lXkDe0,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 848,"Title: The Profile of an Online Purchaser: A Case Study of Pinterest Authors: Caroline Lo*, Stanford University Dan Frankowski, Pinterest Jure Leskovec, Stanford University Abstract: Online e-commerce applications are becoming a primary vehicle for people to find, compare, and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to characterize, understand, and model user long-term purchasing intent, which is important as it allows for personalized and context relevant e-commerce services. In this paper we study user activity and purchasing behavior with the goal of building models of time-varying user purchasing intent. We analyze the purchasing behavior of nearly three million Pinterest users to determine short-term and long-term signals in user behavior that indicate higher purchase intent. We find that users with long-term purchasing intent tend to save and clickthrough on more content. However, as users approach the time of purchase their activity becomes more topically focused and actions shift from saves to searches. We further find that purchase signals in online behavior can exist weeks before a purchase is made and can also be traced across different purchase categories. Finally, we synthesize these insights in predictive models of user purchasing intent. Taken together, our work identifies a set of general principles and signals that can be used to model user purchasing intent across many content discovery applications. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M24S,144,2d,hd,FALSE,https://i.ytimg.com/vi/Hs865lXkDe0/maxresdefault.jpg,,308,0,0,0,0
417,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,PFJ_-StN3_4,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 634,"Title: Probabilistic Robust Route Recovery with Spatio-Temporal Dynamics Authors: Hao Wu*, Fudan University Jiangyun Mao, Fudan University Weiwei Sun, Fudan University Baihua Zheng, Singapore Management University Hanyuan Zhang, Fudan University Ziyang Chen, Fudan University Wei Wang, Fudan University Abstract: Vehicle trajectories are one of the most important data in Location-Based Services. The quality of trajectories directly affects the ser- vices. However, in the real applications, trajectory data are not always sampled densely. In this paper, we study the problem of recovering the entire route between two distant consecutive locations in a trajectory. We claim that solving the problem in a data-driven and probabilistic way is more suitable for this issue if data sparsity is well handled. Most existing works solve the problem without using those informative historical data or solve it in an empirical way. We propose a novel route recovery system in a fully probabilistic way with incorporating temporal and spatial dynamics together and addressed all the data sparsity problem brought by the probabilistic method. It outperforms the existing works maintaining a high accuracy (over 80%) and shows a strong robustness even when the length of routes to be recovered is quite long (about 30 road segments) or the data is quite sparse. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net",22,People & Blogs,PT2M57S,177,2d,sd,FALSE,https://i.ytimg.com/vi/PFJ_-StN3_4/maxresdefault.jpg,,197,0,0,0,0
418,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YRU36Vd2TBk,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 330,"Title: Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns Authors: Roel Bertens*, Utrecht University Jilles Vreeken, Max Planck Institute for Informatics and Saarland University Arno Siebes, Utrecht University Abstract: We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover high-quality pattern sets directly from data, we introduce DITTO, a highly efficient algorithm that approximates the ideal result very well. Experiments show that DITTO correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, DITTO discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M34S,94,2d,hd,FALSE,https://i.ytimg.com/vi/YRU36Vd2TBk/maxresdefault.jpg,,1000,28,0,0,5
419,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nWcrJVze5AI,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 28,"Title: Bid-aware Gradient Descent for Unbiased Learning with Censored Data in Display Advertising Authors: Weinan Zhang, University College London Tianxiong Zhou, TukMob Jun Wang, University College London Jian Xu, TouchPal Inc Abstract: In real-time display advertising, ad slots are sold per impression via an auction mechanism. For an advertiser, the campaign information is incomplete—- the user responses (e.g, clicks or conversions) and the market price of each ad impression are observed only if the advertiser’s bid had won the corresponding ad auction. Despite the predictions, such as bid landscape forecasting, click-through rate (CTR) estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data, the training data is, however, gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and testing. However, little study has been done on how to obtain the weights independent of previous bidding strategies and consequently integrate them into the final CTR prediction and bid generation steps. In this paper, we formulate CTR estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient Descents (BGD) which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solution. The learning framework has been deployed on Yahoo!‘s real-time bidding platform and provided 2.97% AUC lift for CTR estimation and 9.30% eCPC drop for bid optimisation in an online A/B test. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M4S,124,2d,hd,FALSE,https://i.ytimg.com/vi/nWcrJVze5AI/maxresdefault.jpg,,283,0,0,0,0
420,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pYPL2T41JIk,2016-06-30T17:20:25Z,30/6/16 17:20,KDD2016 paper 319,"Title: Parallel Dual Coordinate Descent Method for Large-scale Linear Classification in Multi-core Environments Authors: Wei-Lin Chiang, National Taiwan University Mu-Chu Lee, National Taiwan University Chih-Jen Lin*, National Taiwan University Abstract: Dual coordinate descent method is one of the most effective approaches for large-scale linear classification. However, its sequential design makes the parallelization difficult. In this work, we target at the parallelization in a multi-core environment. After pointing out difficulties faced in some existing approaches, we propose a new framework to parallelize the dual coordinate descent method. The key idea is to make the majority of all operations (gradient calculation here) parallelizable. The proposed framework is shown to be theoretically sound. Further, we demonstrate through experiments that the new framework is robust and efficient in a multi-core environment. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M3S,183,2d,hd,FALSE,https://i.ytimg.com/vi/pYPL2T41JIk/maxresdefault.jpg,,211,0,0,0,0
421,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,1cPGqaz7y9E,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 617,"Title: A Text Clustering Algorithm Using an Online Clustering Scheme for Initialization Authors: Jianhua Yin*, Tsinghua University Jianyong Wang, Tsinghua University Abstract: In this paper, we propose a text clustering algorithm using an online clustering scheme for initialization called FGSDMM+. FGSDMM+ assumes that there are at most $K_{max}$ clusters in the corpus, and regards these $K_{max}$ potential clusters as one large potential cluster at the beginning. During initialization, FGSDMM+ processes the documents one by one in an online clustering scheme. The first document will choose the potential cluster, and FGSDMM+ will create a new cluster to store this document. Later documents will choose one of the non-empty clusters or the potential cluster with probabilities derived from the Dirichlet multinomial mixture model. Each time a document chooses the potential cluster, FGSDMM+ will create a new cluster to store that document and decrease the probability of later documents choosing the potential cluster. After initialization, FGSDMM+ will run a collapsed Gibbs sampling algorithm several times to obtain the final clustering result. Our extensive experimental study shows that FGSDMM+ can achieve better performance than three other clustering methods on both short and long text datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M25S,145,2d,hd,FALSE,https://i.ytimg.com/vi/1cPGqaz7y9E/maxresdefault.jpg,,173,0,0,0,0
422,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,4dhga_AuMcc,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 311,"Title: Efficient Shift-Invariant Dictionary Learning Authors: Guoqing Zheng*, Carnegie Mellon University Yiming Yang, Carnegie Mellon University Jaime Carbonell, Carnegie Mellon University Abstract: Shift-invariant dictionary learning (SIDL) refers to the problem of discovering a latent basis (the dictionary) that captures informative local patterns at different locations of input sequences, and a sparse coding for each sequence as a linear combination of the latent basis elements. It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors, where the focus is on global patterns instead of shift-invariant local patterns. Unsupervised discovery of shift-invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large, and the number of possible linear combinations of such local patterns is even more so. In this paper we propose a new framework for unsupervised discovery of both the shift-invariant basis and the sparse coding of input data, with efficient algorithms for tractable computation. Empirical evaluations on multiple time series data sets demonstrate the effectiveness and efficiency of the proposed method. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M25S,145,2d,hd,FALSE,https://i.ytimg.com/vi/4dhga_AuMcc/maxresdefault.jpg,,181,1,0,0,0
423,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,NPCbsDrAqTo,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 14,"Title: NetCycle: Collective Evolution Inference in Heterogeneous Information Networks Authors: Yizhou Zhang, Fudan University Xiong Yun, Fudan University Xiangnan Kong, Worcester Polytechnic Institute Yangyong Zhu, Fudan University Abstract: Collective inference has become an active research topic in the last decade, where the response variables on a group of instances are correlated and should be inferred collectively, instead of independently. Previous works on collective inference mainly focus on exploiting the autocorrelation among static variables in networks. There are also many approaches on time series analysis, which exploit the autocorrelation in time sequence data, i.e., the cross-correlation of each response variable at different time points. However, in many real-world applications, the response variables of instances are not static, but dynamically evolving over time; their evolution are not following a static correlation through different time points, but following a life cycle. In this paper, we study the problem, called collective evolution inference, where the goal is to predict the values of the target variables for a collection of instances at the end of their life cycles. This problem is very important in many real-world applications, e.g., fund-raising result predictions in crowdfunding and gene-expression predictions in bioinformatics. This problem is challenging because different instances in the network can co-evolve over time and they can be at different stages of their life cycles and thus have different evolving trends. Moreover, the networks in collective evolution inference problems are usually heterogeneous networks, which involve multiple types of nodes interconnected by multiple types of links. We propose an approach, called NetCycle, to this problem by incorporating information from both the correlation among related instances and their life cycles. We compared our approach with existing methods of collective inference and time sequence analysis on two real-world networks. The results demonstrate that our proposed approach can improve the performance of evolution inference by considering the autocorrelation through networks and the life cycles. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M53S,173,2d,hd,FALSE,https://i.ytimg.com/vi/NPCbsDrAqTo/maxresdefault.jpg,,714,1,0,0,0
424,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Ua3n7Wp8Ibw,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 17,"Title: Targeted Topic Modeling for Focused Analysis Authors: Shuai Wang, University of Illinois at Chicago Zhiyuan Chen, University of Illinois at Chicago Geli Fei, Univ of Illinois at Chicago Bing Liu, Univ of Illinois at Chicago Sherry Emery, University of Illinois at Chicago Abstract: One of the overarching tasks of document analysis is to find what people talk about. One of the main techniques for the purpose is topic modeling. So far many models have been proposed. However, the existing models typically perform full analysis on the whole data to find all topics. This is certainly useful, but in practice we also found that the user almost always also wants to perform more detailed analysis on some specific aspects (which we refer to as targets). Current full-analysis models are not suitable for such analyses as their output topics are often too coarse and may not even be on target. For example, given a set of tweets about e-cigarette, the user wants to find out what sub-topics are discussed that are specifically related to children. Likewise, given a collection of online reviews about camera, the user (consumer or manufacturer) is interested in the aspect screen and desires to find out its aspect-specific sub-topics. As we will see in our experiments, current topic models are ineffective for such targeted analyses. This paper proposes a novel targeted topic model (TTM) to enable such focused analyses on any specific aspect of interest. Our experimental results demonstrate the effectiveness of the TTM model. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M59S,179,2d,hd,FALSE,https://i.ytimg.com/vi/Ua3n7Wp8Ibw/maxresdefault.jpg,,238,0,0,0,0
425,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,b5oQqDC39HU,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 843,"Title: Latent Space Model for Road Networks to Predict Time-Varying Traffic Authors: Dingxiong Deng, University of Southern California Cyrus Shahabi, University of Southern California Ugur Demiryurek, University of Southern California Linhong Zhu, University of Southern California Rose Yu, University of Southern California Yan Liu, University of Southern California Abstract: Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM’s. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M45S,105,2d,hd,FALSE,https://i.ytimg.com/vi/b5oQqDC39HU/maxresdefault.jpg,,383,2,0,0,0
426,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,eiA74XXGSF8,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 832,"Title: Identifying Police Officers at Risk of Adverse Events Authors: Samuel Carton, University of Michigan Jennifer Helsby*, University of Chicago Kenneth Joseph, Carnegie Mellon University Ayesha Mahmud, Princeton University Youngsoo Park, University of Arizona Joe Walsh, University of Chicago Crystal Cody, Charlotte-Mecklenburg Police Department Estella Patterson, Charlotte-Mecklenburg Police Department Lauren Haynes, University of Chicago Rayid Ghani, University of Chicago Abstract: Adverse events between police and the public can cause serious and sometimes deadly harm, damage police legitimacy, and result in costly litigation. These events can be prevented by targeting interventions based on an Early Intervention System (EIS) that flags officers who are at a high risk for adverse events. Today, these systems are not data-driven and rely on simple thresholds based on expert intuition. In this paper, we describe our work with the Charlotte-Mecklenburg Police Department (CMPD) that uses machine learning to predict which officers will have an adverse event. This approach increases true positives by 15% and decreases false positives by 55%. Our work also sheds light on features related to officer characteristics, situational factors, and neighborhood factors that are predictive of adverse events. This work provides a starting point for departments that want to take a comprehensive data-driven approach to improve policing and reduce harms to both officers and members of the public. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT5M4S,304,2d,hd,FALSE,https://i.ytimg.com/vi/eiA74XXGSF8/maxresdefault.jpg,,196,0,0,0,0
427,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pUmSWIvbGUo,2016-06-30T17:20:24Z,30/6/16 17:20,KDD2016 paper 619,"Title: A Real Linear and Parallel Multiple Longest Common Subsequences (MLCS) Algorithm Authors: Yanni Li, Xidian University Hui Li*, Xidian University Tihua Duan, Shanghai Finance University Sheng Wang, Coventry University Zhi Wang, Xidian University Yang Cheng, Xidian University Abstract: Information in various applications is always expressed as character sequences over a finite alphabet (e.g., DNA or gene sequences). In big data era, the lengths and sizes of these sequences are growing explosively, leading to grand challenges for the classical NP-hard problem, namely searching for the Multiple Longest Common Subsequences (MLCS) from multiple sequences. In this paper, we first unveil the fact that the leading dominant-point-based MLCS algorithms are unable to be applied to long and large-scale sequences alignments. To overcome their defects and tackle the longer and large-scale or even big sequences alignments, based on the proposed novel problem-solving model and various strategies, e.g., parallel topological sorting, optimal calculating, reuse of intermediate results, subsection calculation and serialization, etc., we present a novel parallel MLCS algorithm. Exhaustive experiments on the datasets of both synthetic and real-world biological sequences demonstrate that both the time and space of the proposed algorithm are only linear to the number of dominants from aligned sequences, and the proposed algorithm significantly outperforms the state-of-the-art dominant-point-based MLCS algorithms, being applicable to the longer and large-scale sequences alignments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT5M17S,317,2d,hd,FALSE,https://i.ytimg.com/vi/pUmSWIvbGUo/maxresdefault.jpg,,159,1,0,0,0
428,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 575,"Title: Dynamic and Robust Wildfire Risk Prediction System: An Unsupervised Approach Authors: Mahsa Salehi*, IBM Australia Laura Rusu, IBM Research Timothy Lynar, IBM Research Anna Phan, IBM Research Abstract: Ability to predict the risk of damaging weather events (e.g. wildfires) is crucial in helping emergency services in their decision-making processes, to mitigate and reduce the severity of such events. Today, wildfire rating systems have been in operation extensively in many countries around the world to estimate the danger of wildfires. In this paper we propose a data-driven approach to predict the wildfire risk. We show how we address the inherent challenges of such an approach that arise mainly due to the temporal dynamicity of weather data. Weather observations naturally change in time, with finer-scale variation (e.g. stationary day or stationary night) or large variations (non-stationary day or night), and this determines a temporal variation of the predicted fire danger. We show how our dynamic wildfire danger prediction model addresses the aforementioned challenges using context-based anomaly detection techniques and can be customized to different regions. We call our predictive model a Context-Based Fire Risk (CBFR) model. The advantage of our model is that it maintains multiple historical models for different temporal variations (e.g. day versus night), and use ensemble learning techniques to predict wildfire risk with high accuracy. In addition, it is completely unsupervised and does not rely on expert knowledge, which makes it flexible and easy to be applied in any region of interest. Our CBFR model is also scalable and can potentially be parallelized to speed up computation. Considering multiple wildfires (a.k.a. bushfires in Australia) locations in the Blue Mountains 2013 bushfires, Australia as a case study, we have compared the results of our system with the existing well-established Australian bushfire rating system. The experimental results show that our predictive model has a substantially higher accuracy in predicting the fire risk, which makes it an effective model to supplement the operational Australian bushfire rating system. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M39S,219,2d,hd,FALSE,https://i.ytimg.com/vi/-Qxhu6Qxpdc/maxresdefault.jpg,,4344,154,3,0,5
429,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,UR_f2rmMJkk,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 303,"Title: Multi-layer Representation Learning for Medical Concepts Authors: Edward Choi*, Georgia Institute of Technology Mohammad Taha Bahador, Georgia Institute of Technology Elizabeth Searles, Children Healthcare of Atlanta Catherine Coffey, Children Healthcare of Atlanta Jimeng Sun, Georgia Institute of Technology Abstract: Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M15S,195,2d,hd,FALSE,https://i.ytimg.com/vi/UR_f2rmMJkk/maxresdefault.jpg,,2772,15,0,0,1
430,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Vbaf9yJ6HBc,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 819,"Title: DopeLearning: A Computational Approach to Rap Lyrics Generation Authors: Eric Malmi*, Aalto University Pyry Takala, Aalto University Hannu Toivonen, University of Helsinki Tapani Raiko, Aalto University Aristides Gionis, Aalto University Abstract: Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M20S,140,2d,hd,FALSE,https://i.ytimg.com/vi/Vbaf9yJ6HBc/maxresdefault.jpg,,24516,48,2,0,3
431,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,WaZ0EL3E7XY,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 295,"Title: Contextual Intent Tracking for Personal Assistants Authors: Yu Sun*, University of Melbourne Nicholas Jing Yuan, Microsoft Research Yingzi Wang, Microsoft Research Xing Xie, Microsoft Research Kieran McDonald, Microsoft Corporation Rui Zhang, University of Melbourne Abstract: A new paradigm of recommendation is emerging in intelligent personal assistants such as Apple’s Siri, Google Now, and Microsoft Cortana, which recommends “the right information at the right time” and proactively helps you “get things done”. This type of recommendation requires precisely tracking users’ contemporaneous intent, i.e., what type of information (e.g., weather, stock prices) users currently intend to know, and what tasks (e.g., playing music, getting taxis) they intend to do. Users’ intent is closely related to context, which includes both external environments such as time and location, and users’ internal activities that can be sensed by personal assistants. The relationship between context and intent exhibits complicated co-occurring and sequential correlation, and contextual signals are also heterogeneous and sparse, which makes modeling the context-intent relationship a challenging task. To solve the intent tracking problem, we propose the Kalman filer regularized PARAFAC2 (KP2) nowcasting model, which compactly represents the structure and co-movement of context and intent. The KP2 model utilizes collaborative capabilities among users, and learns for each user a personalized dynamic system that enables efficient nowcasting of users’ intent. Extensive experiments using real-world data sets from a commercial personal assistant show that the KP2 model significantly outperforms various methods, and provides inspiring implications for deploying large-scale proactive recommendation systems in personal assistants. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,sd,FALSE,,,1726,49,0,0,0
432,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aBC_9mpFXGQ,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 574,"Title: Singapore in Motion: Insights on Public Transport Service Level Through Farecard and Mobile Data Analytics Authors: Hasan Poonawala*, IBM Vinay Kolar, IBM Sebastien Blandin, IBM Laura Wynter, IBM Sambit Sahu, IBM Abstract: Given the changing dynamics of mobility patterns and rapid growth of cities, transport agencies seek to respond more rapidly to needs of the public with the goal of offering an effective and competitive public transport system. A more data-centric approach for transport planning is part of the evolution of this process. In particular, the vast penetration of mobile phones provides an opportunity to monitor and derive insights on transport usage. Real time and historical analyses of such data can give a detailed understanding of mobility patterns of people and also suggest improvements to current transit systems. On its own, however, mobile geolocation data has a number of limitations. We thus propose a joint telco-and-farecard-based learning approach to understanding urban mobility. The approach enhances telecommunications data by leveraging it jointly with other sources of real-time data. The approach is illustrated on the first- and last-mile problem as well as route choice estimation within a densely-connected train network. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M39S,159,2d,hd,FALSE,https://i.ytimg.com/vi/aBC_9mpFXGQ/maxresdefault.jpg,,303,0,0,0,0
433,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,pXQuz7poFY4,2016-06-30T17:20:23Z,30/6/16 17:20,KDD2016 paper 8,"Title: Skinny-dip: Clustering in a Sea of Noise Authors: Samuel Maurus, Helmholtz Zentrum München Claudia Plant, Helmholtz Zentrum München Abstract: Can we find heterogeneous clusters hidden in data sets with 80% noise? Although such settings occur in the real-world, we struggle to find methods from the abundance of clustering techniques that perform well with noise at this level. Indeed, perhaps this is enough of a departure from classical clustering to warrant its study as a separate problem. In this paper we present SkinnyDip which, based on Hartigan. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M33S,153,2d,hd,FALSE,https://i.ytimg.com/vi/pXQuz7poFY4/maxresdefault.jpg,,889,0,0,0,0
434,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 290,"Title: Domain Adaptation in the Absence of Source Domain Data Authors: Boris Chidlovskii*, XRCE Stephane Clinchant, Xerox Research Centre Europe Gabriela Csurka, Xerox Research Centre Europe Abstract: Most of the existing domain adaptation methods make an assumption of freely available source domain data. An equal access to both source and target data makes possible measuring the discrepancy between their distributions and building representations common to both target and sources. In reality, such a simplifying assumption rarely holds, for source data are routinely a subject of legal, technical and contractual constraints and obligations between data owners and data customers. More common are situations where not source domain data but their decision making procedures are available for the adaptation. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other industrial applications, the owner of a source data is allowed to share not the entire sets, but some representative source examples such as class means. In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is limited to classification rules or a few representative examples. We adopt the recent techniques of {\it feature corruption} and their {\it marginalization}, both in supervised (the Marginalized Corrupted Features\cite{maaten2013}) and unsupervised (marginalized Stacked Denoising Autoencoder\cite{chen12}) settings. We test and compare them on customer and publicly available source datasets. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M4S,124,2d,sd,FALSE,https://i.ytimg.com/vi/-gJTjrRkIvs/maxresdefault.jpg,,128,0,0,0,0
435,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,WbSJiBjSVDA,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 567,"Title: Graph Wavelets via Sparse Cuts Authors: Arlei Lopes da Silva*, University of California, Santa Barbara Xuan-Hong Dang, University of California, Santa Barbara Prithwish Basu, Raytheon BBN Technologies Ambuj Singh, University of California, Santa Barbara Ananthram Swami, Army Lab Abstract: Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Processing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a both compact and accurate manner. However, how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open question. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an efficient heuristic. We formulate the basis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regularized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal, producing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene networks) and outperforming the best baseline by up to 8 times. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M,180,2d,hd,FALSE,https://i.ytimg.com/vi/WbSJiBjSVDA/maxresdefault.jpg,,866,3,0,0,0
436,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XmetwslSNGY,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 3,"Title: Bayesian Inference of Arrival Rate and Substitution Behavior from Sales Transaction Data with Stockouts Authors: Benjamin Letham Lydia M. Letham Cynthia Rudin Abstract: When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some customers leave with no purchase while others substitute alternative products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/ When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some customers leave with no purchase while others substitute alternative products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales.",22,People & Blogs,PT2M11S,131,2d,hd,FALSE,https://i.ytimg.com/vi/XmetwslSNGY/maxresdefault.jpg,,538,2,0,0,0
437,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,btddscklGu4,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 808,"Title: A Multi-Task Learning Formulation for Survival Analysis Authors: Yan Li*, Wayne State University Jie Wang, University of Michigan at Ann Arbor Jieping Ye, University of Michigan at Ann Arbor Chandan Reddy, Wayne State University Abstract: Predicting the occurrence of a particular event of interest at future time points is the primary goal of survival analysis. The presence of incomplete observations due to time limitations or loss of data traces is known as censoring which brings unique challenges in this domain and differentiates survival analysis from other standard regression methods. The popularly used survival analysis methods such as Cox proportional hazard model and parametric survival regression suffer from some strict assumptions and hypotheses that are not realistic in most of the real-world applications. To overcome the weaknesses of these two types of methods, in this paper, we reformulate the survival analysis problem as a multi-task learning problem and propose a new multi-task learning based formulation to predict the survival time by estimating the survival status at each time interval during the study duration. We propose an indicator matrix to enable the multi-task learning algorithm to handle censored instances and incorporate some of the important characteristics of survival problems such as non-negative non-increasing list structure into our model through max-heap projection. We employ the l2,1-norm penalty which enables the model to learn a shared representation across related tasks and hence select important features and alleviate over-fitting in high-dimensional feature spaces; thus, reducing the prediction error of each task. To efficiently handle the two non-smooth constraints, in this paper, we propose an optimization method which employs Alternating Direction Method of Multipliers (ADMM) algorithm to solve the proposed multi-task learning problem. We demonstrate the performance of the proposed method using real-world microarray gene expression high-dimensional benchmark datasets and show that our method outperforms state-of-the-art methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M8S,128,2d,hd,FALSE,https://i.ytimg.com/vi/btddscklGu4/maxresdefault.jpg,,290,0,0,0,0
438,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,dN9Wqn0tuZ4,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 289,"Title: DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks Authors: Shuangfei Zhai*, Binghamton University Keng-hao Chang, Microsoft Ruofei Zhang, Microsoft Zhongfei Zhang, Binghamton University Abstract: In this paper, we investigate the use of recurrent neural networks (RNNs) in the context of online advertising. We use RNNs to map both queries and ads to real valued vectors, with which the relevance of a given (query, ad) pair can be easily computed. On top of the RNN, we propose a novel attention network, which learns to assign attention scores to different word locations according to their intent importance (hence the name DeepIntent). The vector output of a sequence is thus computed by a weighted sum of the hidden states of the RNN at each word according their attention scores. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs, which are sampled from a commercial search engine. We show that in most cases the attention network improves the quality of learned vector representations, evaluated by AUC on a manually labeled dataset. Moreover, we highlight the effectiveness of the learned attention scores from two aspects: query rewriting and a modified BM25 metric. We show that using the learned attention scores, one is able to produce sub-queries that are of better qualities than those of the state-of-the-art methods. Also, by modifying the term frequency with the attention scores in a standard BM25 formula, one is able to improve its performance evaluated by AUC. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M25S,145,2d,hd,FALSE,https://i.ytimg.com/vi/dN9Wqn0tuZ4/maxresdefault.jpg,,555,3,1,0,1
439,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,h_w8RrHZI88,2016-06-30T17:20:22Z,30/6/16 17:20,KDD2016 paper 813,"Title: Lossless Separation of Web Pages into Layout Code and Data Authors: Adi Omari*, Technion Benny Kimelfeld, Technion Sharon Shoham, Academic College of Tel Aviv Yaffo Eran Yahav, Technion Abstract: A modern web page is often served by running layout code on data, producing an HTML document that enhances the data with front/back matters and layout/style operations. In this paper, we consider the opposite task: separating a given web page into a data component and a layout program. This separation has various important applications: page encoding may be significantly more compact (reducing web traffic), data representation is normalized across web designs (facilitating wrapping, retrieval and extraction), and repetitions are diminished (expediting site updates and redesign). We present a framework for defining the separation task, and devise an algorithm for synthesizing layout code from a web page while distilling its data in a lossless manner. The main idea is to synthesize layout code hierarchically for parts of the page, and use a combined program-data representation cost to decide whether to align intermediate programs. When intermediate programs are aligned, they are transformed into a single program, possibly with loops and conditionals. At the same time, differences between the aligned programs are captured by the data component such that executing the layout code on the data results in the original page. We have implemented our approach and conducted a thorough experimental study of its effectiveness. Our experiments show that our approach features state of the art (and higher) performance in both size compression and record extraction. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M24S,144,2d,hd,FALSE,https://i.ytimg.com/vi/h_w8RrHZI88/maxresdefault.jpg,,906,17,0,0,0
440,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,25Vlh4odxJs,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 801,"Title: Robust Large-Scale Machine Learning in the Cloud Authors: Steffen Rendle*, Google, Inc. Dennis Fetterly, Google, Inc. Eugene Shekita, Google, Inc. Bor-Yiing Su, Google, Inc. Abstract: The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD’s cost effectiveness and scalability. Using Google’s internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the ‘large-scale’ Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M32S,152,2d,hd,FALSE,https://i.ytimg.com/vi/25Vlh4odxJs/maxresdefault.jpg,,571,1,0,0,1
441,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,6TtVOgTIA0M,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 881,"Title: Ranking Universities Based on Career Outcomes of Graduates Authors: Navneet Kapur, GoFundMe Nikita Lytkin, LinkedIn Corporation Bee-Chung Chen, LinkedIn Corporation Deepak Agarwal, LinkedIn Corporation Igor Perisic, LinkedIn Corporation Abstract: Every year, millions of new students enter higher educational programs. Publicly available rankings of academic programs play a key role in prospective students’ decisions regarding which universities to apply to and enroll in. While surveys indicate that majority of freshmen enter college to get good jobs after graduation, established methodologies for ranking universities rely on indirect indicators of career outcomes such as reputational assessments of the universities among academic peers, acceptance and graduation rates, learning environment, and availability of research funding. In addition, many of these methodologies rely on arbitrary choices of weighting factors for the different ranking indicators, and suffer from lack of analyses of statistical stability. In this paper, we addresses these challenges holistically by developing a novel methodology for ranking and recommending universities for different professions on the basis of career outcomes of professionals who graduated from those schools. Our methodology incorporates a number of techniques for achieving statistical stability, and represents a step towards personalized educational recommendations based on interests and ambitions of individuals. We have applied this methodology on LinkedIn’s Economic Graph data of over 400 million professional from around the world. The resulting university rankings have been made available to the public and demonstrate that there are valuable insights to be gleaned from professional career data on LinkedIn. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/6TtVOgTIA0M/maxresdefault.jpg,,424,2,0,0,0
442,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,98yMCJ9P7ao,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 287,"Title: Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding Authors: Xiang Ren*, University of Illinois at Urbana-Champaign Wenqi He, University of Illinois at Urbana-Champaign Meng Qu,University of Illinois at Urbana-Champaign Heng Ji, Rensselaer Polytechnic Institute Clare Voss, Army Research Laboratory Jiawei Han, University of Illinois at Urbana-Champaign Abstract: Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy (i.e., incorrect for the entity mention’s local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples, given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE, to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations. Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M56S,176,2d,hd,FALSE,https://i.ytimg.com/vi/98yMCJ9P7ao/maxresdefault.jpg,,563,2,1,0,0
443,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BC3toqSsZg8,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 553,"Title: Rebalancing Bike Sharing Systems: A Multi-source Data Smart Optimization Authors: Junming Liu, Rutgers University Leilei Sun, Dalian University of Technology Hui Xiong*, Rutgers University Weiwei Chen, Rutgers University Abstract: Bike sharing systems, aiming at providing the missing links in public transportation systems, are becoming popular in urban cities. A key to success for a bike sharing systems is the effectiveness of rebalancing operations, that is, the efforts of restoring the number of bikes in each station to its target value by routing vehicles through pick-up and drop-off operations. There are two major issues for this bike rebalancing problem: the determination of station inventory target level and the large scale multiple capacitated vehicle routing optimization with outlier stations. The key challenges include demand prediction accuracy for inventory target level determination, and an effective optimizer for vehicle routing with hundreds of stations. To this end, in this paper, we develop a Meteorology Similarity Weighted K-Nearest-Neighbor (MSWK) regressor to predict the station pick-up demand based on large-scale historic trip records. Based on further analysis on the station network constructed by station-station connections and the trip duration, we propose an inter station bike transition (ISBT) model to predict the station drop-off demand. Then, we provide a mixed integer nonlinear programming (MINLP) formulation of multiple capacitated bike routing problem with the objective of minimizing total travel distance. To solve it, we propose an Adaptive Capacity Constrained K-centers Clustering (AdaCCKC) algorithm to separate outlier stations (the demands of these stations are very large and make the optimization infeasible) and group the rest stations into clusters within which one vehicle is scheduled to redistribute bikes between stations. In this way, the large scale multiple vehicle routing problem is reduced to inner cluster one vehicle routing problem with guaranteed feasible solutions. Finally, the extensive experimental results on the NYC Citi Bike system show the advantages of our approach for bike demand prediction and large-scale bike rebalancing optimization. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M38S,158,2d,hd,FALSE,https://i.ytimg.com/vi/BC3toqSsZg8/maxresdefault.jpg,,664,0,0,0,0
444,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BX8DGaAQLK4,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 463,"Title: ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages Authors: Matteo Riondato, Two Sigma Investments Eli Upfal, Brown University Abstract: We present ABRA, a suite of algorithms that compute and maintain probabilistically-guaranteed, high-quality, approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms rely on random sampling and their analysis leverages on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, this is the first application of these concepts to the field of graph analysis. The results of our experimental evaluation show that our approach is much faster than exact methods, and vastly outperforms, in both speed and number of samples, current state-of-the-art algorithms with the same quality guarantees. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M13S,133,2d,hd,FALSE,https://i.ytimg.com/vi/BX8DGaAQLK4/maxresdefault.jpg,,271,3,0,0,0
445,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Plk4-DcLJqc,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 250,"Title: Computational Drug Repositioning Using Continuous Self-controlled Case Series Authors: Zhaobin Kuang*, UW-Madison James Thomson, Morgridge Institute Michael Caldwell, Marshfield Clinic Peggy Peissig, Marshfield Clinic Ron Stewart, Morgridge Institutev Page David, University of Wisconsin Abstract: Computational Drug Repositioning (CDR) is the task of discovering potential new indications for existing drugs by mining large-scale heterogeneous drug-related data sources. Leveraging the patient-level temporal ordering information between numeric physiological measurements and various drug prescriptions provided in Electronic Health Records (EHRs), we propose a Continuous Self-controlled Case Series (CSCCS) model for CDR. As an initial evaluation, we look for drugs that can control Fasting Blood Glucose (FBG) level in our experiments. Applying CSCCS to the Marshfield Clinic EHR, well-known drugs that are indicated for controlling blood glucose level are rediscovered. Furthermore, some drugs with recent literature support for the potential effect of blood glucose level control are also identified. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M31S,151,2d,sd,FALSE,,,175,1,0,0,0
446,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TqlKnYlifvk,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 803,"Title: Scalable Pattern Matching over Compressed Graphs via Dedensification Authors: Antonio Maccioni*, Roma Tre University Daniel Abadi, Yale University Abstract: One of the most common operations on graph databases is graph pattern matching (e.g., graph isomorphism and more general types of “subgraph pattern matching”). In fact, in some graph query languages every single query is expressed as a graph matching operation. Consequently, there has been a significant amount of research effort in optimizing graph matching operations in graph database systems. As graph databases have scaled in recent years, so too has recent work on scaling graph matching operations. However, the performance of recent proposals for scaling graph pattern matching is limited by the presence of high-degree nodes. These high-degree nodes result in an explosion of intermediate result sizes during query execution, and therefore significant performance bottlenecks. In this paper we present a dedensification technique that losslessly compresses the neighborhood around high-degree nodes. Furthermore, we introduce a query processing technique that enables direct operation of graph query processing operations over the compressed data, without ever having to decompress the data. For pattern matching operations, we show how this technique can be implemented as a layer above existing graph database systems, so that the end-user can benefit from this technique without requiring modifications to the core graph database engine code. Our technique reduces the size of the intermediate result sets during query processing, and thereby improves query performance. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M33S,153,2d,hd,FALSE,https://i.ytimg.com/vi/TqlKnYlifvk/maxresdefault.jpg,,369,4,0,0,0
447,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VJidthkKv_0,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 1160,"Title: Predictors without Borders: Behavioral Modeling of Product Adoption in Three Developing Countries Authors: Muhammad Raza Khan, University of Washington Joshua Blumenstock*, University of Washington Abstract: Billions of people around the world live without access to banks or other formal financial institutions. In the past several years, many mobile operators have launched “Mobile Money” platforms that deliver basic financial services over the mobile phone network. While many believe that these services can improve the lives of the poor, in many countries adoption of Mobile Money still remains anemic. In this paper, we develop a predictive model of Mobile Money adoption that uses billions of mobile phone communications records to understand the behavioral determinants of adoption. We describe a novel approach to feature engineering that uses a Deterministic Finite Automaton to construct thousands of behavioral metrics of phone use from a concise set of recursive rules. These features provide the foundation for a predictive model that is tested on mobile phone operators logs from Ghana, Pakistan, and Zambia, three very different developing-country contexts. The results highlight the key correlates of Mobile Money use in each country, as well as the potential for such methods to predict and drive adoption. More generally, our analysis provides insight into the extent to which homogenized supervised learning methods can generalize across geographic contexts. We find that without careful tuning, a model that performs very well in one country frequently does not generalize to another. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M58S,178,2d,hd,FALSE,https://i.ytimg.com/vi/VJidthkKv_0/maxresdefault.jpg,,192,0,0,0,0
448,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,iiM5VOD0QA8,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 283,"Title: When Social Influence Meets Item Inference Authors: Hui-Ju Hung*, The Pennsylvania State University Hong-Han Shuai, Academia Sinica De-Nian Yang, Academia Sinica Liang-Hao Huang, Academia Sinica Wang-Chien Lee, The Pennsylvania State University Jian Pei, Simon Fraser University Ming-Syan Chen, National Taiwan University Abstract: Research issues and data mining techniques for product recommendation and viral marketing have been widely studied. Existing works on seed selection in social networks do not take into account the effect of product recommendations in e-commerce stores. In this paper, we investigate the seed selection problem for viral marketing that considers both effects of social influence and item inference (for product recommendation). We develop a new model, Social Item Graph (SIG), that captures both effects in form of hyperedges. Accordingly, we formulate a seed selection problem, called Social Item Maximization Problem (SIMP), and prove the hardness of SIMP. We design an efficient algorithm with performance guarantee, called Hyperedge-Aware Greedy (HAG), for SIMP and develop a new index structure, called SIG-index, to accelerate the computation of diffusion process in HAG. Moreover, to construct realistic SIG models for SIMP, we develop a statistical inference based framework to learn the weights of hyperedges from data. Finally, we perform a comprehensive evaluation on our proposals with various baselines. Experimental result validates our ideas and demonstrates the effectiveness and efficiency of the proposed model and algorithms over baselines. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M18S,138,2d,hd,FALSE,https://i.ytimg.com/vi/iiM5VOD0QA8/maxresdefault.jpg,,2309,15,0,0,0
449,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,j2s4zoQzyCY,2016-06-30T17:20:21Z,30/6/16 17:20,KDD2016 paper 548,"Title: Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations Authors: Bo Kang*, Ghent University Jefrey Lijffijt, Ghent University Raul Santos-Rodriguez, University of Bristol Tijl De Bie, Ghen University Abstract: Methods that find insightful low-dimensional projections are essential to effectively explore high-dimensional data. Principal Component Analysis is used pervasively to find low-dimensional projections, not only because it is straightforward to use, but it is also often effective, because the variance in data is often dominated by relevant structure. However, even if the projections highlight real structure in the data, not all structure is interesting to every user. If a user is already aware of, or not interested in the dominant structure, Principal Component Analysis is less effective for finding interesting components. We introduce a new method called Subjectively Interesting Component Analysis (SICA), designed to find data projections that are \emph{subjectively interesting}, i.e, projections that truly surprise the end-user. It is rooted in information theory and employs an explicit model of a user’s prior expectations about the data. The corresponding optimization problem is a simple eigenvalue problem, and the result is a trade-off between explained variance and novelty. We present five case studies on synthetic data, images, time-series, and spatial data, to illustrate how SICA enables users to find (subjectively) interesting projections. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M24S,144,2d,hd,FALSE,https://i.ytimg.com/vi/j2s4zoQzyCY/maxresdefault.jpg,,164,0,0,0,0
450,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:20Z,30/6/16 17:20,KDD2016 paper 448,"Title: Point-of-Interest Recommendations: Learning Potential Check-ins from Friends Authors: Huayu Li, University of North Carolina at Charlotte Yong Ge, University of North Carolina at Charlotte Hengshu Zhu, Baidu Inc. Abstract: The emergence of Location-based Social Network (LBSN) services provides a wonderful opportunity to build personalized Point-of-Interest (POI) recommender systems. Although a personalized POI recommender system can significantly facilitate users’ outdoor activities, it faces many challenging problems, such as the hardness to model user’s POI decision making process and the difficulty to address data sparsity and user/location cold-start problem. To cope with these challenges, we define three types of friends (i.e., social friends, location friends, and neighboring friends) in LBSN, and develop a two-step framework to leverage the information of friends to improve POI recommendation accuracy and address cold-start problem. Specifically, we first propose to learn a set of potential locations that each individual’s friends have checked-in before and this individual is most interested in. Then we incorporate three types of check-ins (i.e., observed check-ins, potential check-ins and other unobserved check-ins) into matrix factorization model using two different loss functions (i.e., the square error based loss and the ranking error based loss). To evaluate the proposed model, we conduct extensive experiments with many state-of-the-art baseline methods and evaluation metrics on two real-world data sets. The experimental results demonstrate the effectiveness of our methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M59S,179,2d,hd,FALSE,https://i.ytimg.com/vi/-FIOK2BaXA4/maxresdefault.jpg,,994,9,0,0,0
451,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,#NAME?,2016-06-30T17:20:20Z,30/6/16 17:20,KDD2016 paper 1156,"Title: EMBERS AutoGSR: Automated Coding of Civil Unrest Events Authors: Parang Saraf*, Virginia Tech Naren Ramakrishnan, Virginia Tech Abstract: We describe the EMBERS AutoGSR system that conducts automated coding of civil unrest events from news articles published in multiple languages. The nuts and bolts of the AutoGSR system constitute an ecosystem of filtering, ranking, and recommendation models to determine if an article reports a civil unrest event and, if so, proceed to identify and encode specific characteristics of the civil unrest event such as the when, where, who, and why of the protest. AutoGSR is a deployed system for the past 6 months continually processing data 24x7 in languages such as Spanish, Portugese, English and encoding civil unrest events in 10 countries of Latin America: Argentina, Brazil, Chile, Colombia, Ecuador, El Salvador, Mexico, Paraguay, Uruguay, and Venezuela. We demonstrate the superiority of AutoGSR over both manual approaches and other state-of-the-art encoding systems for civil unrest. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M38S,158,2d,hd,FALSE,https://i.ytimg.com/vi/-dEjiRtqKbA/maxresdefault.jpg,,5752,417,14,0,55
452,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BLJAw7E9WG8,2016-06-30T17:20:20Z,30/6/16 17:20,kdd2016 paper 245,"Title: Regime Shifts in Streams: Real-time Forecasting of Co-evolving Time Sequences Authors: Yasuko Matsubara*, Kumamoto University Yasushi Sakurai, Kumamoto University Abstract: Given a large, online stream of multiple co-evolving event sequences, such as sensor data and Web-click logs, that contains various types of non-linear dynamic evolving patterns of different durations, how can we efficiently and effectively capture important patterns? How do we go about forecasting long-term future events? In this paper, we present REGIMECAST, an efficient and effective method for forecasting co-evolving data streams. REGIMECAST is designed as an adaptive non-linear dynamical system, which is inspired by the concept of “regime shifts” in natural dynamical systems. Our method has the following properties: (a) Effective: it operates on large data streams, captures important patterns and performs long-term forecasting; (b) Adaptive: it automatically and incrementally recognizes the latent trends and dynamic evolution patterns (i.e., regimes) that are unknown in advance; (c) Scalable: it is fast and the computation cost does not depend on the length of data streams; (d) Any-time: it provides a response at any time and generates long-range future events. Extensive experiments on real datasets demonstrate that REGIME- CAST does indeed make long-range forecasts, and it outperforms state-of-the-art competitors as regards accuracy and speed. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M20S,140,2d,sd,FALSE,,,1661,7,0,0,0
453,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BZ9WObsLKaQ,2016-06-30T17:20:20Z,30/6/16 17:20,KDD2016 paper 520,"Title: Causal Clustering for 1-Factor Measurement Models Authors: Erich Kummerfeld*, University of Pittsburgh Joseph Ramsey, Carnegie Mellon University Abstract: Many scientific research programs aim to learn the causal structure of real world phenomena. This learning problem is made more difficult when the target of study cannot be directly observed. One strategy commonly used by social scientists is to create measurable ``indicator’’ variables that covary with the latent variables of interest. Before leveraging the indicator variables to learn about the latent variables, however, one needs a measurement model of the causal relations between the indicators and their corresponding latents. These measurement models are a special class of Bayesian networks. This paper addresses the problem of reliably inferring measurement models from measured indicators, without prior knowledge of the causal relations or the number of latent variables. We present a provably correct novel algorithm, FindOneFactorClusters (FOFC), for solving this inference problem. Compared to other state of the art algorithms, FOFC is faster, scales to larger sets of indicators, and is more reliable at small sample sizes. We also present the first correctness proofs for this problem that do not assume linearity or acyclicity among the latent variables. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M34S,154,2d,sd,FALSE,,,172,0,0,0,0
454,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,b-6eP-_0SF8,2016-06-30T17:20:20Z,30/6/16 17:20,KDD2016 paper 277,"Title: Distributing the Stochastic Gradient Sampler for Large-Scale LDA Authors: Yuan Yang*, Beihang University Jianfei Chen, Tsinghua University Jun Zhu, Tsinghua University Abstract: Learning large-scale Latent Dirichlet Allocation (LDA) models is bene?cial for many applications that involve large collections of documents. Recent work has been focusing on developing distributed algorithms in the batch setting, while leaving the stochastic methods behind, which can effectively explore statistical redundancy in big data and thereby are complementary to distributed computing. The distributed stochastic gradient Langevin dynamics (DSGLD) represents one attempt to combine stochastic sampling and distributed computing, but it suffers from drawbacks such as excessive communications and sensitivity to partitioning of datasets across nodes. DSGLD is typically limited to learn small models that have about 10^3 topics and 10^3 vocabulary size. In this paper, we present embarrassingly parallel SGLD (EPSGLD), a novel distributed scheme of stochastic sampling methods for topic models. Our sampler is built upon a divide-and-conquer architecture which enables us to produce robust and asymptotically exact samples with less communication overhead than DSGLD. We further propose several techniques to reduce the overhead in I/O and memory usage. Experiments on Wikipedia and ClueWeb12 documents demonstrate that EPSGLD can scale up to large models with 10^10 parameters (i.e., 10^5 topics, 10^5 vocabulary size), four orders of magnitude larger than DSGLD, and converge faster. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M27S,147,2d,hd,FALSE,https://i.ytimg.com/vi/b-6eP-_0SF8/maxresdefault.jpg,,134,0,0,0,0
455,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,vMlZwQZMwDs,2016-06-30T17:20:20Z,30/6/16 17:20,KDD2016 paper 798,"Title: Question Independent Grading using Machine Learning: The Case of Computer Program Grading Authors: Gursimran Singh*, Aspiring Minds Shashank Srikant, Aspiring Minds Varun Aggarwal, Aspiring Minds Abstract: Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no labeled samples for grading responses to a new, unseen question. We extend our previous work wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model across questions, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system’s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M11S,191,2d,hd,FALSE,https://i.ytimg.com/vi/vMlZwQZMwDs/maxresdefault.jpg,,4960,115,1,0,0
456,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,2iYpq7v1L7Q,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 1146,"Title: Approximate Personalized PageRank on Dynamic Graphs Authors: Hongyang Zhang*, Stanford University Peter Lofgren, Stanford University Abstract: We propose and analyze two algorithms for maintaining approximate Personalized PageRank (PPR) vectors on a dynamic graph, where edges are added or deleted. Our algorithms are natural dynamic versions of two known local variations of power iteration. One, Forward Push, propagates probability mass forwards along edges from a source node, while the other, Reverse Push, propagates local changes backwards along edges from a target. In both variations, we maintain an invariant between two vectors, and when an edge is updated, our algorithm first modifies the vectors to restore the invariant, then performs any needed local push operations to restore accuracy. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M36S,156,2d,hd,FALSE,https://i.ytimg.com/vi/2iYpq7v1L7Q/maxresdefault.jpg,,567,2,0,0,0
457,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,BPQqFIVNm-g,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 1142,"Title: Optimal Reserve Prices in Upstream Auctions: Empirical Application on Online Video Advertising Authors: Miguel Angel Alcobendas Lisbona*, Yahoo Inc. Kuang-chih Lee, Yahoo Inc. Sheide Chammas, Yahoo Inc. Abstract: We consider optimal reserve prices in BrightRoll Video Exchange when the inventory opportunity comes from other exchanges (downstream marketplaces). We show that the existence of downstream auctions impacts BrightRoll floors decision. Moreover, it renders the classical derivation of the optimal floor inadequate and sub-optimal. We derive the new downstream-corrected reserve price and compare its performance with respect to the existing floor and the classical optimal price. The downstream-corrected optimal reserve price proves superior to both. The relevance of this study transcends its particular context and is applicable to a wide range of scenarios where sequential auctions exist and where marketplaces interact with each other. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M2S,182,2d,hd,FALSE,https://i.ytimg.com/vi/BPQqFIVNm-g/maxresdefault.jpg,,219,0,0,0,0
458,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,CwK5Sz7blLI,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 514,"Title: From Online Behaviors to Offline Retailing Authors: Ping Luo, Chinese Academy of Sciences Su Yan, Chinese Academy of Sciences Zhiqiang Liu, Baidu, Inc. Zhiyong Shen, Baidu, Inc. Shengwen Yang, Baidu, Inc. Qing He, Chinese Academy of Sciences Abstract: To combat the ease of online shopping in pajamas, offline mall owners focus increasingly on driving satisfaction and improving retention by identifying customers’ preferences. However, most of these studies are based on customers’ offline consuming history only. Benefiting from the internet, we can also get customers’ online behaviors, such as the search logs, web browsing logs, online shopping logs, and so on. Might these seemingly irrelevant information from two different modalities (i.e. online and offline) be somehow interrelated? How can we make use of the online behaviors and offline actions jointly to promote recommendation for offline retailing? In this study, we formulate this task as a cross-modality recommendation problem, and present its solution via a proposed probabilistic graphical model, called Online-to-Offline Topic Modeling (O2OTM). Specifically, this method explicitly models the relationships between online and offline topics so that the likelihood of both online and offline behaviors is maximized. Then, the recommendation is made only based on the pairs of online and offline topics, denoted by (t,l), with high values of lift, such that the existence of the online topic t greatly increases the response on the corresponding offline topic l compared with the average response for the population without the online topic t. Furthermore, we evaluate this solution in both live and retrospect experiments. The real-world deployment of this model for the anniversary promotion campaign of a famous shopping mall in Beijing shows that our approach increases the occurred customer purchases per promotion message by 29.75% compared with the baseline. Also, our model finds some interesting interpretable relationships between the online search topics and offline brand topics. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M,180,2d,sd,FALSE,,,228,0,0,0,0
459,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,FBlhhebFhTI,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 519,"Title: A Multiple Test Correction for Streams and Cascades of Statistical Hypothesis Tests Authors: Francois Petitjean*, Monash University Geoff Webb, Monash University Abstract: Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, i.e. rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in a multiple comparison setting require the set of hypotheses to be pre-determined. This renders statistical testing virtually unusable for many modern applications of statistical inference such as model selection, because neither the set of hypotheses that will be tested, nor even the number of hypotheses, can be known in advance. This paper introduces a multiple-testing correction that can be used in applications for which there are repeated pools of null hypotheses from each which a single null hypothesis is to be rejected and neither the specific hypotheses nor their number are known in advance. To demonstrate the importance and relevance of this work to current machine learning problems, we further refine the theory to the problem of model selection and show how to use Stepwise Multiple Testing for learning graphical models. We assess its ability to discover graphical models on more than 7,000 datasets, studying the ability of Stepwise Multiple Testing to outperform the state of the art on data with varying size, dimensionality, as well as with varying density and power of the present correlations. Stepwise Multiple Testing provides a significant improvement in statistical efficiency, often requiring only half as much data to reach the same discovery, while strictly controlling FWER. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M33S,213,2d,hd,FALSE,https://i.ytimg.com/vi/FBlhhebFhTI/maxresdefault.jpg,,5254,47,5,0,0
460,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,IpzII7G6UBg,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 777,"Title: Online Asymmetric Active Learning with Imbalanced Data Authors: Xiaoxuan Zhang*, University of Iowa Tianbao Yang, University of Iowa Padmini Srinivasan, University of Iowa Abstract: This paper considers online learning with imbalanced streaming data under a query budget, where the act of querying for labels is constrained to a budget limit. In particular, we propose an asymmetric active querying strategy that assigns different probabilities for query to examples predicted as positive and negative. To corroborate the proposed asymmetric query model, we provide a theoretical analysis on a weighted mistake bound. We conduct extensive evaluations of the proposed asymmetric active querying strategy in comparison with several baseline querying strategies and with previous online learning algorithms for imbalanced data. In particular, we perform two types of evaluations according to which examples appear as “positive”/ “negative”. In push evaluation only the positive predictions given to the user are taken into account; in push and query evaluation the decision to query is also considered for evaluation. The push and query evaluation strategy is particularly suited for a recommendation setting because the items selected for querying for labels may go to the end-user to enable customization and personalization. These would not be shown any differently to the end-user compared to recommended content (i.e., the examples predicated as positive). Additionally, given our interest in imbalanced data we measure F-score instead of accuracy that is traditionally considered by online classification algorithms. We also compare the querying strategies on five classification tasks from different domains, and show that the probabilistic query strategy achieves higher F-scores on both types of evaluation than deterministic strategy, especially when the budget is small, and the asymmetric query model further improves performance. When compared to the state-of-art cost-sensitive online learning algorithm under a budget, our online classification algorithm with asymmetric querying achieves a higher F-score on four of the five tasks, especially on the push evaluation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M45S,165,2d,hd,FALSE,https://i.ytimg.com/vi/IpzII7G6UBg/maxresdefault.jpg,,206,0,0,0,0
461,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,dX3M1QNQh1c,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 793,"Title: Developing a Data-Driven Player Ranking Metric in Soccer Using Predictive Model Weights Authors: Joel Brooks*, Massachusetts Institute of Technology Matthew Kerr, Massachusetts Institute of Technology John Guttag, Massachusetts Institute of Technology Abstract: Quantitative evaluation of the ability of soccer players to contribute to team offensive performance is typically based on goals scored, assists made, and shots taken. In this paper, we describe a novel ranking system based entirely on the value of passes completed. This value is derived based on the relationship of pass locations in a possession and shot opportunities generated. This relationship is learned by applying a supervised machine learning model to pass locations in event data from the 2012-2013 La Liga season. Interestingly, though this metric is based entirely on passes, the derived player rankings are largely consistent with general perceptions of offensive ability, e.g., Messi and Ronaldo are near (but not at) the top. Additionally, when used to rank midfielders, it separates the more offensively-minded players from others. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M56S,176,2d,hd,FALSE,https://i.ytimg.com/vi/dX3M1QNQh1c/maxresdefault.jpg,,327,0,0,0,0
462,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hTN1QuiHu-I,2016-06-30T17:20:19Z,30/6/16 17:20,KDD2016 paper 276,"Title: PTE: Enumerating Trillion Triangles On Distributed Systems Authors: Ha-Myung Park*, KAIST Sung-Hyon Myaeng, KAIST U Kang, Seoul National University Abstract: How can we enumerate triangles from an enormous graph with billions of vertices and edges? Triangle enumeration is an important task for graph data analysis with many applications including identifying suspicious users in social networks, detecting web spams, finding communities, etc. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time. In this paper, we propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. Experimental results show that PTE provides up to 47 times faster performance than recent distributed algorithms on real world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges, which any previous triangle computation algorithm fail to process. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M2S,122,2d,hd,FALSE,https://i.ytimg.com/vi/hTN1QuiHu-I/maxresdefault.jpg,,310,0,0,0,0
463,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8dWM7loVUUc,2016-06-30T17:20:18Z,30/6/16 17:20,KDD2016 paper 775,"Title: Semi-Markov Switching Vector Autoregressive Model-Based Anomaly Detection in Aviation Systems Authors: Igor Melnyk*, University of Minnesota Arindam Banerjee, University of Minnesota Bryan Matthews, NASA Ames Research Center Nikunj Oza, NASA Ames Research Center Abstract: In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are flights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous flight segments, due to mechanical, environmental, or human factors in order to identifying operationally significant events and provide insights into the flight operations and highlight otherwise unavailable potential safety risks and precursors to accidents. For this purpose, we propose a framework which represents each flight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model's prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M12S,192,2d,hd,FALSE,https://i.ytimg.com/vi/8dWM7loVUUc/maxresdefault.jpg,,260,0,0,0,0
464,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Dn4pg2Cij3c,2016-06-30T17:20:18Z,30/6/16 17:20,KDD2016 paper 1132,"Title: EMBERS at 4 years: Experiences Operating an Open Source Forecasting System Authors: Sathappan Muthiah*, Virginia Tech Naren Ramakrishnan, Virginia Tech Patrick Butler, Virginia Tech Rupinder Khandpur, Virginia Tech Parang Saraf, Virginia Tech Anil Vullikanti, Virginia Tech Achla Marathe, Virginia Tech Graham Katz, CACI Andrew Doyle, CACI Jaime Arredondo, UCSD Dipak Gupta, SDSU David Mares, UCSD Jose Cadena, Virginia Tech Liang Zhao, Virginia Tech Nathan Self, Virginia Tech Alla Rozovskaya, Virginia Tech Kristen Summers, IBM Abstract: EMBERS is an anticipatory intelligence system forecasting population-level events in multiple countries of Latin America. A deployed system from 2012, EMBERS has been generating alerts 24x7 by ingesting a broad range of data sources including news, blogs, tweets, machine coded events, currency rates, and food prices. In this paper, we describe our experiences operating EMBERS continuously for nearly 4 years, with specific attention to the discoveries it has enabled, correct as well as missed forecasts, and lessons learnt from participating in a forecasting tournament including our perspectives on the limits of forecasting and ethical considerations. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M20S,80,2d,sd,FALSE,,,136,2,0,0,0
465,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_s0B9_gmB6c,2016-06-30T17:20:18Z,30/6/16 17:20,KDD2016 paper 773,"Title: Burstiness Scale: a parsimonious model for characterizing random series of events Authors: Renato Assunção*, Universidade Federal de Minas Gerais Rodrigo A S Alves, Centro Federal de Educação Tecnológica de Minas Gerais Pedro O.S. Vaz de Melo, Universidade Federal de Minas Gerais Abstract: The problem to accurately and parsimoniously characterize random series of events (RSEs) present in the Web, such as e-mail conversations or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events and periods of inactivities. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a highly parsimonious way to characterize general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstyness scale, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M46S,166,2d,hd,FALSE,https://i.ytimg.com/vi/_s0B9_gmB6c/maxresdefault.jpg,,148,1,0,0,0
466,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,j6K-lK2E1oE,2016-06-30T17:20:18Z,30/6/16 17:20,KDD2016 paper 755,"Title: An Engagement-Based Customer Lifetime Value System for E-commerce Authors: Ali Vanderveld*, Groupon Angela Han, Groupon Addhyan Pandey, Groupon Rajesh Parekh, Facebook Abstract: A comprehensive understanding of individual customer value is crucial to any successful customer relationship management strategy. It is also the key to building products for long-term value returns. Modeling customer lifetime value (CLTV) can be fraught with technical difficulties, however, due to both the noisy nature of user-level behavior and the potentially large customer base. Here we describe a new CLTV system that solves these problems. This was built at Groupon, a large global e-commerce company, where confronting the unique challenges of local commerce means quickly iterating on new products and the optimal inventory to appeal to a wide and diverse audience. Given current purchaser frequency we need a faster way to determine the health of individual customers, and given finite resources we need to know where to focus our energy. Our CLTV system predicts future value on an individual user basis with a random forest model which includes features that account for nearly all aspects of each customer’s relationship with our platform. This feature set includes those quantifying engagement via email and our mobile app, which give us the ability to predict changes in value far more quickly than models based solely on purchase behavior. We further model different customer types, such as one-time buyers and power users, separately so as to allow for different feature weights and to enhance the interpretability of our results. Additionally, we developed an economical scoring framework wherein we re-score a user when any trigger events occur and apply a decay function otherwise, to enable frequent scoring of a large customer base with a complex model. This system is deployed, predicting the value of hundreds of millions of users on a daily cadence, and is actively being used across our products and business initiatives. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M40S,160,2d,hd,FALSE,https://i.ytimg.com/vi/j6K-lK2E1oE/maxresdefault.jpg,,504,3,0,0,0
467,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,l8en1xYMyBc,2016-06-30T17:20:18Z,30/6/16 17:20,KDD2016 paper 511,"Title: Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta Authors: Michael Madaio*, Carnegie Mellon University Shang-Tse Chen, Georgia Tech Oliver L. Haimson, University of California, Irvine Wenwen Zhang, Georgia Tech Xiang Cheng, Emory University Matthew Hinds-Aldrich, Atlanta Fire Rescue Dept. Duen Horng Chau, Georgia Tech Bistra Dilkina, Georgia Tech Abstract: The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD’s fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD’s criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD’s inspection processes and Atlanta residents’ safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M5S,125,2d,hd,FALSE,https://i.ytimg.com/vi/l8en1xYMyBc/maxresdefault.jpg,,2074,5,0,0,0
468,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,3AzdnVjza_Q,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 1089,"Title: Identifying Decision Makers from Professional Social Networks Authors: Shipeng Yu*, LinkedIn Evangelia Christakopoulou, University of Minnesota Abhishek Gupta, LinkedIn Abstract: Sales professionals help organizations win clients for products and services. Generating new clients starts with identifying the right decision makers at the target organization. For the past decade, online professional networks have collected tremendous amount of data on people’s identity, their network and behavior data of buyers and sellers building relationships with each other for a variety of use-cases. Sales professionals are increasingly relying on these networks to research, identify and reach out to potential prospects, but it is often hard to find the right people effectively and efficiently. In this paper we present LDMS, the LinkedIn Decision Maker Score, to quantify the ability of making a sales decision for each of the 400M+ LinkedIn members. It is the key data-driven technology underlying Sales Navigator, a proprietary LinkedIn product that is designed for sales professionals. We will specifically discuss the modeling challenges of LDMS, and present two graph-based approaches to tackle this problem by leveraging the professional network data at LinkedIn. Both approaches are able to leverage both the graph information and the contextual information on the vertices, deal with small amount of labels on the graph, and handle heterogeneous graphs among different types of vertices. We will show some offline evaluations of LDMS on historical data, and also discuss its online usage in multiple applications in live production systems as well as future use cases within the LinkedIn ecosystem. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M16S,136,2d,hd,FALSE,https://i.ytimg.com/vi/3AzdnVjza_Q/maxresdefault.jpg,,606,2,0,0,0
469,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,7X1CmMVJFUk,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 510,"Title: Engagement Capacity and Engaging Team Formation for Reach Maximization of Online Social Media Platforms Authors: Alexander Nikolaev*, University at Buffalo Shounak Gore, University at Buffalo Venu Govindaraju, University at Buffalo Abstract: The challenges of assessing the ``health’’ of online social media platforms and strategically growing them are recognized by many practitioners and researchers. For those platforms that primarily rely on user-generated content, the reach—the degree of participation referring to the percentage and involvement of users—is a key indicator of success. This paper lays a theoretical foundation for measuring engagement as a driver of reach that achieves growth via positive externality effects. The paper takes a game theoretic approach to quantifying engagement, viewing a platform’s social capital as a cooperatively created value and finding a fair distribution of this value among the contributors. It introduces engagement capacity, a measure of the ability of users and user groups to engage peers, and formulates the Engaging Team Formation Problem (EngTFP) to identify the sets of users that ``make a platform go’‘. We show how engagement capacity can be useful in characterizing forum user behavior and in the reach maximization efforts. We also stress how engagement analysis differs from influence measurement. Computational investigations with Twitter and Health Forum data reveal the properties of engagement capacity and the utility of EngTFP. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M35S,155,2d,hd,FALSE,https://i.ytimg.com/vi/7X1CmMVJFUk/maxresdefault.jpg,,213,4,0,0,1
470,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,AjRW0hP-zAg,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 1131,"Title: Modeling Precursors for Event Forecasting via Nested Multi-Instance Learning Authors: Yue Ning*, Virginia Tech Sathappan Muthiah, Virginia Tech Huzefa Rangwala, George Mason University Naren Ramakrishnan, Virginia Tech Abstract: Forecasting events like civil unrest movements, disease outbreaks, financial market movements and government elections from open source indicators such as news feeds and social media streams is an important and challenging problem. From the perspective of human analysts and policy makers, forecasting algorithms need to provide supporting evidence and identify the causes related to the event of interest. We develop a novel multiple instance learning based approach that jointly tackles the problem of identifying evidence-based precursors and forecasts events into the future. Specifically, given a collection of streaming news articles from multiple sources we develop a nested multiple instance learning approach to forecast significant societal events across three countries in Latin America. Our algorithm is able to identify news articles considered as precursors for a protest. Our empirical evaluation shows the strengths of our proposed approaches in filtering candidate precursors, forecasting the occurrence of events with a lead time and predicting the characteristics of different events in comparison to several other formulations. We demonstrate through case studies the effectiveness of our proposed model in filtering the candidate precursors for inspection by a human analyst. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M57S,117,2d,sd,FALSE,,,514,0,0,0,0
471,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,I1XLrDn74u0,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 496,"Title: Temporal Order-based First-Take-All Hashing for Fast Attention-Deficit-Hyperactive-Disorder Detection Authors: Hao Hu*, University of Central Florida Joey Velez-Ginorio, University of Central Florida Guo-Jun Qi, University of Central Florida Abstract: Attention Deficit Hyperactive Disorder (ADHD) is one of the most common childhood disorders and can continue through adolescence and adulthood. Although the root cause of the problem still remains unknown, recent advancements in brain imaging technology reveal there exists differences between neural activities of Typically Developing Children (TDC) and ADHD subjects. Inspired by this, we propose a novel First-Take-All (FTA) hashing framework to investigate the possibility of fast detecting ADHD subjects through the fMRI time-series of neuron activities. By hashing time courses of region of interests (ROIs) in the brain into fixedsize hash codes, FTA can compactly encode the temporal order differences between the neural activity patterns that are key to distinguish TDC and ADHD subjects. Such patterns can be directly learned via minimizing the training loss incurred by the generated FTA codes. By conducting similarity search on the resultant FTA codes, automated ADHD detection can be achieved in an efficient fashion. The experiments’ results on the real world ADHD detection benchmark demonstrate the FTA can outperform the state-of-the-art baselines despite it uses only neural activity time series without any phenotypic information. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M47S,167,2d,hd,FALSE,https://i.ytimg.com/vi/I1XLrDn74u0/maxresdefault.jpg,,72,0,0,0,0
472,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,TJN0jSlg4S4,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 741,"Title: User Identity Linkage by Latent User Space Modelling Authors Xin Mu*, Nanjing University Feida Zhu, Singapore Management University Zhi-Hua Zhou, Nanjing University Ee-Peng Lim, Singapore Management University Jing Xiao,Ping An Technology (Shenzhen) Co., Ltd Jianzong Wang, Ping An Technology (Shenzhen) Co., Ltd Abstract User identity linkage across social platforms is an important problem of great research challenge and practical value. In real applications, the task often assumes an extra degree of difficulty by requiring linkage across multiple platforms. While pair-wise user linkage between two platforms, which has been the focus of most existing solutions, provides reasonably convincing linkage, the result depends by nature on the order of platform pairs in execution with no theoretical guarantee on its stability. In this paper, we explore a new concept of Latent User Space to more naturally model the relationship between the underlying real users and their observed projections onto the varied social platforms, such that the more similar the real users, the closer their profiles in the latent user space. We propose two effective algorithms, a batch model(ULink) and an online model(ULink-On), based on latent user space modelling. Two simple yet effective optimization methods are used for optimizing objective function: the first one based on the constrained concave-convex procedure(CCCP) and the second on accelerated proximal gradient. To our best knowledge, this is the first work to propose a unified framework to address the following two important aspects of the multi-platform user identity linkage problem—- (I) the platform multiplicity and (II) online data generation. We present experimental evaluations on real-world data sets for not only traditional pairwise-platform linkage but also multi-platform linkage. The results demonstrate the superiority of our proposed method over the state-of-the-art ones. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M29S,149,2d,hd,FALSE,https://i.ytimg.com/vi/TJN0jSlg4S4/maxresdefault.jpg,,306,1,0,0,0
473,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,hokkyOafE_E,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 744,"Title: Unbounded Human Learning: Optimal Scheduling for Spaced Repetition Authors: Siddharth Reddy*, Cornell University Igor Labutov, Cornell University Siddhartha Banerjee, Cornell University Thorsten Joachims, Cornell University Abstract: In the study of human learning, there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure. This plays a crucial role in the design of educational software, leading to a trade-off between teaching new material and reviewing what has already been taught. A common way to balance this trade-off is spaced repetition, which uses periodic review of content to improve long-term retention. Though spaced repetition is widely used in practice, e.g., in electronic flashcard software, there is little formal understanding of the design of these systems. Our paper addresses this gap in three ways. First, we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay. Second, we use this memory model to develop a stochastic model for spaced repetition systems. We propose a queueing network model of the Leitner system for reviewing flashcards, along with a heuristic approximation that admits a tractable optimization problem for review scheduling. Finally, we empirically evaluate our queueing model through a Mechanical Turk experiment, verifying a key qualitative prediction of our model: the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT54S,54,2d,hd,FALSE,https://i.ytimg.com/vi/hokkyOafE_E/maxresdefault.jpg,,136,0,0,0,0
474,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,nhecgV-fui0,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 1033,"Title: From Prediction to Action: A Closed-Loop Approach for Data-Guided Network Resource Allocation Authors: Yanan Bao*, University of California, Davis Huasen Wu, University of California, Davis Xin Liu, University of California, Davis Abstract: Machine learning methods have been widely used in modeling and predicting network user experience. In this paper, moving beyond user experience prediction, we propose a closed-loop approach that uses data-generated prediction models to explicitly guide resource allocation for user experience improvement. The closed-loop approach leverages and verifies the causal relation that often exists between certain feature values (e.g., bandwidth) and user experience in computer networks. The approach consists of three components: we train a neural network classifier to predict user experience, utilize the trained neural network classifier as the objective function to decide network resource allocation, and then evaluate user experience with allocated resource to (in)validate and adjust the original model. We propose a Dual decomposition algorithm to solve the neural network-based resource optimization problem, which is complex and non-convex. We further develop an iterative mechanism for classifier optimization. Numerical results show that the Dual algorithm reduces the expected number of unsatisfied users by up to 2x compared with the baseline, and the optimized classifier further improves the performance by 50%. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M1S,181,2d,hd,FALSE,https://i.ytimg.com/vi/nhecgV-fui0/maxresdefault.jpg,,442,15,0,0,3
475,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,wxy8of8Hvk0,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 1038,"Title: Batch model for batched timestamps data analysis with application to the SSA disability program Authors: Qingqi Yue*, NIH Ao Yuan, NIH Xuan Che, NIH Elizabeth Rasch, NIH Minh Huynh, Impaq Chunxiao Zhou, NIH Abstract: The Office of Disability Adjudication and Review (ODAR) is responsible for holding hearings, issuing decisions, and reviewing appeals as part of the Social Security Administration’s disability determining process. In order to control and process cases, the ODAR has established a Case Processing and Management System (CPMS) to record management information since December 2003. The CPMS provides a detailed case status history for each case. Due to the large number of appeal requests and limited resources, the number of pending claims at ODAR was over one million cases by March 31, 2015. Our National Institutes of Health (NIH) team collaborated with SSA and developed a Case Status Change Model (CSCM) project to meet the ODAR’s urgent need of reducing backlogs and improving hearings and appeals process. One of the key issues in our CSCM project is to estimate the expected service time and its variation for each case status code. As the CPMS timestamps data of case status codes presents apparent batch pattern, we propose a batch model and use constrained least squares method to estimate the mean service times and the variances. We also propose a batch search algorithm to determine the optimal batches, when the batch partition is not given as in the real data. Simulation studies are conducted to evaluate the performance of the proposed methods. In addition, we applied the same method to analyze a real CPMS data from ODAR/SSA. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M8S,188,2d,hd,FALSE,https://i.ytimg.com/vi/wxy8of8Hvk0/maxresdefault.jpg,,167,0,0,0,0
476,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,zo0BbIkSHkE,2016-06-30T17:20:17Z,30/6/16 17:20,KDD2016 paper 770,"Title: CompanyDepot: Employer Name Normalization in the Online Recruitment Industry Authors: Qiaoling Liu*, CareerBuilder Faizan Javed, CareerBuilder Matt McNair, CareerBuilder Abstract: Entity linking which links entity mentions in text to the corresponding entities in a knowledge base (KB) has many applications in both open domain and specific domains. For example, in the recruitment domain, linking employer names in job postings or resumes to entities in an employer KB is very important to many business applications. In this paper, we focus on this employer name normalization task, which has several unique challenges: handling employer names from both job postings and resumes, leveraging the corresponding location context, and handling name variations, irrelevant input data, and noises in the KB. We present a system called Crystal which contains a machine learning based approach Crystal-ML and a heuristic approach Crystal-H to address these challenges in three steps: (1) searching for candidate entities based on a customized search engine for the KB; (2) ranking the candidate entities using learning-to-rank methods or heuristics; and (3) validating the top-ranked entity via binary classification or heuristics. While Crystal-ML shows better extendability and flexibility, Crystal-H serves as a strong baseline and useful way to collect training data for Crystal-ML. The proposed system achieves 2.5%-21.4% higher coverage at the same precision level compared to an existing system used at CareerBuilder over multiple real-world datasets. Applying the system to a similar task of academic institution name normalization further shows the generalization ability of the method. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M2S,182,2d,hd,FALSE,https://i.ytimg.com/vi/zo0BbIkSHkE/maxresdefault.jpg,,156,1,0,0,0
477,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,1ntW2vJSQWE,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 1081,"Title: Recurrent Marked Temporal Point Process Authors: Nan Du*, Georgia Tech Hanjun Dai, Max Plank Institute Rakshit Trivedi, Max Plank Institute Utkarsh Upadhyay, Max Plank Institute Manuel Gomez-Rodriguez, MPI-SWS Le Song, MPI-SWS Abstract: Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart city and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indices rather than random variables. Marked temporal point processes and intensity functions are the mathematical framework for modeling such event data. However, typical point process models, such as Hawkes processes, continuous Markov chains, autoregressive conditional duration processes, are making strong assumptions about the generative processes of event data which may or may not reflect reality, and the parametric assumptions have also restricted the expressive power of temporal point process models. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose a novel point process, referred to as the Recurrent Temporal Point Process (RTPP), to simultaneously model the event timings and markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and parameterize the function using a recurrent neural network. We develop an efficient stochastic gradient algorithm for learning RTPP which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case that the true models are parametric models, RTPP can learn the dynamics of such models without knowing the actual parametric forms; and in the case that the true models are unknown, RTPP can also learn the dynamics, and achieve better predictive performance than other parametric alternatives based on prior knowledge. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M10S,190,2d,hd,FALSE,https://i.ytimg.com/vi/1ntW2vJSQWE/maxresdefault.jpg,,1431,15,2,0,1
478,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,8OWeDin3WPQ,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 1027,"Title: A Non-parametric Approach to Detect Epileptogenic Lesions using Restricted Boltzmann Machines Authors: Yijun Zhao*, Tufts University Bilal Ahmed, Tufts University Carla Brodley, Northeastern University Jennifer Dy, Northeastern University Abstract: Visual detection of lesional areas on a cortical surface is critical in rendering a successful surgical operation for Treatment Resistant Epilepsy (TRE) patients. Unfortunately, 45% of Focal Cortical Dysplasia (FCD, the most common kind of TRE) patients have no visual abnormalities in their brains. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M51S,171,2d,hd,FALSE,https://i.ytimg.com/vi/8OWeDin3WPQ/maxresdefault.jpg,,295,4,0,0,0
479,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9INY8OCLkS0,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 1085,"Title: Improving Survey Aggregation with Sparsely Represented Signals Authors: Tianlin Shi, Stanford University Forest Agostinelli*, University of California - Irvine Matthew Staib, MIT David Wipf, Microsoft Research Thomas Moscibroda, Microsoft Research Abstract: In this paper, we develop a new aggregation technique to reduce the cost of surveying. Our method aims to jointly estimate a vector of target quantities such as public opinion or voter intent across time and maintain good estimates when using only a fraction of the data. Inspired by the James-Stein estimator, we resolve this challenge by shrinking the estimates to a global mean which is assumed to have a sparse representation in some known basis. This assumption has lead to two different methods for estimating the global mean: orthogonal matching pursuit and deep learning. Both of which significantly reduce the number of samples needed to achieve good estimates of the true means of the data and, in the case of actual voting outcomes, can estimate the outcome of the 2012 United States elections while saving hundreds of thousands of samples. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M42S,102,2d,sd,FALSE,,,264,3,0,0,0
480,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,IEBUxabXpfk,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 722,"Title: Joint Optimization of Multiple Performance Metrics in Online Video Advertising Authors: Sahin Geyik*, Turn Inc. Sergey Faleev, Turn Inc. Jianqiang Shen, Turn Inc. Sean O'Donnell, Turn Inc. Santanu Kolay, Turn Inc. Abstract: The field of online advertising, in essence, deals with the problem of presenting ads to online users in the most appropriate contexts to achieve a multitude of advertiser goals. A vast amount of work in online advertising has been focused on optimizing banner display advertising campaigns where the main goal lies in direct response metrics, such as a click or conversion. In this paper, we are exploring the newly popularized space of online video advertising, where brand recognition is the key focus. We propose a framework based on a feedback mechanism where we optimize multiple video specific performance indicators while making sure the delivery constraints (budget and user reach) of advertisers are satisfied. While our main focus is on improving metrics such as engagement (amount of view time), and viewability (whether a campaign is within eyesight of a user), we also discuss the possibilities of expanding to other metrics. We demonstrate the benefit of our framework via empirical results in multiple real-world advertising campaigns. To the best of our knowledge, this is the first paper that deals with challenges that arise due to the nature of online video advertising. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M9S,189,2d,hd,FALSE,https://i.ytimg.com/vi/IEBUxabXpfk/maxresdefault.jpg,,146,0,0,0,0
481,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,RJ2EP2U0xXE,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 483,"Title: Audience Expansion for Online Social Network Advertising Authors: Haishan Liu*, LinkedIn Corporation David Pardoe, LinkedIn Corporation Kun Liu, LinkedIn Corporation Abstract: Online social network advertising platforms, such as that provided by LinkedIn, generally allow marketers to specify targeting options so that their ads appear to a desired demographic. Audience Expansion is a technique developed at LinkedIn to simplify targeting and identify new audiences with similar attributes to the original target audience. We developed two methods to achieve Audience Expansion: the campaign-agnostic expansion and the campaign-aware expansion. In this paper, we describe details of these methods, present in-depth analysis of their trade-offs, and demonstrate a hybrid strategy that possesses the combined strength of both methods. Through large scale online experiments, we show the effectiveness of the proposed approach, and as a result, the benefits it brings to the whole marketplace including both LinkedIn and advertisers. The achieved benefits can be characterized as: 1) simplified targeting process and increased reach for advertisers, and 2) better utilization of LinkedIn’s ads inventory and a higher and more efficient market participation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M2S,182,2d,hd,FALSE,https://i.ytimg.com/vi/RJ2EP2U0xXE/maxresdefault.jpg,,397,3,0,0,0
482,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,VOb1OSmKGn0,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 767,"Title: A Subsequence Interleaving Model for Sequential Pattern Mining Authors: Jaroslav Fowkes, University of Edinburgh Charles Sutton, University of Edinburgh Abstract: Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M43S,163,2d,hd,FALSE,https://i.ytimg.com/vi/VOb1OSmKGn0/maxresdefault.jpg,,311,3,0,0,0
483,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,xIfdy0YB0C4,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 766,"Title: Fast Component Pursuit for Large-Scale Inverse Covariance Estimation Authors: Lei Han*, Rutgers University Yu Zhang, Hong Kong University of Science and Technology Tong Zhang, Rutgers University Abstract The maximum likelihood estimation (MLE) for the Guassian graphical model, which is also known as the inverse covariance estimation problem, has gained increasing interest recently. Most of the existing works assume that the inverse covariance estimator is sparse or contains a sparse structure, and then construct the model with the $\ell_1$ regularization. In this paper, different from existing works, we study the inverse covariance estimation problem from another perspective by efficiently modeling a low-rank structure for the inverse covariance, which is assumed to be a combination of a low-rank part and a diagonal matrix. Specifically, we propose an efficient COmponent Pursuit (COP) method to obtain the low-rank part, where each component can be sparse. One motivation for this assumption is that the low-rank structure is common in many applications, e.g., the climate and financial data, and another one is that such assumption can reduce the computational complexity when computing its inverse. For optimization, the COP method greedily learns a rank-one component in each iteration by maximizing the log-likelihood. Moreover, the COP algorithm enjoys several appealing properties including the existence of an efficient solution in each iteration and the theoretical guarantee on the convergence of this greedy approach. Experiments on large-scale synthetic and real-world datasets, including thousands of millions variables, show that the COP method is faster than the state-of-the-art techniques for inverse covariance estimation when achieving comparable performance on test data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M52S,172,2d,hd,FALSE,https://i.ytimg.com/vi/xIfdy0YB0C4/maxresdefault.jpg,,72,0,0,0,0
484,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,yRTjN2FyTR8,2016-06-30T17:20:16Z,30/6/16 17:20,KDD2016 paper 995,"Title: Robust Extreme Multi-label Learning Authors: Chang Xu*, Peking University Dacheng Tao, University of Technology Sydney Chao Xu, Peking University Abstract: Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M56S,176,2d,hd,FALSE,https://i.ytimg.com/vi/yRTjN2FyTR8/maxresdefault.jpg,,435,1,0,0,0
485,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,HsDu0bq5058,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 967,"Title: Communication Efficient Distributed Kernel Principal Component Analysis Authors: Yingyu Liang*, Princeton University Bo Xie, Georgia Institute of Technology David Woodruff, IBM Research Le Song, Georgia Institute of Technology Maria-Florina Balcan, Carnegie Mellon University Abstract: Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing $k$ principal components with relative error $\epsilon$ over $s$ workers has communication cost $\tilde{O}(s \nsparse k/\epsilon+s k^2/\epsilon^3)$ words, where $\nsparse$ is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets and showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M32S,152,2d,sd,FALSE,https://i.ytimg.com/vi/HsDu0bq5058/maxresdefault.jpg,,182,0,0,0,0
486,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,IvVsdu3mAfE,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 945,"Title: Improving the Sensitivity of Online Controlled Experiments: Case Studies at Netflix Authors: Huizhi Xie*, Netflix Juliette Aurisset, Netflix Abstract: Controlled experiments are widely regarded as the most scientific way to establish a true causal relationship between product changes and their impact on business metrics. Many technology companies rely on such experiments as their main data-driven decision-making tool. The sensitivity of a controlled experiment refers to its ability to detect differences in business metrics due to product changes. At Netflix, with tens of millions of users, increasing the sensitivity of controlled experiments is critical as failure to detect a small effect, either positive or negative, can have a substantial revenue impact. This paper focuses on methods to increase sensitivity by reducing the sampling variance of business metrics. We define Netflix business metrics and share context around the critical need for improved sensitivity. We review popular variance reduction techniques that are broadly applicable to any type of controlled experiment and metric. We describe an innovative implementation of stratified sampling at Netflix where users are assigned to experiments in real time and discuss some surprising challenges with the implementation. We conduct case studies to compare these variance reduction techniques on a few Netflix datasets. We share empirical results and provide guidance on the choice of variance reduction techniques in large-scale controlled experiments. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M45S,165,2d,hd,FALSE,https://i.ytimg.com/vi/IvVsdu3mAfE/maxresdefault.jpg,,272,0,0,0,0
487,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,W-hq4GahEvg,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 764,"Title: Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification in Flu Virus Data Authors: Lei Han*, Rutgers University Yu Zhang, Hong Kong University of Science and Technology Xiu-Feng Wan, Mississippi State University Tong Zhang, Rutgers University Abstract Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates/features can significantly improve the interpretability for biological data. One major challenge is the exponentially expanded feature space when adding high-order feature interaction to the model. To tackle the huge dimensionality, hierarchical sparse models (HSM) are developed by enforcing sparsity under heredity structures in the interactions among the covariates. However, existing methods only consider pairwise interactions, making the discovery of important high-order interactions a non-trivial open problem. In this paper, we propose a generalized hierarchical sparse model (GHSM) as a generalization of HSM models to tackle arbitrary-order interactions. The GHSM applies the $\ell_1$ penalty to all the model coefficients under a constraint that given any covariate, if none of its associated $k$th-order interaction contribute to the regression model, then neither do its associated higher-order interactions. The resulting objective function is non-convex with a challenge lying in the coupled variables appearing in the arbitrary-order hierarchical constraints and we devise an efficient optimization algorithm to directly solve it. Specifically, we decouple the variables in the constraints via both the general iterative shrinkage and thresholding (GIST) and the alternating direction method of multipliers (ADMM) methods into three subproblems, each of which is proved to admit an efficiently analytical solution. We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the influenza virus data, where we expand the feature space up to the 5th-order interactions. Empirical results demonstrate the effectiveness and efficiency of the proposed methods and the learned high-order interactions have meaningful synergistic covariate patterns in the influenza virus antigenicity. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M42S,162,2d,hd,FALSE,https://i.ytimg.com/vi/W-hq4GahEvg/maxresdefault.jpg,,107,0,0,0,0
488,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,aMujYAEFwZo,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 984,"Title: The Limits of Popularity-Based Recommendations, and the Role of Social Ties Authors: Marco Bressan*, Sapienza University of Rome Stefano Leucci, Sapienza University of Rome Alessandro Panconesi, Sapienza University of Rome Prabhakar Raghavan, Google Erisa Terolli, Sapienza University of Rome Abstract: In this paper we introduce a mathematical model that captures some of the salient features of recommender systems that are based on popularity and that try to exploit social ties among the users. We show that, under very general conditions, the market always converges to a steady state, for which we are able to give an explicit form. Thanks to this we can tell rather precisely how much a market is altered by a recommendation system, and determine the power of users to influence others. Our theoretical results are complemented by experiments with real world social networks showing that social graphs prevent large market distortions in spite of the presence of highly influential users. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT56S,56,2d,hd,FALSE,https://i.ytimg.com/vi/aMujYAEFwZo/maxresdefault.jpg,,3386,80,2,0,5
489,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,duu_9fGzKho,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 942,"Title: Revisiting Random Binning Feature: Fast Convergence and Strong Parallelizability Authors: Lingfei Wu*, College of William and Mary En-Hsu Yen, University of Texas at Austin Jie Chen, IBM Research Rui Yan, Baidu Inc. Abstract: Kernel method has been developed as one of the standard approaches for nonlinear learning, which however, does not scale to large data set due to its quadratic complexity in the number of samples. A number of kernel approximation methods have thus been proposed in the recent years, among which the random features method gains much popularity due to its simplicity and direct reduction of nonlinear problem to a linear one. Many different random basis functions have since been proposed to approximate different types of kernel. Among them the Random Binning (RB) features, proposed in the first random features paper \cite{rahimi2007random}, has drawn significantly less attention than that of Random Fourier (RF) features proposed also in \cite{rahimi2007random}. However, in this work we observe the RB approach, with right choice of optimization solver, could be orders of magnitude faster than other random features and kernel approximation methods to achieve the same accuracy. We thus propose the first analysis of RB from the perspective of optimization, which by interpreting RB as a Randomized Block Coordinate Descent in the infinite-dimensional space, gives a faster convergence compared to that of other random features. In particular, we show that by drawing $R$ grids with at least $\kappa$ expected number of non-empty bins per grid, RB achieves a convergence rate of $O(1/\kappa R)$, which is not only better than the existing $O(1/\sqrt{R})$ rate from Monte Carlo analysis, but also shows a $\kappa$ times speedup over other random features under the same analysis framework. In addtion, we demonstrate another advantage of RB in the L1-regularized setting, where unlike other random features, a RB-based Coordinate Descent solver can be parallelized with guaranteed speedup proportional to $\kappa$. Our extensive experiments demonstrate the superior performance of the RB features over other random features and kernel approximation methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M41S,161,2d,hd,FALSE,https://i.ytimg.com/vi/duu_9fGzKho/maxresdefault.jpg,,119,0,0,0,0
490,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,jdCZXtEqCJk,2016-06-30T17:20:15Z,30/6/16 17:20,KDD2016 paper 1069,"Title: Smart Reply: Automated Response Suggestion for Email Authors: Karol Kurach*, Google, Inc. Anjuli Kannan, Google, Inc. Sujith Ravi, Google, Inc. Tobias Kaufmann, Google, Inc. Andrew Tomkins, Google, Inc. Balint Miklos, Google, Inc. Greg Corrado, Google, Inc. László Lukács, Google, Inc. Marina Ganea, Google, Inc. Peter Young, Google, Inc. Vivek Ramavajjala, Google, Inc. Abstract: In this paper we propose and investigate a novel end-to-end method for generating automatic short e-mail responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for handling 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art deep learning models trained on a larger scale than before. We describe the architecture of the system as well as the challenges that we faced while building it, like response diversity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only modest amount of explicitly labeled data. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT1M41S,101,2d,hd,FALSE,https://i.ytimg.com/vi/jdCZXtEqCJk/maxresdefault.jpg,,1178,8,0,0,0
491,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,9xPu3BuClg8,2016-06-30T17:20:14Z,30/6/16 17:20,KDD2016 paper 977,"Title: Predict Risk of Relapse for Patients with Multiple Stages of Treatment of Depression Authors: Zhi Nie*, Arizona State University Pinghua Gong, University of Michigan at Ann Arbor Jieping Ye, University of Michigan at Ann Arbor Abstract: Depression is a serious mood disorder afflicting millions of people around the globe. Medications of different types and with different effects on neural activity have been developed for its treatments during the past few decades. Due to the heterogeneity of the disorder, many patients cannot achieve symptomatic remission from a single clinical trial. Instead they need multiple clinical trials to achieve remission, resulting in a multiple stage treatment pattern. Furthermore those who indeed achieve symptom remission are still faced with substantial risk of relapse. One promising approach to predict the risk of relapse is censored regression. Traditional censored regression typically applies only to situations in which the exact time of event of interest is known. However, follow-up studies that track the patients’ relapse status can only provide an interval of time during which relapse occurs. The exact time of relapse is usually unknown. In this paper, we present a censored regression approach with a truncated $l_1$ loss function that can handle the uncertainty of relapse time. Based on this general loss function, we develop a gradient boosting algorithm and a stochastic dual coordinate ascent algorithm when the hypothesis in the loss function is represented as (1) an ensemble of decision trees and (2) a linear combination of covariates, respectively. As an extension of our linear model, a multi-stage linear approach is further proposed to harness the data collected from multiple stages of treatment. We evaluate the proposed algorithms using a real-world clinical trial dataset. Results show that our methods outperform the well-known Cox proportional hazard model. In addition, the risk factors identified by our multi-stage linear model not only corroborate findings from recent research but also yield some new insights into how to develop effective measures for prevention of relapse among patients after their initial remission from the acute treatment stage. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M44S,164,2d,sd,FALSE,https://i.ytimg.com/vi/9xPu3BuClg8/maxresdefault.jpg,,123,0,0,0,0
492,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,GYIlPkx1uuc,2016-06-30T17:20:14Z,30/6/16 17:20,KDD2016 paper 975,"Title: Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features Authors: Ying Shan*, Microsoft Corporation Thomas Hoens, Microsoft Corporation Jian Jiao, Microsoft Corporation Haijing Wang, Microsoft Corporation Dong Yu, Microsoft Research JC Mao, Microsoft Corporation Abstract: Manually crafted combinatorial features have been the “secret sauce” behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model, which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from the ground up, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M55S,175,2d,hd,FALSE,https://i.ytimg.com/vi/GYIlPkx1uuc/maxresdefault.jpg,,395,0,0,0,1
493,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Gq7mCUk8QIQ,2016-06-30T17:20:14Z,30/6/16 17:20,KDD2016 paper 943,"Title: Evaluating Mobile App Release Authors: Ya Xu*, LinkedIn Corporation Nanyu Chen, LinkedIn Corporation Abstract: We have seen an explosive growth of mobile usage, particularly on mobile apps. It is more important than ever to be able to properly evaluate mobile app release. A/B testing is a standard framework to evaluate new ideas. We have seen much of its applications in the online world across the industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different, and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process. Mobile infrastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps, which will be discussed in details in this paper. In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi-experimental framework [21]. Not all features can be A/B tested due to infrastructure changes and wholistic product redesign. We propose and establish quasi-experiment techniques for measuring impact from mobile app release, with results shared from a recent major app launch at LinkedIn. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M37S,157,2d,hd,FALSE,https://i.ytimg.com/vi/Gq7mCUk8QIQ/maxresdefault.jpg,,1101,2,1,0,1
494,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,Tbn0PQJVaV4,2016-06-30T17:20:14Z,30/6/16 17:20,KDD2016 paper 1054,"Title: How to Compete Online for News Audience: Modeling Words that Attract Clicks Authors: Joon Hee Kim*, Korea Advanced Institute of Science and Technology Amin Mantrach, Yahoo! Research Alex Jaimes, Yahoo! Research Alice Oh, Korea Advanced Institute of Science and Technology Abstract: Headlines are particularly important for online news outlets where there are many similar news stories competing for users’ attention. Traditionally, journalists have followed rules-of-thumb and experience to master the art of crafting catchy headlines, but with the valuable resource of large-scale click-through data of online news articles, we can apply quantitative analysis and text mining techniques to acquire an in-depth understanding of headlines. In this paper, we conduct a large-scale analysis and modeling of 150K news articles published over a period of four months on the Yahoo home page. We define a simple method to measure click-value of individual words, and analyze how temporal trends and linguistic attributes affect click-through rate (CTR). We then propose a novel generative model, headline click-based topic model (HCTM), that extends latent Dirichlet allocation (LDA) to reveal the effect of topical context on the click-value of words in headlines. HCTM leverages clicks in aggregate on previously published headlines to identify words for headlines that will generate more clicks in the future. We show that by jointly taking topics and clicks into account we can detect changes in user interests within topics. We evaluate HCTM in two different experimental settings and compare its performance with ALDA (adapted LDA), LDA, and TextRank. The first task, full headline, is to retrieve full headline used for a news article given the body of news article. The second task, good headline, is to specifically identify words in the headline that have high click values for real users. For full headline task, our model performs on par with ALDA, a state-of-the art web-page summarization method that utilizes click-through information. For good headline task, which is of more practical importance to both individual journalists and online news outlets, our model significantly outperforms all other comparative methods. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M25S,145,2d,hd,FALSE,https://i.ytimg.com/vi/Tbn0PQJVaV4/maxresdefault.jpg,,401,0,0,0,0
495,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,YqW80g_Vzds,2016-06-30T17:20:14Z,30/6/16 17:20,KDD2016 paper 1044,"Title: Crime Rate Inference with Big Data Authors: Hongjian Wang*, Penn State University Zhenhui L, Penn State University Daniel Kifer, Penn State University Corina Graif, Penn State University Abstract: Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens’ life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also showed that these new features are significant in the feature importance analysis. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M33S,153,2d,hd,FALSE,https://i.ytimg.com/vi/YqW80g_Vzds/maxresdefault.jpg,,318,1,0,0,0
496,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,gtQOwAYS8LI,2016-06-30T17:20:13Z,30/6/16 17:20,KDD2016 paper 1164,"Title: Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit Authors: Chunqiu Zeng*, Florida International University Qing Wang, Florida International University Tao Li, Florida International University Shekoofeh Mokhtari, Florida International University Abstract: Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling. Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M56S,176,2d,hd,FALSE,https://i.ytimg.com/vi/gtQOwAYS8LI/maxresdefault.jpg,,862,3,0,0,0
497,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,uNVUA1Y_mhs,2016-06-30T17:20:12Z,30/6/16 17:20,KDD2016 paper 1184,"Title: Joint Community and Structural Hole Spanner Detection via Harmonic Modularity Authors: Lifang He*, Shenzhen University Chun-Ta Lu, University of Illinois at Chicago Jiaqi Ma, Tsinghua University Jianping Cao, National University of Defense Linlin Shen, Shenzhen University Philip S. Yu, University of Illinois at Chicago Abstract: Detecting communities (or modular structures) and structural hole spanners, the nodes bridging different communities in a network, are two essential tasks in the realm of network analytics. Due to the topological nature of communities and structural hole spanners, these two tasks are naturally tangled with each other, while there has been little synergy between them. In this paper, we propose a novel harmonic modularity method to tackle both tasks simultaneously. Specifically, we first applied harmonic function analysis to measure the smoothness of community structure and to obtain the community indicator. We then investigated the sparsity level of the interactions between communities, with particular emphasis on the nodes connecting to multiple communities, to discriminate the indicator of SH spanners and assist the community guidance. Extensive experiments on real-world networks demonstrate that our proposed method outperforms several state-of-the-art methods in the community detection task and also in the SH spanner identification task (even the methods that require the supervised community information). Furthermore, by removing the SH spanners spotted by our method, we show that the quality of other community detection methods can be further improved. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M28S,148,2d,sd,FALSE,https://i.ytimg.com/vi/uNVUA1Y_mhs/maxresdefault.jpg,,264,1,0,0,1
498,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,5qr8S7O3SPw,2016-06-30T17:20:11Z,30/6/16 17:20,KDD2016 paper 1190,"Title: Pseudo-Document-based Topic Modeling of Short Texts without Auxiliary Information Authors: Yuan Zuo*, Beihang University Junjie Wu, Beihang University Has Lin, Beihang University Hui Xiong, Rutgers Abstract: Recent years have witnessed the unprecedented growth of online social media, which empower short texts as the prevalent format for information of Internet. Given the nature of sparsity, however, short text topic modeling remains a critical yet much-watched challenge in both academy and industry. Rich research efforts have been put on building different types of probabilistic topic models for short texts, among which the self aggregation methods without using auxiliary information become an emerging solution for providing informative cross-text word co-occurrences. However, models along this line are still rarely seen, and the representative one SATM is prone to overfitting and computationally expensive. In light of this, in this paper, we propose a novel probabilistic model called PTM for short text topic modeling. PTM introduces the concept of {\it pseudo document} to implicitly aggregate short texts against data sparsity. By modeling the topic distributions of latent pseudo documents rather than short texts, PTM is expected to gain excellent performance in both accuracy and efficiency. A Sparsity-enhanced PTM (SPTM for short) is also proposed by applying Spike and Slab prior, with the purpose of eliminating undesired correlations between pseudo documents and latent topics. Extensive experiments on various real-world data sets with state-of-the-art baselines demonstrate the high quality of topics learned by PTM and its robustness with reduced training samples. It is also interesting to show that i) SPTM gains a clear edge over PTM when the number of pseudo documents is relatively small, and ii) the constraint that a short text belongs to only one pseudo document is critically important for the success of PTM. We finally take an in-depth semantic analysis to unveil directly the fabulous function of pseudo documents in finding cross-text word co-occurrences for topic modeling. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M23S,203,2d,hd,FALSE,https://i.ytimg.com/vi/5qr8S7O3SPw/maxresdefault.jpg,,437,2,1,0,0
499,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,RoUrdoU_2F4,2016-06-30T17:20:11Z,30/6/16 17:20,KDD2016 paper 1202,"Title: Sampling of Attributed Networks from Hierarchical Generative Models Authors: Pablo Robles Granda*, Purdue University Sebastian Moreno, Universidad Adolfo Ibañez Jennifer Neville, Purdue University Abstract: Network sampling is a widely used procedure in social network analysis; where a random network is sampled from a generative network model (GNM). Recently proposed GNMs, allow generation of networks with more realistic structural characteristics than earlier ones. This facilitates tasks such as hypothesis testing, sensitivity analysis, etc. However, sampling of attributed networks (networks with vertex-attributes) remains a challenging problem. While the recent work of [18] has provided a promising approach for attributed-network sampling, the approach only works with relatively simple GNM. This method does not work well with certain complex hierarchical models, capable of modeling the variation and complex characteristics of real world network structures. In these complex hierarchical GNMs the probability mass is allocated to certain regions of the network space, which allows to create dependencies in the network to achieve realistic characteristics — in contrast, in simple models the mass is spread through the entire space. Thus, it is hard to identify candidate networks from the sampling space of hierarchical GNMs. We propose a novel sampling method, CSAG, to sample from hierarchical GNMs. CSAG constrains every step of the sampling process to consider the structure of the GNM in order to bias the search to regions of the space with higher likelihood. This allows to generate networks with both correlated attributes and complex structure. We implemented CSAG using mixed Kronecker Product Graph Models and evaluated our approach on three real-world datasets. The results show that CSAG jointly models the correlation and structure of the datasets better than the state of the art, and maintains the variability of the GNM while providing a reduction of 5 times or more in the error of the correlation. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M57S,177,2d,hd,FALSE,https://i.ytimg.com/vi/RoUrdoU_2F4/maxresdefault.jpg,,623,4,0,0,0
500,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,XNf7gBGEq28,2016-06-30T17:20:10Z,30/6/16 17:20,KDD2016 paper 1227,"Title: Identifying Earmarks in Congressional Bills Authors: Madian Khabasa*, Microsoft Vrushank Vora, Data Science for Social Good Joe Walsh, Data Science for Social Good Ellery Wulczyn, Wikimedia Foundation Matthew Heston, Northwestern University Rayid Ghani, University of Chicago Chris Berry, University of Chicago Abstract: Earmarks are legislative provisions that direct federal funds to specific projects, circumventing the competitive grant making process of federal agencies. Identifying and cataloging earmarks is a tedious, time-consuming process carried out by experts from public interest groups. In this paper, we present a machine learning system for automatically extracting earmarks from congressional bills and reports. We first describe a table-parsing algorithm for extracting budget allocations from appropriations tables in congressional bills. We then use machine learning classifiers to identify budget allocations as earmarked objects with nearly 95% out of sample accuracy. Using this system, we construct the first publicly available database of earmarks dating back to 1995. Our machine learning approach adds transparency, accuracy and speed to the congressional appropriations process. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT3M15S,195,2d,hd,FALSE,https://i.ytimg.com/vi/XNf7gBGEq28/maxresdefault.jpg,,142,0,0,0,0
501,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,LIBLD8NM04Y,2016-06-30T17:20:09Z,30/6/16 17:20,KDD2016 paper 1236,"Title: Online Feature Selection: A Limited-Memory Substitution Algorithm and Its Asynchronous Parallel Variation Authors: Haichuan Yang*, University of Rochester Ryohei Fujimaki, NEC lab Yukitaka Kusumura, NEC lab Ji Liu, University of Rochester Abstract: Feature selection plays a key role in many learning and mining tasks. In some big data scenario, all features cannot be seen at the same time or are generated sequentially and one needs to immediately identify all important features after scan all features once or twice. Optimization based online feature is an important category of online feature selection algorithms to serve such application. However, most existing optimization based online feature selection algorithms explicitly or implicitly adopt ℓ1 norm regularization to identify important features. It suffers two main disadvantages: 1) the penalty term for ℓ1 norm term is hard to choose; 2) the memory usage is hard to control. To overcome these two drawbacks, this paper proposes a limited-memory and model parameter free online feature selection algorithm, namely online substitution (OS) algorithm. To improve the selection efficiency, an asynchronous parallel algorithm for OS (Asy-OS) is also proposed. Convergence analysis is provided for both OS and Asy-OS. Empirical study suggests that the performance of OS and Asy-OS is comparable to the benchmark algorithm Grafting, but requires much less memory cost and is easy to choose parameter. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M51S,171,2d,hd,FALSE,https://i.ytimg.com/vi/LIBLD8NM04Y/maxresdefault.jpg,,364,1,0,0,0
502,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,_s3p4s9dUCU,2016-06-30T17:20:09Z,30/6/16 17:20,KDD2016 paper 958,"Title: Scalable Betweenness Centrality Maximization via Sampling Authors: Ahmad Mahmoody*, Brown University Eli Upfal, Brown University Charalampos Tsourakakis, Harvard Abstract Betweenness centrality is a fundamental centrality measure in social network analysis. Given a large-scale network, how can we find the most central nodes? This question is of great importance to many key applications that rely on BWC, including community detection and understanding graph vulnerability. Despite the large amount of work on scalable approximation algorithm design for BWC, estimating BWC on large-scale networks remains a computational challenge. In this paper, we study the Centrality Maximization problem. We present an efficient randomized algorithm that provides approximation with high probability. Our results improve the current state-of-the-art result. Furthermore, we provide the first theoretical evidence for the validity of a crucial assumption in betweenness centrality estimation, namely that in real-world networks shortest paths pass through the top-k central nodes, where k is a constant. This also explains why our algorithm runs in near linear time on real- world networks. We also show that our algorithm and analysis can be applied to a wider range of centrality measures, by providing a general analytical framework. On the experimental side, we perform an extensive experimental analysis of our method on real-world networks, demonstrate its accuracy and scalability, and study different properties of central nodes. Then, we compare the sampling method used by the state-of-the-art algorithm with our method. Furthermore, we perform a study of BWC in time evolving networks, and see how the centrality of the central nodes in the graphs changes over time. Finally, we compare the performance of the stochastic Kronecker model to real data, and observe that it generates a similar growth pattern. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M19S,139,2d,hd,FALSE,https://i.ytimg.com/vi/_s3p4s9dUCU/maxresdefault.jpg,,1938,23,0,0,1
503,UCPsUUDUlcTJuP-fRa7z85aQ,KDD2016 video,l2IlkEC0-hk,2016-06-30T17:20:08Z,30/6/16 17:20,KDD2016 paper 799,"Title: Identifying Earmarks in Congressional Bills Authors Lingyang Chu*, Simon Fraser University Zhefeng Wang, University of Science and Technology of China Jian Pei, Simon Fraser University Jiannan Wang, Simon Fraser University Zijin Zhao, Simon Fraser University Enhong Chen, Simon Fraser University Abstract Given a signed network where edges are weighted in real number, and positive weights indicate cohesion between vertices and negative weights indicate opposition, we are interested in finding $k$-oppositive cohesive groups ($k$-OCG). Each $k$-OCG is a a group of $k$ subgraphs such that (1) within each subgraph edges are dense and cohesive; and (2) the edges crossing different subgraphs are dense and oppositive. Finding $k$-OCGs is challenging since the subgraphs are often small, there are multiple $k$-OCGs in a large signed networks, and many existing dense subgraph extraction methods cannot handle edges of two signs. We model finding $k$-OCGs as a quadratic optimization problem. However, the classical Proximal Gradient method is very costly since it has to use the entire adjacency matrix, which is huge on large networks. We develop FOCG, an algorithm that is two orders of magnitudes faster than the Proximal Gradient method. The main idea is to only search in small subgraphs and thus avoid using a major portion of the adjacency matrix. Our experimental results on synthetic and real data sets as well as a case study clearly demonstrate the effectiveness of finding $k$-OCGs and the efficiency of our method. More on http://www.kdd.org/kdd2016/ KDD2016 Conference will be recorded and published on http://videolectures.net/",22,People & Blogs,PT2M4S,124,2d,hd,FALSE,https://i.ytimg.com/vi/l2IlkEC0-hk/maxresdefault.jpg,,173,1,0,0,0